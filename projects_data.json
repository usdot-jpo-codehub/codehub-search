{"_index":"projects","_type":"project","_id":"23056647_102626987","_score":1,"_source":{"commits":56,"contributors":3,"contributors_list":[{"avatar_url":"https://avatars1.githubusercontent.com/u/4628742?v=4","profile_url":"https://github.com/wwhyte-si","user_type":"User","username":"wwhyte-si"},{"avatar_url":"https://avatars1.githubusercontent.com/u/7940050?v=4","profile_url":"https://github.com/conz27","user_type":"User","username":"conz27"},{"avatar_url":"https://avatars2.githubusercontent.com/u/10130982?v=4","profile_url":"https://github.com/hmusavi","user_type":"User","username":"hmusavi"}],"created_at":"2017-09-06T15:40:05Z","forks":{"forkedRepos":[{"id":"1860862_123641608","name":"1609dot2-asn","org_name":"Edon07"}]},"full_name":"usdot-jpo-ode/1609dot2-asn","languages":{},"organization":{"org_avatar_url":"https://avatars2.githubusercontent.com/u/23056647?v=4","org_type":"Organization","organization":"usdot-jpo-ode","organization_url":"https://github.com/usdot-jpo-ode"},"origin":"PUBLIC","project_description":"ASN.1 schemas for IEEE 1609.2","project_name":"1609dot2-asn","rank":106,"readMe":{"content":"# 1609dot2-asn\nASN.1 schemas for IEEE 1609.2\n","url":"https://raw.githubusercontent.com/usdot-jpo-ode/1609dot2-asn/scms-asn-v1.2.1-release/README.md"},"releases":[],"repository":"1609dot2-asn","repository_url":"https://github.com/usdot-jpo-ode/1609dot2-asn","stage_id":"23056647_102626987","stars":1,"suggest":[{"input":["1609dot2-asn"],"output":"1609dot2-asn# name"},{"input":["1609dot2-asn"],"output":"1609dot2-asn# name"},{"input":["ASN1","schemas","for","IEEE","16092"],"output":"1609dot2-asn# description"},{"input":[],"output":"1609dot2-asn# languages"},{"input":["wwhyte-si","conz27","hmusavi"],"output":"1609dot2-asn# contributors"}],"updated_at":"2018-04-16T17:13:53Z","watchers":8}}
{"_index":"projects","_type":"project","_id":"29404495_120921410","_score":1,"_source":{"commits":10,"contributors":2,"contributors_list":[{"avatar_url":"https://avatars0.githubusercontent.com/u/1124162?v=4","profile_url":"https://github.com/jmcarter9t","user_type":"User","username":"jmcarter9t"},{"avatar_url":"https://avatars1.githubusercontent.com/u/15731135?v=4","profile_url":"https://github.com/James-OHara","user_type":"User","username":"James-OHara"}],"created_at":"2018-02-09T15:22:54Z","forks":{"forkedRepos":[{"id":"18469242_138419629","name":"privacy-protection-application","org_name":"pir8aye"},{"id":"1124162_121287497","name":"privacy-protection-application","org_name":"jmcarter9t"}]},"full_name":"usdot-its-jpo-data-portal/privacy-protection-application","language":"C++","languages":{"C++":"945000","CMake":"10649","CSS":"74967","HTML":"261016","JavaScript":"258143"},"organization":{"org_avatar_url":"https://avatars1.githubusercontent.com/u/29404495?v=4","org_type":"Organization","organization":"usdot-its-jpo-data-portal","organization_url":"https://github.com/usdot-its-jpo-data-portal"},"origin":"PUBLIC","project_description":"Privacy Protection Application (PPA): Code and instructions for building two tools for processing vehicle trip, or trajectory, data to protect driver privacy by hiding potentially sensitive locations. These tools were developed to process large databases of trips generated during connected vehicle pilot studies, but they could be used on any type of moving object data defined as time-sequences of geolocations that also includes heading and speed.","project_name":"privacy-protection-application","rank":53,"readMe":{"content":"# Privacy Protection Application (PPA) \nThis repository contains code and instructions for building two tools (GUI and CLI) for\nprocessing vehicle trip, or trajectory, data. These tools were developed to process large databases of trips generated\nduring connected vehicle pilot studies; however, the **Privacy Protection Application can be used on any type of moving object\ndata defined as time-sequences of geolocations that also includes heading and speed.**\n\nThe [Privacy Protection Application Manual](./docs/cvdi-user-manual.md) describes the GUI tool's interface, and the\nparameter settings and their influence on application operation. Users should review that material prior to using either\ntool.\n\n# Table of Contents\n- [Getting Involved](#getting-involved)\n- [Release Notes](#release-notes)\n- [Dependancies and Prerequisites](#dependancies-and-prerequisitives)\n- [Building the Privacy Protection Application](#building-the-privacy-protection-application)\n- [Post Build Application Installation Instructions](#post-build-application-installation)\n- [Running the Command Line Tool](#running-the-command-line-tool)\n- [Issues, Comments, and Additional Inquiries](#issues-and-questions)\n\n# Getting Involved\n\nThis project, sponsored by the U.S. Department of Transportation, limits the risk of traveler\nidentification when using valuable mobility data that will aid the research community. To project [factsheet](https://www.its.dot.gov/factsheets/pdf/Managing_ITS_DataPrivacyRisk.pdf) contains more information. You can become involved in this project in several\nways:\n\n- **Pull the code and try it out!**  The [instructions](#introduction) that immediately follow will get you started.  [Build](#building-the-privacy-protection-application) instructions are also provided.  Let us know if you need help.\n    - Github has [instructions](https://help.github.com/articles/signing-up-for-a-new-github-account) for setting up an account and getting started with repositories.\n- If you would like to improve this code base or the documentation, [fork the project](https://github.com/usdot-its-jpo-data-portal/privacy-protection-application#fork-destination-box) and submit a pull request (see the next section).\n- If you find a problem with the code or the documentation, please submit an [issue](https://github.com/usdot-its-jpo-data-portal/privacy-protection-application/issues/new).\n- If the PPA does not solve your mobility data privacy problem, please submit an [issue](https://github.com/usdot-its-jpo-data-portal/privacy-protection-application/issues/new) and prefix the issue title with **[New Feature]**. We would like to help.\n\n## Introduction\n\nThe PPA project uses the [Pull Request Model](https://help.github.com/articles/using-pull-requests). This involves the following project components:\n\n- The Organization Privacy Protection Application Project's [master branch](https://github.com/usdot-its-jpo-data-portal/privacy-protection-application).\n- PPA releases are made via [tags](https://github.com/usdot-its-jpo-data-portal/privacy-protection-application/releases) out of master.\n- A personal GitHub account.\n- A fork of a project release tag or master branch in your personal GitHub account.\n\nA high level overview of our model and these components is as follows. All work will be submitted via pull requests.\nDevelopers will work on branches on their personal machines (local clients), push these branches to their **personal GitHub repos** and issue a pull\nrequest to the organization PPA project. One the project's main developers must review the Pull Request and merge it\nor, if there are issues, discuss them with the submitter. This will ensure that the developers have a better\nunderstanding of the code base *and* we catch problems before they enter `master`. The following process should be followed:\n\n## Initial Setup\n\n1. If you do not have one yet, create a personal (or organization) account on GitHub (assume your account name is `<your-github-account-name>`).\n1. Log into your personal (or organization) account.\n1. Fork [private-protection-application](https://github.com/usdot-its-jpo-data-portal/privacy-protection-application/fork) into your personal GitHub account.\n1. On your computer (local client), clone the master branch from you GitHub account:\n```bash\n$ git clone https://github.com/<your-github-account-name>/privacy-protection-appliation.git\n```\n\n### Additional Resources for Initial Setup\n  \n- [About Git Version Control](http://git-scm.com/book/en/v2/Getting-Started-About-Version-Control)\n- [First-Time Git Setup](http://git-scm.com/book/en/Getting-Started-First-Time-Git-Setup)\n- [Article on Forking](https://help.github.com/articles/fork-a-repo)\n\n## Development in Your Fork\n\nAs an example, assume you are enhancing the existing code or documentation, or working on an\n[issue](https://github.com/usdot-its-jpo-data-portal/privacy-protection-application/issues).  Complete the following\nsteps on your computer (local client):\n\n1. Add an upstream remote repository pointing to the Privacy Protection Application project repository.\n    ```bash\n    $ git remote add upstream https://github.com/usdot-its-jpo-data-portal/privacy-protection-application.git\n    ```\n1. Pull from the upstream remote repository to get the latest master branch.\n    ```bash\n    $ git pull --rebase upstream master\n    ```\n1. Create a branch on your client (your forked repository).\n    ```bash\n    $ git branch enhancement\n    ```\n   where ```enhancement``` is the name you want to use for the branch.\n1. Switch to this branch\n    ```bash\n    $ git checkout enhancement\n    ```\n1. Make the changes need to enhance the code. Note that before doing a pull request (below) you also want to ensure that any changes that have been accepted upstream can be integrated with any changes you are making now, so perform the following command in your forked repository branch prior to issuing a pull request:\n    ```bash\n    $ git pull upstream master\n    ```\n1. When you have completed coding and integrating the upstream code, run the unit tests on your client in your build directory.\n    ```bash\n    $ cd ${BUILD_DIR}/cv-lib-test\n    $ ./cvlib_tests\n    ```\n1. When all the tests pass you can **add, commit, and push your updates to your `enchancement` branch in your fork.** Follow these steps:\n    1. Check which files have been changed\n        ```bash\n        $ git status\n        ```\n    1. Add the files that changed based on your enhancement, or that you want to include in the pull request.\n        ```bash\n        $ git add <file1> <file2>\n        ```\n    1. Commit the files with a message describing the enhancement or bug fix.\n        ```bash\n        $ git commit -m \"improved this aspect of the code.\"\n        ```\n    1. Push the branch changes (enhancements) to your GitHub account:\n        ```bash\n        $ git push origin enhancement\n        ```\n\n## Create A Pull Request\n\n1. After you have pushed your `enhancement` branch to your GitHub account, issue a pull request on GitHub. Details on how to perform a [pull request](https://help.github.com/articles/using-pull-requests) are on GitHub.\n1. If the pull request is closing an issue, include a comment with `fixes #issue_number`.  This comment will close the issue when the pull request is merged.  For more information see [here](https://github.com/blog/1506-closing-issues-via-pull-requests).\n1. One of the main project developers will review your pull request and either merge it, or send you feedback. **Do not merge your own pull request.** Code review is essential. If you have not received feedback on your pull request in a timely fashion, contact us via email.\n1. Once your pull request has been reviewed and merged (possibly closing an issue), your enhancement will now be part of the privacy protection application project `master` branch.\n1. On your client machine, you can delete your branch.\n    ```bash\n    $ git branch -d enhancement\n    ```\n1. Pull from the privacy protection application project's `master` branch to have your changes reflected in your local (laptop/desktop) `master` branch:\n    ```bash\n    $ git checkout master\n    $ git pull --rebase upstream master\n    ```\n1. To include these in your fork's master branch, push them to your GitHub account.\n    ```git push origin master```\n1. At this point the three master branches (one on organization, one on your GitHub account, and one on your client) are all in sync.\n\n# Release Notes\n\n## Initial Release\n- [v1.0](https://github.com/usdot-its-jpo-data-portal/privacy-protection-application/releases/tag/v1.0): Release of the GUI and the CLI for public use and evaluation.\n\n# Dependancies and Prerequisites\n\n- Install [Git](https://git-scm.com/) or you will not be able to interact with this repository.\n- Install [CMake](https://cmake.org) to build these applications.\n- [Catch](https://github.com/philsquared/Catch) is used for unit testing, but it is included in the repository.\n- [Node.js](https://nodejs.org/en/download) is needed to build and run the privacy protection application; this should\n  also install the required Node Package Manager (`npm`). See below for more information. On OS X, Node.js can also be installed\n  using MacPorts and Brew.\n    - (Ubuntu Install of Node.js)\n        ```bash\n        $ curl -sL https://deb.nodesource.com/setup_9.x | sudo -E bash -\n        $ sudo apt install -y nodejs\n        ```\n    - The final command will install `node_version.h` in `/usr/include/node/`\n- If building on Windows, [Visual C++ Build Tools](http://landinghub.visualstudio.com/visual-cpp-build-tools) are needed. You may run into [this issue](https://stackoverflow.com/questions/33743493/why-visual-studio-2015-cant-run-exe-file-ucrtbased-dll). We found that having Win10 SDK installed fixed the issue on our system.\n- We built the Windows application using cmake generated Nmake files. Other build environments were not tested.\n\n# Building the Privacy Protection Application\n\nThe following procedure outlines a fresh install after the above dependencies have been installed.  It builds both the\ncommand line tool and the graphical user interface form of the application. By following the procedures the repository\nwill reside in the `$PPA_BASE_DIR`. An out of source build directory, `$PPA_BUILD_DIR`), will also be created.\n\nThe command line tool executable will be located in `$PPA_BUILD_DIR`.\n\n- CLI will not build unless it has nan.h\n- node installation is not enough\n- good direction on node installation on linux: https://nodejs.org/en/download/package-manager/\n- installed nan and it didn't work.\n\n1. Install Node.js if not installed (see [Dependencies](#dependancies-and-prerequisites)). You can check for installation using these version commands:\n    ```bash\n    $ node -v\n    $ npm -v\n    ```\n1. Make a directory (`$PPA_BASE_DIR`) where the repository will be cloned and clone the repository. \n    ```bash    \n    $ git clone https://github.com/usdot-its-jpo-data/privacy-protection-application ${PPA_BASE_DIR}\n    ```\n1. Make a directory (`$PPA_BUILD_DIR`) to build the application (and CLI tool). Out of source builds are recommended.\n    ```bash \n    $ mkdir ${PPA_BUILD_DIR}\n    ```\n1. Using Node Package Manager (`npm`), install the following build tools and dependencies:\n    - Install the CMake-like build system for Node.js called [Cmake.js](https://github.com/cmake-js/cmake-js)\n        ```bash\n        $ sudo npm install --save -g cmake-js\n        ```\n    - Install the [Electron Package Manager](https://github.com/electron-userland/electron-packager)\n        ```bash\n        $ sudo npm install  --save-dev -g electron-packager\n        ```\n    - Install [Electron](https://github.com/electron/electron). Running the install command in the `PPA_BASE_DIR` avoids an error on linux.\n        ```bash\n        $ cd ${PPA_BASE_DIR}\n        $ npm install  --save-dev --save-exact electron\n        ```\n    - Using [Native Abstractions for Node.js](https://www.npmjs.com/package/nan). Running the install command in the `PPA_BASE_DIR` avoids an error on linux.\n        ```bash\n        $ cd ${PPA_BASE_DIR}\n        $ npm install --save nan\n        ```\n1. The file `package.json` must be in the `$PPA_BASE_DIR` to build using `cmake-js`. *It should be included when you clone the repository.*\n1. Make a directory (`$PPA_APP_DIR`) to install the application.\n1. Build the tools. This will build both the GUI and CLI tools.\n    ```bash\n    $ cmake-js -d ${PPA_BASE_DIR} -O ${PPA_BUILD_DIR} -a x64 -r electron -v 1.4.15\n    // the command line tool is built at this point.\n    $ electron-packager --icon=${PPA_BUILD_DIR}/cv-gui-electron/${PPA_BUILD_DIR}/Release/electron-app/images/${OS_IMAGE} --overwrite --out ${PPA_APP_DIR} --electron-version=1.7.12 ${PPA_BUILD_DIR}/cv-gui-electron/${PPA_BUILD_DIR}/Release/electron-app\n    ```\n    Where (`$OS_IMAGE`) is the images type for your operating system. (Windows = *.ico, OSX = *.icns, Linux = *.png). Make sure to substitute your architecture (x64 above) and Electron version (1.4.15 above) in previous commands.\n\n# Post Build Installation\n\nInstallation of the Route Sanitizer application involves downloading the\napplication zip file, unzipping the file, and creating a shortcut for\neasy application launch. The process is almost identical on Windows\n7/8/10 and Mac OS X.  Windows screen shots are given in the main body of\nthe user’s manual while Mac screen shots are given in Appendix A. The\nnumbers in red circles highlight areas in the figures that pertain to\ncertain instructions: the bracketed number in the instruction\ncorresponds to the circled number in the figure. The steps that follow\noutline this process:\n\n1. Move the entire RouteSanitizer folder to any desired location on\nyour computer before creating any shortcuts. The Download folder is\ngenerally not a good location for the unzipped application. We suggest\ncreating a project folder and putting the RouteSanitizer application\nfolder inside the project folder. Separate folder can be created inside\nthe project folder for data files and/or result files.\n\n1. Create Route Sanitizer shortcuts.\n\n    1. Windows:\n\n        1. In the “RouteSanitizer-win32-x64” folder find the file named “RouteSanitizer.exe” or “RouteSanitizer”. Note that the file name extension “.exe” might not be visible.\n\n        1. Right click on the RouteSanitizer file, select “Send to” in the first pop-up menu and then “Desktop (create shortcut)” in the second pop-up menu. (For pictorial instructions, see: <http://www.thewindowsclub.com/create-desktop-shortcut-windows-10> .) If you want to rename the shortcut, right click anywhere on the desktop shortcut and select “Rename”. The name text can then be edited.\n\n        1. If you would also like the RouteSanitizer shortcut to appear in the Windows task bar, just drag the RouteSanitizer desktop shortcut to the desired location on the Windows task bar.\n\n    1. Mac:\n\n        1. In the “RouteSanitizer- darwin -x64” folder find the file named “RouteSanitizer”. Note that on Macs the “.app” file name extension is not visible by default even if extensions are visible for other file types.\n\n        1. Right click on the “RouteSanitizer” application and select “Make Alias” from the popup menu. Drag the shortcut “RouteSanitizer alias” created in the “RouteSanitizer- darwin -x64” folder to the desktop.  (For pictorial instructions, see: <http://www.macworld.co.uk/how-to/mac-software/how-create-shortcuts-on-mac-3613491/> .) If you want to rename the shortcut, right click on the name portion of the desktop shortcut and select “Rename”. The name text can then be edited.\n\n        1. If you would also like the RouteSanitizer shortcut to appear in the Mac dock bar, just drag the RouteSanitizer desktop shortcut to the desired location on the dock bar.\n\n1. Launch the RouteSanitizer application.\n\n    1. To launch the RouteSanitizer application, just double click on\nthe desktop shortcut or single click on the taskbar (Windows) or dock\n(Mac) shortcut. The application can also be started by double clicking\non the application executable in RouteSanitizer folder.\n\n    1. When running a new version of the RouteSanitizer application for\nthe first time on a Mac, you might get a popup window that says\n“RouteSanitizer can’t be opened because it is from an unidentified\ndeveloper”. In this case, right click on the application or shortcut and\nselect “Open” from the popup menu. This time the unidentified developer\npopup window will have an “Open” button. Click the “Open” button to\nstart the application. The next time the application should start by\ndouble clicking on the shortcut or application.\n\nTo un-install the RouteSanitizer application, just delete the entire\nRouteSanitizer folder. No special un-install procedure is required on\neither Windows or Mac computers.\n\nTo install a new version of RouteSanitizer, delete the old version and\nthen go through the same install procedure for the new version. If you\nwant to keep multiple versions of RouteSanitizer, rename the folder of\nthe older version before installing the new version. You may also need\nto delete the shortcut to the old version and make a new one with a\ndifferent name if desired.\n\n# Running the Command Line Tool\n\nIn addition to the GUI form of the tool, the Privacy Protection Tool can be used from the command line after it has been built. The usage information follows. **Command line options override parameters specified in the configuration file.** When using the producer above the CLI tool will be located in `${PPA_BUILD_DIR}/cl-tool`.\n\n```bash\nUsage: cv_di [OPTIONS] SOURCE\nOPTIONS\n -c, --config         A configuration file for de-identification.\n -o, --out_dir        The output directory (default: working directory).\n -n, --count_pts      Print summary of the points after de-identification to standard error.\n -q, --quad           The file .quad file containing the circles defining the regions.\n -k, --kml_dir        The KML output directory (default: working directory).\n -t, --thread         The number of threads to use (default: 1 thread).\n -h, --help           Print this message.\n```\n\nSOURCE is a text file that contains the path to a trip file on each line. As an example, the following command uses a configuration file to process the trips contained in SOURCE:\n\n```bash\n$ ./cv_di -c <configuration file> <source-file>\n```\n\n# Running The Library Tests\n\nThe library tests are designed to cover most of the functions and routines used in the Privacy Protection Tool. To run the compiled library tests, you need to change directory into the test directory and execute the test command:\n\n```bash\n$ cd cv-lib-test\n$ ./cvlib_tests\n```\n\n# Issues and Questions\n\nIf you need to contact the principal investigator or developers for this project, \nplease submit an [issue](https://github.com/usdot-its-jpo-data-portal/privacy-protection-application/issue).  \nPlease consult the project [factsheet](https://www.its.dot.gov/factsheets/pdf/Managing_ITS_DataPrivacyRisk.pdf) for additional information and contacts.\n\n","url":"https://raw.githubusercontent.com/usdot-its-jpo-data-portal/privacy-protection-application/master/README.md"},"releases":[],"repository":"privacy-protection-application","repository_url":"https://github.com/usdot-its-jpo-data-portal/privacy-protection-application","stage_id":"29404495_120921410","stars":3,"suggest":[{"input":["privacy-protection-application"],"output":"privacy-protection-application# name"},{"input":["privacy-protection-application"],"output":"privacy-protection-application# name"},{"input":["Privacy","Protection","Application","PPA","Code","and","instructions","for","building","two","tools","for","processing","vehicle","trip","or","trajectory","data","to","protect","driver","privacy","by","hiding","potentially","sensitive","locations","These","tools","were","developed","to","process","large","databases","of","trips","generated","during","connected","vehicle","pilot","studies","but","they","could","be","used","on","any","type","of","moving","object","data","defined","as","time-sequences","of","geolocations","that","also","includes","heading","and","speed"],"output":"privacy-protection-application# description"},{"input":["JavaScript","CSS","C++","HTML","CMake"],"output":"privacy-protection-application# languages"},{"input":["jmcarter9t","James-OHara"],"output":"privacy-protection-application# contributors"}],"updated_at":"2018-10-08T17:14:53Z","watchers":6}}
{"_index":"projects","_type":"project","_id":"33698304_117576470","_score":1,"_source":{"commits":18,"contributors":1,"contributors_list":[{"avatar_url":"https://avatars1.githubusercontent.com/u/33724014?v=4","profile_url":"https://github.com/andrewm-aero","user_type":"User","username":"andrewm-aero"}],"created_at":"2018-01-15T17:44:36Z","forks":{"forkedRepos":[]},"full_name":"usdot-jpo-sdcsdw/fedgov-cv-whtools-webapp","language":"CSS","languages":{"CSS":"115328","HTML":"45341","JavaScript":"42968","Shell":"1086"},"organization":{"org_avatar_url":"https://avatars1.githubusercontent.com/u/33698304?v=4","org_type":"Organization","organization":"usdot-jpo-sdcsdw","organization_url":"https://github.com/usdot-jpo-sdcsdw"},"origin":"PUBLIC","project_description":"Web interface to the SDW","project_name":"fedgov-cv-whtools-webapp","rank":30,"readMe":{"content":"# Connected Vehicles Warehouse Tools Webapp Project\n\nThe fedgov-cv-whtools-webapp project is a webapp providing tools to access the Connected Vehicle Warehouses.\n\n![Diagram](doc/images/fedgov-cv-whtools-webapp-diagram.png)\n\n![Diagram](doc/images/fedgov-cv-whtools-webapp-screenshot-query.png)\n\n![Diagram](doc/images/fedgov-cv-whtools-webapp-screenshot-deposit.png)\n\n<a name=\"toc\"/>\n\n## Table of Contents\n\n[I. Release Notes](#release-notes)\n\n[II. Documentation](#documentation)\n\n[III. Getting Started](#getting-started)\n\n[IV. Running the Application (Standalone)](#running-standalone)\n\n[V. Running the Application (Docker)](#running-docker)\n\n---\n\n<a name=\"release-notes\" id=\"release-notes\"/>\n\n## [I. Release Notes](ReleaseNotes.md)\n\n<a name=\"documentation\"/>\n\n## II. Documentation\n\nThis repository produces a WAR file containing a Jetty Servlet, so it can be deployed on any Jetty server that supports websockets.\n\nThe application can also be deployed using a docker container. This container will run the application under a Jetty server, and can be configured to use SSL certificates.\n\n<a name=\"getting-started\"/>\n\n## III. Getting Started\n\nThe following instructions describe the proceedure to fetch, build, and run the application\n\n### Prerequisites\n* JDK 1.8: http://www.oracle.com/technetwork/pt/java/javase/downloads/jdk8-downloads-2133151.html\n* Maven: https://maven.apache.org/install.html\n* Git: https://git-scm.com/\n* Docker: https://docs.docker.com/engine/installation/\n* SDW Websockets Interface: https://github.com/usdot-jpo-sdcsdw/fedgov-cv-webapp-websocket\n* PerXerCodec: https://github.com/usdot-jpo-sdcsdw/per-xer-codec\n\n---\n### Obtain the Source Code\n\n#### Step 1 - Clone public repository\n\nClone the source code from the GitHub repository using Git command:\n\n```bash\ngit clone https://github.com/usdot-jpo-sdcsdw/fedgov-cv-whtools-webapp.git\n```\n\n<a name=\"running\"/>\n\n## IV. Running the application (Standalone)\n\n---\n### Build and Deploy the Application\n\n**Step 1**: Build the WAR file\n\n```bash\nmvn package\n```\n\n**Step 2**: Deploy the WAR file\n\n```bash\n# Consult your webserver's documentation for instructions on deploying war files \ncp target/whtools.war ... \n```\n\n<a name=\"running-docker\"/>\n\n## V. Running the Application (Docker)\n\n---\n### Build and Deploy the Application\n\n**Step 1**: Build the WAR file\n\n```bash\nmvn package\n```\n\n**Step 2**: Build the Docker image, providing the path to the native library for the PER-XER codec\n\n```bash\ndocker build -t dotcv/whtools-webapp --build-arg CODEC_SO_PATH=... .\n```\n\nThis path depends on which OS you are building on. If you are building on a Linux system, codec is located at target/libper-xer-codec.so after running the maven build.\nIf you are building on OSX, you will need to provide the path to the Linux SO you built manually, according to the instructions provided by that project.\n\n**Step 3**: Run the Docker image in a Container, mounting the SSL certificate keystore directory, and specifying the following:\n* Keystore filename\n* Keystore password\n* HTTP Port\n* HTTPS Port\n\n\n```bash\ndocker run -p HTTP_PORT:8080 \\\n           -p HTTPS:_PORT:8443 \\\n           -e JETTY_KEYSTORE_PASSWORD=... \\\n           -v KEYSTORE_DIRECTORY:/usr/local/jetty/etc/keystore_mount \\\n           -e JETTY_KEYSTORE_RELATIVE_PATH=... \\\n            dotcv/whtools-webapp:latest\n```\n\n</a>","url":"https://raw.githubusercontent.com/usdot-jpo-sdcsdw/fedgov-cv-whtools-webapp/master/README.md"},"releases":[],"repository":"fedgov-cv-whtools-webapp","repository_url":"https://github.com/usdot-jpo-sdcsdw/fedgov-cv-whtools-webapp","stage_id":"33698304_117576470","stars":1,"suggest":[{"input":["fedgov-cv-whtools-webapp"],"output":"fedgov-cv-whtools-webapp# name"},{"input":["fedgov-cv-whtools-webapp"],"output":"fedgov-cv-whtools-webapp# name"},{"input":["Web","interface","to","the","SDW"],"output":"fedgov-cv-whtools-webapp# description"},{"input":["CSS","HTML","JavaScript","Shell"],"output":"fedgov-cv-whtools-webapp# languages"},{"input":["andrewm-aero"],"output":"fedgov-cv-whtools-webapp# contributors"}],"updated_at":"2018-10-05T15:20:47Z","watchers":1}}
{"_index":"projects","_type":"project","_id":"33698304_117581073","_score":1,"_source":{"commits":2,"contributors":2,"contributors_list":[{"avatar_url":"https://avatars1.githubusercontent.com/u/33724014?v=4","profile_url":"https://github.com/andrewm-aero","user_type":"User","username":"andrewm-aero"},{"avatar_url":"https://avatars2.githubusercontent.com/u/35463231?v=4","profile_url":"https://github.com/mahmad-aero","user_type":"User","username":"mahmad-aero"}],"created_at":"2018-01-15T18:32:32Z","forks":{"forkedRepos":[]},"full_name":"usdot-jpo-sdcsdw/udp-interface","language":"Roff","languages":{"Dockerfile":"467","Java":"106106","Roff":"5242902"},"organization":{"org_avatar_url":"https://avatars1.githubusercontent.com/u/33698304?v=4","org_type":"Organization","organization":"usdot-jpo-sdcsdw","organization_url":"https://github.com/usdot-jpo-sdcsdw"},"origin":"PUBLIC","project_name":"udp-interface","rank":19,"readMe":{"content":"# UDP Interface\n\nThe SDC/SDW stores and distributes Traveler Information Messages (TIMs). The UDP Interface of the mvp SDC/SDW allows users to programmatically receive these TIMS bundled as Advisory Situation Data Distributions. To receive data from this interface, a specific UDP dialog sequence must be followed. The below diagram depicts this sequence. \n\n![UDP Dialog Sequence](images/udp_dialog_sequence.png)\n\n**Each of these messages are UPER encoded.\n\n\n### Prerequisites\n* JDK 1.8: http://www.oracle.com/technetwork/pt/java/javase/downloads/jdk8-downloads-2133151.html\n* Maven: https://maven.apache.org/install.html\n* Git: https://git-scm.com/\n\n\n### Getting Started\n\n#### Step 1 - Clone this repository\n```\ngit clone https://github.com/usdot-jpo-sdcsdw/udp-interface.git\n```\n#### Step 2 - Clone and install dependencies\nFollow instructions for cloning and installing the common-models and per-xer-codec repositories\n\n#### Step 2 - Build the application\n```\ncd udp-interface\nmvn clean install\n```\n\n#### Step 3 - Set Up & Running the Application\nCopy the UDPInterface-1.0.0-SNAPSHOT-jar-with-dependencies to a desired directory. In this example we will use \"runDir\"\n```\nmkdir runDir\ncp /target/UDPInterface-1.0.0-SNAPSHOT-jar-with-dependencies.jar ./runDir/\n```\nCopy the native library generated by the building the per-xer-codec to runDir. \n```\ncp libper-xer-codec.so ./runDir/ \n```\nCreate config/settings.properties. Sample [settings.properties](src/main/resources/config/settings.properties)\n```\ncd runDir\nmkdir config\nnano config/settings.properties\n```\n\nRun the application\n```\njava -Djava.library.path=. -jar UDPInterface-1.0.0-SNAPSHOT-jar-with-dependencies.jar\n```\n\n\n## Built With\n\n* [Maven](https://maven.apache.org/) - Dependency Management\n\n\n## License\n\nThis project is licensed under the Apache License - see  [LICENSE](LICENSE) file for details\n\n\n","url":"https://raw.githubusercontent.com/usdot-jpo-sdcsdw/udp-interface/master/README.md"},"releases":[],"repository":"udp-interface","repository_url":"https://github.com/usdot-jpo-sdcsdw/udp-interface","stage_id":"33698304_117581073","stars":1,"suggest":[{"input":["udp-interface"],"output":"udp-interface# name"},{"input":["udp-interface"],"output":"udp-interface# name"},{"input":[""],"output":"udp-interface# description"},{"input":["Roff","Java","Dockerfile"],"output":"udp-interface# languages"},{"input":["andrewm-aero","mahmad-aero"],"output":"udp-interface# contributors"}],"updated_at":"2018-12-06T14:50:58Z","watchers":1}}
{"_index":"projects","_type":"project","_id":"23056647_164011290","_score":1,"_source":{"commits":1,"contributors":1,"contributors_list":[{"avatar_url":"https://avatars0.githubusercontent.com/u/12912578?v=4","profile_url":"https://github.com/mvs5465","user_type":"User","username":"mvs5465"}],"created_at":"2019-01-03T18:56:19Z","forks":{"forkedRepos":[]},"full_name":"usdot-jpo-ode/jpo-file-ingester","languages":{},"organization":{"org_avatar_url":"https://avatars2.githubusercontent.com/u/23056647?v=4","org_type":"Organization","organization":"usdot-jpo-ode","organization_url":"https://github.com/usdot-jpo-ode"},"origin":"PUBLIC","project_description":"Ingests files and publishes the data to Kafka","project_name":"jpo-file-ingester","rank":26,"readMe":{"content":"# jpo-file-ingester\nIngests files and publishes the data to Kafka\n","url":"https://raw.githubusercontent.com/usdot-jpo-ode/jpo-file-ingester/master/README.md"},"releases":[],"repository":"jpo-file-ingester","repository_url":"https://github.com/usdot-jpo-ode/jpo-file-ingester","stage_id":"23056647_164011290","stars":0,"suggest":[{"input":["jpo-file-ingester"],"output":"jpo-file-ingester# name"},{"input":["jpo-file-ingester"],"output":"jpo-file-ingester# name"},{"input":["Ingests","files","and","publishes","the","data","to","Kafka"],"output":"jpo-file-ingester# description"},{"input":[],"output":"jpo-file-ingester# languages"},{"input":["mvs5465"],"output":"jpo-file-ingester# contributors"}],"updated_at":"2019-01-03T18:56:21Z","watchers":5}}
{"_index":"projects","_type":"project","_id":"23056647_99961005","_score":1,"_source":{"commits":41,"contributors":3,"contributors_list":[{"avatar_url":"https://avatars0.githubusercontent.com/u/1120608?v=4","profile_url":"https://github.com/kimPerry","user_type":"User","username":"kimPerry"},{"avatar_url":"https://avatars2.githubusercontent.com/u/18171845?v=4","profile_url":"https://github.com/angular-cli","user_type":"User","username":"angular-cli"},{"avatar_url":"https://avatars0.githubusercontent.com/u/549261?v=4","profile_url":"https://github.com/tonychen091","user_type":"User","username":"tonychen091"}],"created_at":"2017-08-10T20:10:24Z","forks":{"forkedRepos":[{"id":"42840583_152477920","name":"jpo-tim-builder","org_name":"darrelld05"},{"id":"13427284_116835718","name":"jpo-tim-builder","org_name":"wayties"},{"id":"2973053_99962922","name":"jpo-tim-builder","org_name":"Trihydro"}]},"full_name":"usdot-jpo-ode/jpo-tim-builder","language":"CSS","languages":{"Batchfile":"5006","CSS":"544829","HTML":"299434","Java":"29619","JavaScript":"305158","Shell":"7058","TypeScript":"46345"},"organization":{"org_avatar_url":"https://avatars2.githubusercontent.com/u/23056647?v=4","org_type":"Organization","organization":"usdot-jpo-ode","organization_url":"https://github.com/usdot-jpo-ode"},"origin":"PUBLIC","project_description":"SUBMODULE: Visual GUI for building TIM messages for testing. Interacts with the ODE RESTful API ","project_name":"jpo-tim-builder","rank":91,"readMe":{"content":"# CV TIM Builder\r\nThe CV TIM Builder is an open source application built to send test TIMs (traveler information messages) to RSUs through the ODE. \r\n\r\n<a name=\"toc\"/>\r\n\r\n## Table of Contents \r\n\r\n[I. Release Notes](#release-notes) \r\n\r\n[II. Documentation](#documentation) \r\n\r\n[III. Getting Started](#getting-started) \r\n\r\n[IV. Running the Application](#running) \r\n\r\n--- \r\n\r\n<a name=\"release-notes\"/>\r\n \r\n## I. Release Notes\r\n\r\n### Release 2\r\n- Added functionality to deposit TIMs to SDW\r\n- NE and SW default lat/longs are at the NW and SE corners of Wyoming\r\n\r\n### Release 1\r\n- Functionality to create and send TIMs to RSUs by building a Path between mileposts\r\n- Functionality to disable TIMs from RSUs\r\n\r\n<a name=\"documentation\"/>\r\n\r\n## II. Documentation\r\nThe TIM Builder has been designed to be configurable enough to work with any group involved with the CV project. It has been integrated with Docker so that it can be deployed on a system without requiring a large list of dependencies. \r\n\r\nThere are three Docker containers used when deploying this application:\r\n\r\n* A MySQL database which contains data for RSUs, milepost locations, and ITIS codes \r\n* A Java Spring REST Service which retreives data from the MySQL database\r\n* An Angular2 web front end for designing TIMs, sending them, and deleting existing TIMs off RSUs\r\n\r\nUsers will need to provides their own data in CSV files which will need to follow the format specified. More information on data entry can be found here (link to service README).  \r\n\r\nThis repository will be continually updated with adjustments to TIM fields.\r\n\r\n## III. Getting Started\r\n\r\nThe following instructions describe the procedure to fetch, build, and run the application. \r\n\r\n### Prerequisites\r\n* JDK 1.8: http://www.oracle.com/technetwork/pt/java/javase/downloads/jdk8-downloads-2133151.html\r\n* Maven: https://maven.apache.org/install.html\r\n* Git: https://git-scm.com/\r\n* Docker: https://docs.docker.com/engine/installation/\r\n\r\n---\r\n### Obtain the Source Code\r\n\r\n#### Step 1 - Clone public repository\r\n\r\nClone the source code from the GitHub repository using Git command:\r\n\r\n```bash\r\ngit clone https://github.com/usdot-jpo-ode/jpo-tim-builder.git\r\n```\r\n\r\n## IV. Running the Application\r\n---\r\n### Build and Deploy the Application\r\n\r\nThe REST service relies on Maven to manage builds and run unit tests.\r\n\r\n**Step 1**: Build the REST service\r\n\r\nNavigate to the service directory and build:\r\n\r\n```bash\r\n cd service/complete\r\n mvn clean install\r\n```\r\nThis build will run unit tests using an in-memory H2 database created with the SQL script unitTestSql.sql located at service\\complete\\src\\main\\resources\\db. \r\n\r\n**Step 2**: Build Docker services \r\n\r\nNavigate back to the root directory and build the Docker services. This may take a some time to complete initially because Docker needs to download Node, and Java. \r\n\r\n```bash\r\n cd ../../\r\n docker-compose build\r\n```\r\n\r\n**Step 3**: Run the application\r\n\r\nThis command will download MySql, and start the MySQL, REST service, and web application containers. The application will then be accessable at  `localhost:4200`. \r\n\r\n```bash\r\n docker-compose up\r\n```\r\n\r\n[Back to top](#toc)\r\n","url":"https://raw.githubusercontent.com/usdot-jpo-ode/jpo-tim-builder/master/README.md"},"releases":[],"repository":"jpo-tim-builder","repository_url":"https://github.com/usdot-jpo-ode/jpo-tim-builder","stage_id":"23056647_99961005","stars":1,"suggest":[{"input":["jpo-tim-builder"],"output":"jpo-tim-builder# name"},{"input":["jpo-tim-builder"],"output":"jpo-tim-builder# name"},{"input":["SUBMODULE","Visual","GUI","for","building","TIM","messages","for","testing","Interacts","with","the","ODE","RESTful","API"],"output":"jpo-tim-builder# description"},{"input":["JavaScript","Shell","CSS","TypeScript","Batchfile","HTML","Java"],"output":"jpo-tim-builder# languages"},{"input":["kimPerry","angular-cli","tonychen091"],"output":"jpo-tim-builder# contributors"}],"updated_at":"2017-09-15T21:57:50Z","watchers":8}}
{"_index":"projects","_type":"project","_id":"29147435_96899300","_score":1,"_source":{"commits":27,"contributors":2,"contributors_list":[{"avatar_url":"https://avatars2.githubusercontent.com/u/464800?v=4","profile_url":"https://github.com/bsumne01","user_type":"User","username":"bsumne01"},{"avatar_url":"https://avatars2.githubusercontent.com/u/8005360?v=4","profile_url":"https://github.com/ldnash","user_type":"User","username":"ldnash"}],"created_at":"2017-07-11T13:58:21Z","forks":{"forkedRepos":[]},"full_name":"VolpeUSDOT/caco-parking-map","language":"JavaScript","languages":{"CSS":"1219","HTML":"2092","JavaScript":"1171179"},"organization":{"org_avatar_url":"https://avatars0.githubusercontent.com/u/29147435?v=4","org_type":"Organization","organization":"VolpeUSDOT","organization_url":"https://github.com/VolpeUSDOT"},"origin":"PUBLIC","project_description":"NPMap based application for parking management system developed by Volpe with the National Park Service Northeast Region and Cape Cod National Seashore.","project_name":"caco-parking-map","rank":52,"readMe":{"content":"# Cape Cod National Seashore Parking Management Web Map\nNPMap based application for parking management system developed by Volpe with the National Park Service Northeast Region and Cape Cod National Seashore.\n\nClient-side code for parking management system developed by Volpe with the National Park Service (NPS) Northeast Region and Cape Cod National Seashore. This parking management system keeps track of how many spaces are occupied/available at NPS beach parking lots on Cape Cod National Seashore and shares this information with the public. Tracking parking lot congestion helps Cape Cod visitors plan their trips and also helps park staff plan and allocate resources.\n\nThis project has two main components: a map created using the NPMap.js library, and a table as an alternative way of displaying the information.\n\nMap page: https://volpeusdot.github.io/caco-parking-map/CACOParking.html\n\nTable page: https://volpeusdot.github.io/caco-parking-map/StatusTable.html\n\n# Contributing\n[Contribution Guidance](./CONTRIBUTING.md)\n\n# License\n[License Information](./LICENSE)\n\n# Terms\n[Additional Terms](./TERMS.md)\n","url":"https://raw.githubusercontent.com/VolpeUSDOT/caco-parking-map/master/README.md"},"releases":[],"repository":"caco-parking-map","repository_url":"https://github.com/VolpeUSDOT/caco-parking-map","stage_id":"29147435_96899300","stars":1,"suggest":[{"input":["caco-parking-map"],"output":"caco-parking-map# name"},{"input":["caco-parking-map"],"output":"caco-parking-map# name"},{"input":["NPMap","based","application","for","parking","management","system","developed","by","Volpe","with","the","National","Park","Service","Northeast","Region","and","Cape","Cod","National","Seashore"],"output":"caco-parking-map# description"},{"input":["JavaScript","HTML","CSS"],"output":"caco-parking-map# languages"},{"input":["bsumne01","ldnash"],"output":"caco-parking-map# contributors"}],"updated_at":"2018-05-15T20:06:13Z","watchers":3}}
{"_index":"projects","_type":"project","_id":"29147435_93409853","_score":1,"_source":{"commits":26,"contributors":3,"contributors_list":[{"avatar_url":"https://avatars2.githubusercontent.com/u/8005360?v=4","profile_url":"https://github.com/ldnash","user_type":"User","username":"ldnash"},{"avatar_url":"https://avatars0.githubusercontent.com/u/29147842?v=4","profile_url":"https://github.com/szitzow-childs","user_type":"User","username":"szitzow-childs"},{"avatar_url":"https://avatars2.githubusercontent.com/u/18051014?v=4","profile_url":"https://github.com/aobergdc","user_type":"User","username":"aobergdc"}],"created_at":"2017-06-05T14:00:35Z","forks":{"forkedRepos":[]},"full_name":"VolpeUSDOT/gtfs-measures","language":"Python","languages":{"Python":"257355"},"organization":{"org_avatar_url":"https://avatars0.githubusercontent.com/u/29147435?v=4","org_type":"Organization","organization":"VolpeUSDOT","organization_url":"https://github.com/VolpeUSDOT"},"origin":"PUBLIC","project_description":"Source code for OST-P/Volpe project exploring estimation of segment-level transit ridership and other measures from GTFS feeds.","project_name":"gtfs-measures","rank":56,"readMe":{"content":"# GTFS for Estimating Transit Ridership and Supporting Multimodal Performance Measures\n\nThis repository contains the code that the U.S. DOT Volpe Center and Office of the U.S DOT Undersecretary for Policy developed for their proof-of-concept project exploring how General Transit Feed Specification (GTFS) public schedule data, widely available from transit agencies and now through the National Transit Map, could be used to estimate road segment-level transit ridership and support multimodal performance measures.\n\n## Link to Associated Report\n\nThe [full report](https://rosap.ntl.bts.gov/view/dot/34279) is available from the National Transportation Library.\n\n## Usage\nThis code estimates segment-level transit service characteristics (e.g. frequency, ridership at the level of individual road segments) based on GTFS feeds and other inputs, mapping the results spatially. For bus transit, it attaches these characteristics to the underlying road network for easy comparison to vehicular Average Annual Daily Traffic (AADT) data available from states, other local agencies, and the [Federal Highway Administration](https://www.fhwa.dot.gov/policyinformation/hpms.cfm). We encourage transit agencies, states, or other agencies to use, adapt, and contribute to this code if they find it useful in understanding and managing their transportation system.\n\n### Components\n- Modeling Scripts: Ingests GTFS, calibration ridership data, and predictive inputs to build a model for frequency and ridership.\n- Spatial Scripts: Prepares a segment-based network of roads that can be attached to transit service and estimated characteristics from the modeling scripts.\n\n## Code Dependencies\n- [Python](https://www.python.org/)\n- [ArcPy](http://pro.arcgis.com/en/pro-app/arcpy/get-started/what-is-arcpy-.htm) (part of ArcGIS)\n- [pygtfs](https://github.com/jarondl/pygtfs)\n- [sqlite3](https://www.sqlite.org/)\n\n## Data Dependencies\n- GTFS Data from the [U.S. DOT National Transit Map](https://www.rita.dot.gov/bts/ntm)\n- Federal Highway Administration's [All Roads Network of Linear Referenced Data](https://www.fhwa.dot.gov/policyinformation/hpms/arnold.cfm) (ARNOLD). (Note: the full geospatial data for this network is not yet available to the public. However, the project code could be modified to use state-specific roads data or other national roads data)\n- For ridership estimation: measured transit ridership data to calibrate the estimation model. While this project also used additional ridership data that transit agencies provided directly, the following agencies provide segment-level ridership as open data to the public:\n  - [Twin Cities Metro Transit](https://gisdata.mn.gov/dataset/us-mn-state-metc-trans-stop-boardings-alightings)\n  - [Bay Area Rapid Transit](https://www.bart.gov/about/reports/ridership)\n- For ridership estimation: Various data that may predict ridership from the U.S. Census and other national inputs as described in this project's full report. Code is designed to be modified to allow for other predictive datasets to be used.\n\n\n## Documentation\nThis code is primarily documented through in-line comments, with some additional documentation listed below:\n- [GTFS to Road Network (ARNOLD) Snapping](https://github.com/VolpeUSDOT/gtfs-measures/blob/master/docs/GTFS_Script_Documentation.md)\n- [Ridership Modeling](https://github.com/VolpeUSDOT/gtfs-measures/blob/master/docs/GTFS_Model_Scripts_Documentation.md)\n\n## Getting involved\nThis public-domain code was developed as part of a proof-of-concept project, and we encourage transit agencies, state DOTs, or other interested parties to consider how these tools to estimate ridership and calculate multimodal performance measures might be useful to them. We also welcome contributions back into this repository as a building block for peers. See [LICENSE](LICENSE),  [CONTRIBUTING](CONTRIBUTING.md), and [TERMS](TERMS.md).\n\nIf you have questions about the code or the project, feel free to open an Issue on this repository or reach out to the contacts noted in the full report linked above.\n\n## Open source licensing info\n1. [TERMS](TERMS.md)\n2. [LICENSE](LICENSE)\n3. [U.S. Government Source Code Policy](https://sourcecode.cio.gov/)\n","url":"https://raw.githubusercontent.com/VolpeUSDOT/gtfs-measures/master/README.md"},"releases":[],"repository":"gtfs-measures","repository_url":"https://github.com/VolpeUSDOT/gtfs-measures","stage_id":"29147435_93409853","stars":1,"suggest":[{"input":["gtfs-measures"],"output":"gtfs-measures# name"},{"input":["gtfs-measures"],"output":"gtfs-measures# name"},{"input":["Source","code","for","OST-PVolpe","project","exploring","estimation","of","segment-level","transit","ridership","and","other","measures","from","GTFS","feeds"],"output":"gtfs-measures# description"},{"input":["Python"],"output":"gtfs-measures# languages"},{"input":["ldnash","szitzow-childs","aobergdc"],"output":"gtfs-measures# contributors"}],"updated_at":"2018-01-25T21:25:45Z","watchers":3}}
{"_index":"projects","_type":"project","_id":"23056647_72044729","_score":1,"_source":{"commits":2604,"contributors":9,"contributors_list":[{"user_type":"User","avatar_url":"https://avatars0.githubusercontent.com/u/12912578?v=4","profile_url":"https://github.com/mvs5465","username":"mvs5465"},{"user_type":"User","avatar_url":"https://avatars2.githubusercontent.com/u/10130982?v=4","profile_url":"https://github.com/hmusavi","username":"hmusavi"},{"user_type":"User","avatar_url":"https://avatars0.githubusercontent.com/u/23481864?v=4","profile_url":"https://github.com/mgarramo","username":"mgarramo"},{"user_type":"User","avatar_url":"https://avatars0.githubusercontent.com/u/9087336?v=4","profile_url":"https://github.com/0111sandesh","username":"0111sandesh"},{"user_type":"User","avatar_url":"https://avatars0.githubusercontent.com/u/549261?v=4","profile_url":"https://github.com/tonychen091","username":"tonychen091"},{"user_type":"User","avatar_url":"https://avatars0.githubusercontent.com/u/29639608?v=4","profile_url":"https://github.com/levesque1","username":"levesque1"},{"user_type":"User","avatar_url":"https://avatars2.githubusercontent.com/u/5840989?v=4","profile_url":"https://github.com/lauraGgit","username":"lauraGgit"},{"user_type":"User","avatar_url":"https://avatars2.githubusercontent.com/u/23216443?v=4","profile_url":"https://github.com/ToryB1","username":"ToryB1"},{"user_type":"User","avatar_url":"https://avatars2.githubusercontent.com/u/1664694?v=4","profile_url":"https://github.com/southernsun","username":"southernsun"}],"created_at":"2016-10-26T21:10:46Z","forks":{"forkedRepos":[{"name":"jpo-ode","id":"21817814_164044174","org_name":"kijeongeun"},{"name":"jpo-ode","id":"42840583_152122560","org_name":"darrelld05"},{"name":"jpo-ode","id":"7475771_145814291","org_name":"kssonu4u"},{"name":"jpo-ode","id":"38541284_130270574","org_name":"MarioDH"},{"name":"jpo-ode","id":"32356764_126688467","org_name":"yangsoso"},{"name":"jpo-ode","id":"1860862_123641545","org_name":"Edon07"},{"name":"jpo-ode","id":"1916753_120336776","org_name":"onthejeep"},{"name":"jpo-ode","id":"33533793_118946510","org_name":"OSUPCVLab"},{"name":"jpo-ode","id":"13427284_116835740","org_name":"wayties"},{"name":"jpo-ode","id":"28535848_94336819","org_name":"DOTAMC"},{"name":"jpo-ode","id":"1120608_90075506","org_name":"kimPerry"},{"name":"jpo-ode","id":"11353428_86829378","org_name":"sharafm2002"},{"name":"jpo-ode","id":"12912578_76863396","org_name":"mvs5465"},{"name":"jpo-ode","id":"10130982_75348929","org_name":"hmusavi"},{"name":"jpo-ode","id":"23481864_75121569","org_name":"mgarramo"},{"name":"jpo-ode","id":"5840989_75118639","org_name":"lauraGgit"}]},"full_name":"usdot-jpo-ode/jpo-ode","language":"Java","languages":{"Batchfile":"1803","C":"4075","CSS":"1617","Dockerfile":"1266","HTML":"4849","Java":"2102097","JavaScript":"3219","Makefile":"983","Shell":"4186"},"organization":{"org_avatar_url":"https://avatars2.githubusercontent.com/u/23056647?v=4","org_type":"Organization","organization":"usdot-jpo-ode","organization_url":"https://github.com/usdot-jpo-ode"},"origin":"PUBLIC","project_description":"US Department of Transportation (USDOT) Intelligent Transportation Systems Operational Data Environment (ITS ODE). This is the main repository that integrates and coordinates ODE Submodules.","project_name":"jpo-ode","rank":2815,"readMe":{"content":"\n\nMaster: [![Build Status](https://travis-ci.org/usdot-jpo-ode/jpo-ode.svg?branch=master)](https://travis-ci.org/usdot-jpo-ode/jpo-ode) [![Quality Gate](https://sonarcloud.io/api/badges/gate?key=usdot.jpo.ode:jpo-ode)](https://sonarcloud.io/dashboard?id=usdot.jpo.ode%3Ajpo-ode)\n\nDevelop: [![Build Status](https://travis-ci.org/usdot-jpo-ode/jpo-ode.svg?branch=develop)](https://travis-ci.org/usdot-jpo-ode/jpo-ode) [![Quality Gate](https://sonarcloud.io/api/badges/gate?key=usdot.jpo.ode:jpo-ode:develop)](https://sonarcloud.io/dashboard?id=usdot.jpo.ode%3Ajpo-ode%3Adevelop)\n\n# jpo-ode\nUS Department of Transportation (USDOT) Intelligent Transportation Systems Operational Data Environment (ITS ODE)\n\nThe ITS ODE is a real-time virtual data router that ingests and processes operational data from various connected devices - including vehicles, infrastructure, and traffic management centers - and distributes it to other devices and subscribing transportation management applications. Using the ITS ODE within intelligent transportation deployments increases data fluidity and interoperability while meeting operational needs and protecting user privacy. The software’s microservices architecture makes it easy to add new capabilities to meet local needs.\n\n![ODE Dataflows](images/data_flow_v2.png)\n\n<a name=\"toc\"/>\n\n## Table of Contents\n\n[I. Release Notes](#release-notes)\n\n[II. Documentation](#documentation)\n\n[III. Collaboration Tools](#collaboration-tools)\n\n[IV. Quickstart Guide](#quickstart-guide)\n\n[V. Running the Application](#running)\n\n[VI. ODE Limitation](#dev-tools)\n\n[VII. Development Tools](#dev-tools)\n\n[VIII. Contribution Information](#contribution-info)\n\n[IX. Troubleshooting](#troubleshooting)\n\n---\n\n<a name=\"release-notes\"/>\n\n\n## [I. Release Notes](ReleaseNotes.md)\n\n\n<a name=\"documentation\"/>\n\n## II. Documentation\nODE provides the following living documents to keep ODE users and stakeholders informed of the latest developments:\n\n1. [ODE Architecture](docs/JPO%20ODE%20Architecture.docx)\n2. [ODE User Guide](docs/JPO_ODE_UserGuide.docx)\n3. [ODE Output Schema Reference Guide](docs/ODE_Output_Schema_Reference.docx)\n4. [ODE REST API Guide](https://usdot-jpo-ode.github.io/)\n5. [ODE Smoke Tests](https://github.com/usdot-jpo-ode/jpo-ode/wiki/JPO-ODE-QA-Documents)\n\nAll stakeholders are invited to provide input to these documents. Stakeholders should direct all input on this document to the JPO Product Owner at DOT, FHWA, and JPO. To provide feedback, we recommend that you create an \"issue\" in this repository (https://github.com/usdot-jpo-ode/jpo-ode/issues). You will need a GitHub account to create an issue. If you don’t have an account, a dialog will be presented to you to create one at no cost.\n\n<a name=\"collaboration-tools\"/>\n\n## III. Collaboration Tools\n\n### Source Repositories - GitHub\n\n- Main repository on GitHub (public)\n\t- https://github.com/usdot-jpo-ode/jpo-ode\n\t- git@github.com:usdot-jpo-ode/jpo-ode.git\n- Data Privacy Module on Github (public)\n\t- https://github.com/usdot-jpo-ode/jpo-cvdp\n\t- git@github.com/usdot-jpo-ode/jpo-cvdp\n- S3 Depositor Module on Github (public)\n\t- https://github.com/usdot-jpo-ode/jpo-s3-deposit\n\t- gith@github.com/usdot-jpo-ode/jpo-s3-deposit\n- Security services repository on GitHub (public)\n        - https://github.com/usdot-jpo-ode/jpo-security-svcs.git\n\t- git@github.com:usdot-jpo-ode/jpo-security-svcs.git\n\n### Agile Project Management - Jira\nhttps://usdotjpoode.atlassian.net/secure/RapidBoard.jspa?projectKey=ODE\n\n### Wiki - Confluence\nhttps://usdotjpoode.atlassian.net/wiki/\n\n### Continuous Integration and Delivery\nhttps://travis-ci.org/usdot-jpo-ode/jpo-ode\n\n<details><summary>Using Travis for your build</summary>\n<br>\nTo allow Travis run your build when you push your changes to your public fork of the jpo-ode repository, you must define the following secure environment variable using Travis CLI (https://github.com/travis-ci/travis.rb).\n\nRun:\n\n```\ntravis login --org\n```\nEnter personal github account credentials.\n\nIn order to allow Sonar to run, personal key must be added with this command:\n(Key can be obtained from the JPO-ODE development team)\n\n```\ntravis env set SONAR_SECURITY_TOKEN <key> -pr <user-account>/<repo-name>\n```\n</details>\n<br>\n\n### Static Code Analysis\nhttps://sonarcloud.io/organizations/usdot-jpo-ode/projects\n\n[Back to top](#toc)\n\n<a name=\"quickstart-guide\"/>\n\n## IV. Quickstart Guide\n\nThe following instructions describe the minimal procedure to fetch, build, and run the main ODE application. If you want to use the privacy protection module and/or S3 depositors, see the [extended features](#extended-features) section. Additionally, different build processes are covered in the extended features section.\n\nSome notes before you begin:\n* If you are installing the ODE in an Ubuntu environment, see this [preparation guide](https://github.com/usdot-jpo-ode/jpo-ode/wiki/Prepare-a-fresh-Ubuntu-instance-for-ODE-installation) that covers installing all of the prerequisites.\n* Docker builds may fail if you are on a corporate network due to DNS resolution errors. \n[See here](https://github.com/usdot-jpo-ode/jpo-ode/wiki/Docker-fix-for-SSL-issues-due-to-corporate-network) for instructions to fix this.\n* Additionally *git* commands may fail for similar reasons, you can fix this by running `export GIT_SSL_NO_VERIFY=1`.\n* Windows users may find more information on installing and using Docker [here](https://github.com/usdot-jpo-ode/jpo-ode/wiki/Docker-management).\n* Users interested in Kafka may find more guidance and configuration options [here](docker/kafka/README.md).\n\n### Prerequisites\n* JDK 1.8: http://www.oracle.com/technetwork/pt/java/javase/downloads/jdk8-downloads-2133151.html\n* Maven: https://maven.apache.org/install.html\n* Git: https://git-scm.com/\n* Docker: https://docs.docker.com/engine/installation/\n* Docker-Compose: https://docs.docker.com/compose/install/\n\nRead the following guides to familiarize yourself with ODE's Docker and Kafka modules.\n\n**Docker**\n\n[README](docker.md)\n\n**Kafka**\n\n[README](kafka.md)\n\n---\n### Obtain the Source Code\n\n#### Step 0 - For Windows Users Only\nIf running on Windows, please make sure that your global git config is set up to not convert End-of-Line characters during checkout. \n\nDisable `git core.autocrlf` (One Time Only)\n\n```bash\ngit config --global core.autocrlf false\n```\n\n#### Step 1 - Download the Source Code\nODE software consists of the following modules hosted on GitHub:\n\n|Name|Visibility|Description|\n|----|----------|-----------|\n|[jpo-ode](https://github.com/usdot-jpo-ode/jpo-ode)|public|Contains the public components of the application code.|\n|[jpo-cvdp](https://github.com/usdot-jpo-ode/jpo-cvdp)|public|Privacy Protection Module|\n|[jpo-s3-deposit](https://github.com/usdot-jpo-ode/jpo-s3-deposit)|public|S3 depositor service. Optional, comment out of `docker-compose.yml` file if not used.|\n|[asn1_codec](https://github.com/usdot-jpo-ode/asn1_codec)|public|ASN.1 Encoder/Decoder module|\n|[jpo-security-svcs](https://github.com/usdot-jpo-ode/jpo-security-svcs)|public|Provides cryptographic services.|\n\nTo download the stable (default branch) source code for the first time, clone the repositories by running the following commands:\n\n```bash\ngit clone --recurse-submodules https://github.com/usdot-jpo-ode/jpo-ode.git\n```\n*Note*: Make sure you specify the --recurse-submodules option on the clone command line. This option will cause the cloning of all dependent submodules:\n- Privacy Protection Module (PPM) - [jpo-cvdp](https://github.com/usdot-jpo-ode/jpo-cvdp)\n- S3 Bucket Depositor - [jpo-s3-deposit](https://github.com/usdot-jpo-ode/jpo-s3-deposit)\n- Security Services Module- [jpo-security](https://github.com/usdot-jpo-ode/jpo-security-svcs)\n- ASN.1 CODEC - [asn1_codec](https://github.com/usdot-jpo-ode/asn1_codec)\n\nOnce you have these repositories obtained, you are ready to build and deploy the application.\n\n##### Downloading the source code from a non-default branch\nThe above steps to pull the code from GitHub repository pulls it from the default branch which is the stable branch. If you wish to pull the source code from a branch that is still under development or beta testing, you will need to specify the branch to pull from. The following commands aid you in that action.\n```\n# Backup user provided source or configuration files used by submodules\ncp asn1_codec/asn1c_combined/J2735_201603DA.ASN .\n\n#Run the following commands to reset existing branch\ngit reset --hard\ngit submodule foreach --recursive git reset --hard\n\n# Pull from the non-default branch\ngit checkout <branch_name>\ngit pull origin <branch_name>\n\n# The next command wipes out all of the submodules and re-initializes them. \ngit submodule deinit -f . && git submodule update --recursive --init\n\n# Restore user provided source or configuration files used by submodules\ncp ./J2735_201603DA.ASN asn1_codec/asn1c_combined/\n```\n**Note**: *These commands can also be performed using the provided script `update_branch`.*\n\n---\n### Build and Deploy the Application\n\nODE uses Docker for building and running the executable. Familiarize yourself with Docker and follow the instructions in the [README.md](docker/README.md).\n\n#### Step 1: Configuration\nIf you wish to change the application properties, such as change the location of the upload service via `ode.uploadLocation.*` properties or set the `ode.kafkaBrokers` to something other than the $DOCKER_HOST_IP:9092, or wish to set the CAS username/password, `ODE_EXTERNAL_IPVs`, etc. instead of setting the environment variables, modify `jpo-ode-svcs\\src\\main\\resources\\application.properties` file as desired.\n\nODE configuration can be customized for every deployment environment using environment variables. These variables can either be set locally or using the *sample.env* file found in the root of the `jpo-ode` repository.\n\nInstructions for how to use the *sample.env* file can be found [here](https://github.com/usdot-jpo-ode/jpo-ode/wiki/Using-the-.env-configuration-file).\n\n**Important!** \nYou must rename `sample.env` to `.env` for Docker to automatically read the file. This file will contain AWS access keys and other private information. Do not push this file to source control.\n\n#### Build Process\n\n**Note** Docker builds may fail if you are on a corporate network due to DNS resolution errors. \n[See here](https://github.com/usdot-jpo-ode/jpo-ode/wiki/Docker-fix-for-SSL-issues-due-to-corporate-network) for instructions to fix this.\n\n**Note** In order for Docker to automatically read the environment variable file, you must rename it from `sample.env` to `.env`.\n\n#### Step 2: Build and deploy the application.\n\nCopy the following files from `jpo-ode` directory into your DOCKER_SHARED_VOLUME directory.\n- Copy jpo-ode/ppm.properties to ${DOCKER_SHARED_VOLUME}/config.properties. Open the newly copied `config.properties` file in a text editor and update the `metadata.broker.list=your.docker.host.ip:9092` line with your system's DOCKER_HOST_IP in place of the dummy `your.docker.host.ip` string. \n- Copy jpo-ode/adm.properties to ${DOCKER_SHARED_VOLUME}/adm.properties\n- Copy jpo-ode/aem.properties to ${DOCKER_SHARED_VOLUME}/aem.properties\n\nNavigate to the root directory of the jpo-ode project and run the following command:\n```\ndocker-compose up --build -d\ndocker-compose ps\n```\nTo bring down the services and remove the running containers run the following command:\n```\ndocker-compose down\n```\nFor a fresh restart, run:\n```\ndocker-compose down\ndocker-compose up --build -d\ndocker-compose ps\n```\nCheck the deployment by running `docker-compose ps`. You can start and stop containers using `docker-compose start` and `docker-compose stop` commands.\nIf using the multi-broker docker-compose file, you can change the scaling by running `docker-compose scale <container>=n` where container is the container you would like to scale and n is the number of instances. For example, `docker-compose scale kafka=3`.\n\n#### Running ODE Application on localhost\nYou can run the application on your local machine while other services are deployed on a host environment. To do so, run the following:\n```bash\n docker-compose start zookeeper kafka\n mvn clean install\n java -jar jpo-ode-svcs/target/jpo-ode-svcs-0.0.1-SNAPSHOT.jar\n```\n\n[Back to top](#toc)\n\n<a name=\"running\"/>\n\n## V. Using ODE Application\nOnce the ODE is deployed and running, you should be able to access the `jpo-ode` web UI at `localhost:8080`.\n\n1. Press the `Connect` button to connect to the ODE WebSocket service.\n2. Press `Choose File` button to select an OBU log file containing BSMs and/or TIM messages as specified by the WYDOT CV Pilot project. See below documents for details:\na. [Wyoming CV Pilot Log File Design](data/Wyoming_CV_Pilot_Log_File_Design.docx) \nb. [WYDOT Log Records](data/wydotLogRecords.h) \n3. Press `Upload` button to upload the file to ODE.\n\nUpload records within the files must be embedding BSM and/or TIM messages wrapped in J2735 MessageFrame and ASN.1 UPER encoded, wrapped in IEEE 1609.2 envelope and ASN.1 COER encoded binary format. The following files are a samples of each supported type. Uploading any of the files below will you will observe the decoded messages returned to the web UI page while connected to the WebSocket interface:\n\n - [data/bsmLogDuringEvent.bin](data/bsmLogDuringEvent.bin)\n - [data/bsmLogDuringEvent.gz](data/bsmLogDuringEvent.gz)\n - [data/bsmTx.bin](data/bsmTx.bin)\n - [data/bsmTx.gz](data/bsmTx.gz)\n - [data/dnMsg.bin](data/dnMsg.bin)\n - [data/dnMsg.gz](data/dnMsg.gz)\n - [data/rxMsg_BSM.bin](data/rxMsg_BSM.bin)\n - [data/rxMsg_BSM.gz](data/rxMsg_BSM.gz)\n - [data/rxMsg_TIM.bin](data/rxMsg_TIM.bin)\n - [data/rxMsg_TIM.gz](data/rxMsg_TIM.gz)\n\nAnother way data can be uploaded to the ODE is through copying the file to the location specified by the `ode.uploadLocationRoot/ode.uploadLocationObuLog`property. If not specified,  Default locations would be `uploads/bsmlog`sub-directory off of the location where ODE is launched.\n\nThe result of uploading and decoding of messages will be displayed on the UI screen.\n\n![ODE UI](images/ode-ui.png)\n\n*Notice that the empty fields in the J2735 message are represented by a `null` value. Also note that ODE output strips the MessageFrame header and returns a pure BSM or TIM in the subscription topic.*\n\n### asn1_codec Module (ASN.1 Encoder and Decoder)\nODE requires the deployment of asn1_codec module. ODE's `docker-compose.yml` file is set up to build and deploy the module in a Docker container. If you wish to run `asn1_codec` module outside Docker (i.e. directly on the host machine), please refer to the documentation of `asn1_codec` module.\n\nThe only requirement for deploying `asn1_codec` module on Docker is the setup of two environment variables `DOCKER_HOST_IP` and `DOCKER_SHARED_VOLUME`.\n\n### PPM Module (Geofencing and Filtering)\n\nTo run the ODE with PPM module, you must install and start the PPM service. PPM service communicates with other services through Kafka Topics. PPM will read from the specified \"Raw BSM\" topic and publish the result to the specified \"Filtered Bsm\" topic. These topic names are specified by the following ODE and PPM properties:\n\n - ODE properties for communications with PPM (set in application.properties)\n\t - ode.kafkaTopicOdeBsmJson  (default = topic.OdeBsmJson)\n\t - ode.kafkaTopicFilteredOdeBsmJson (default = topic.FilteredOdeBsmJson)\n - PPM properties for communications with ODE (set in yourconfig.properties)\n\t - privacy.topic.consumer (default = j2735BsmRawJson)\n\t - privacy.topic.producer (default = j2735BsmFilteredJson)\n\nFollow the instructions [here](https://github.com/usdot-jpo-ode/jpo-cvdp/blob/master/docs/installation.md) (https://github.com/usdot-jpo-ode/jpo-cvdp/blob/master/docs/installation.md) to install and build the PPM service.\n\nDuring the build process, edit the sample config file located in `config/example.properties` and point the property `metadata.broker.list` towards the host of your docker machine or wherever the kafka brokers are hosted. You may use the command `docker-machine ls` to find the kafka service.\n\nAfter a successful build, use the following commands to configure and run the PPM\n\n```\ncd $BASE_PPM_DIR/jpo-cvdp/build\n$ ./bsmjson_privacy -c ../config/ppm.properties\n```\nWith the PPM module running, all filtered BSMs that are uploaded through the web interface will be captured and processed. You will see an output of both submitted BSM and processed data unless the entire record was filtered out.\n\n![PPM](images/PPM.png)\n\n\n[Back to top](#toc)\n\n<a name=\"ode-limitation\"/>\n\n## VI. ODE Limitations\n\nDate: 07/2017\n\nIn its current state, the ODE has been developed to accomplish the goals of data transfer, security, and modularity working with the J2735 and 1609.2 security. The system has been designed to support multiple services orchestrated through the Apache Kafka streaming data pipelines, services built and supported as separate applications and described with each service's repository. As a modular system, each component has been built for functionality first, and additional performance testing is needed to understand the limits of the system with large volumes of data.\n\n<a name=\"dev-tools\"/>\n\n## VII. Development Tools\n\n### Integrated Development Environment (IDE)\n\nInstall the IDE of your choice:\n\n* Eclipse: [https://eclipse.org/](https://eclipse.org/)\n* STS: [https://spring.io/tools/sts/all](https://spring.io/tools/sts/all)\n* IntelliJ: [https://www.jetbrains.com/idea/](https://www.jetbrains.com/idea/)\n\n### Continuous Integration\n\n* TravisCI: https://travis-ci.org/usdot-jpo-ode/jpo-ode\n\n### Continous Deployment\n\nTo be added.\n\n<a name=\"contribution-info\"/>\n\n## VIII. Contribution Information\n\nPlease read our [contributing guide](docs/contributing_guide.md) to learn about our development process, how to propose pull requests and improvements, and how to build and test your changes to this project.\n\n<a name=\"troubleshooting\"/>\n\n## IX. Troubleshooting\n\nPlease read our [Wiki](https://github.com/usdot-jpo-ode/jpo-ode/wiki) for more information, or check the [ODE User Guide](https://github.com/usdot-jpo-ode/jpo-ode/raw/develop/docs/JPO_ODE_UserGuide.docx).\n\n[Back to top](#toc)\n","url":"https://raw.githubusercontent.com/usdot-jpo-ode/jpo-ode/master/README.md"},"releases":[],"repository":"jpo-ode","repository_url":"https://github.com/usdot-jpo-ode/jpo-ode","stage_id":"23056647_72044729","stars":30,"suggest":[{"output":"jpo-ode# name","input":["jpo-ode"]},{"output":"jpo-ode# name","input":["jpo-ode"]},{"output":"jpo-ode# description","input":["US","Department","of","Transportation","USDOT","Intelligent","Transportation","Systems","Operational","Data","Environment","ITS","ODE","This","is","the","main","repository","that","integrates","and","coordinates","ODE","Submodules"]},{"output":"jpo-ode# languages","input":["JavaScript","Shell","CSS","Batchfile","Makefile","C","HTML","Java","Dockerfile"]},{"output":"jpo-ode# contributors","input":["mvs5465","hmusavi","mgarramo","0111sandesh","tonychen091","levesque1","lauraGgit","ToryB1","southernsun"]}],"updated_at":"2019-01-09T22:08:58Z","watchers":19}}
{"_index":"projects","_type":"project","_id":"23056647_99942371","_score":1,"_source":{"commits":162,"contributors":5,"contributors_list":[{"avatar_url":"https://avatars0.githubusercontent.com/u/1124162?v=4","profile_url":"https://github.com/jmcarter9t","user_type":"User","username":"jmcarter9t"},{"avatar_url":"https://avatars2.githubusercontent.com/u/10130982?v=4","profile_url":"https://github.com/hmusavi","user_type":"User","username":"hmusavi"},{"avatar_url":"https://avatars3.githubusercontent.com/u/1867490?v=4","profile_url":"https://github.com/aferber","user_type":"User","username":"aferber"},{"avatar_url":"https://avatars0.githubusercontent.com/u/12912578?v=4","profile_url":"https://github.com/mvs5465","user_type":"User","username":"mvs5465"},{"avatar_url":"https://avatars0.githubusercontent.com/u/549261?v=4","profile_url":"https://github.com/tonychen091","user_type":"User","username":"tonychen091"}],"created_at":"2017-08-10T16:15:03Z","forks":{"forkedRepos":[{"id":"42840583_152478479","name":"asn1_codec","org_name":"darrelld05"},{"id":"22564381_151737039","name":"asn1_codec","org_name":"ttitamu"},{"id":"39533265_135323692","name":"asn1_codec","org_name":"huemoua"},{"id":"1860862_123641564","name":"asn1_codec","org_name":"Edon07"},{"id":"13427284_116835682","name":"asn1_codec","org_name":"wayties"}]},"full_name":"usdot-jpo-ode/asn1_codec","language":"C++","languages":{"Batchfile":"332","C":"30710","C++":"1447521","CMake":"6941","Dockerfile":"2179","Python":"1175","Shell":"5793"},"organization":{"org_avatar_url":"https://avatars2.githubusercontent.com/u/23056647?v=4","org_type":"Organization","organization":"usdot-jpo-ode","organization_url":"https://github.com/usdot-jpo-ode"},"origin":"PUBLIC","project_description":"Module to encode and decode ASN.1 streams of messages using Kafka messaging hub for communication with the data source and data destination in a pub/sub scheme.","project_name":"asn1_codec","rank":222,"readMe":{"content":"# Abstract Syntax Notation One (ASN.1) Codec Module for the Operational Data Environment (ODE)\n\nThe ASN.1 Codec Module (ACM) processes Kafka data streams that preset [ODE\nMetadata](https://github.com/usdot-jpo-ode/jpo-ode/blob/develop/docs/metadata_standards.md) wrapped ASN.1 data.  It can perform\none of two functions depending on how it is started:\n\n1. **Decode**: This function is used to process messages *from* the connected\nvehicle environment *to* ODE subscribers. Specifically, the ACM extacts binary\ndata from consumed messages (ODE Metatdata Messages) and decodes the binary\nASN.1 data into a structure that is subsequently encoded into an alternative\nformat more suitable for ODE subscribers (currently XML using XER).\n\n1. **Encode**: This function is used to process messages *from* the ODE *to*\nthe connected vehicle environment. Specifically, the ACM extracts\nhuman-readable data from ODE Metadata and decodes it into a structure that\nis subsequently *encoded into ASN.1 binary data*.\n\n![ASN.1 Codec Operations](docs/graphics/asn1codec-operations.png)\n\n# Table of Contents\n\n1. [Release Notes](#release-notes)\n1. [Getting Involved](#getting-involved)\n1. [Documentation](#documentation)\n1. [Installation](docs/installation.md)\n1. [Configuration and Operation](docs/configuration.md)\n1. [Interface](docs/interface.md)\n1. [Testing](docs/testing.md)\n1. [Project Management](#project-management)\n\n# Release Notes\n\n- ODE-537/581/584/585/586/537: ASN.1 codec module development and integration. \n\n# Getting Involved\n\nThis project is sponsored by the U.S. Department of Transportation and supports Operational Data Environment data type\nconversions. Here are some ways you can start getting involved in this project:\n\n- **Pull the code and check it out!**  The ASN.1 Codec project uses the [Pull Request Model](https://help.github.com/articles/using-pull-requests).\n    - Github has [instructions](https://help.github.com/articles/signing-up-for-a-new-github-account) for setting up an account and getting started with repositories.\n- If you would like to improve this code base or the documentation, [fork the project](https://github.com/usdot-jpo-ode/asn1_codec#fork-destination-box) and submit a pull request.\n- If you find a problem with the code or the documentation, please submit an [issue](https://github.com/usdot-jpo-ode/asn1_codec/issues/new).\n\n## Introduction\n\nThis project uses the [Pull Request Model](https://help.github.com/articles/using-pull-requests). This involves the following project components:\n\n- The usdot-jpo-ode organization project's [master branch](https://github.com/usdot-jpo-ode/asn1_codec).\n- A personal GitHub account.\n- A fork of a project release tag or master branch in your personal GitHub account.\n\nA high level overview of our model and these components is as follows. All work will be submitted via pull requests.\nDevelopers will work on branches on their personal machines (local clients), push these branches to their **personal GitHub repos** and issue a pull\nrequest to the organization asn1_codec project. One the project's main developers must review the Pull Request and merge it\nor, if there are issues, discuss them with the submitter. This will ensure that the developers have a better\nunderstanding of the code base *and* we catch problems before they enter `master`. The following process should be followed:\n\n## Initial Setup\n\n1. If you do not have one yet, create a personal (or organization) account on GitHub (assume your account name is `<your-github-account-name>`).\n1. Log into your personal (or organization) account.\n1. Fork [asn1_codec](https://github.com/usdot-jpo-ode/asn1_codec/fork) into your personal GitHub account.\n1. On your computer (local client), clone the master branch from you GitHub account:\n```bash\n$ git clone https://github.com/<your-github-account-name>/asn1_codec.git\n```\n\n## Additional Resources for Initial Setup\n  \n- [About Git Version Control](http://git-scm.com/book/en/v2/Getting-Started-About-Version-Control)\n- [First-Time Git Setup](http://git-scm.com/book/en/Getting-Started-First-Time-Git-Setup)\n- [Article on Forking](https://help.github.com/articles/fork-a-repo)\n\n# Documentation\n\nThis documentation is in the `README.md` file. Additional information can be found using the links in the [Table of\nContents](#table-of-contents).  All stakeholders are invited to provide input to these documents. Stakeholders should\ndirect all input on this document to the JPO Product Owner at DOT, FHWA, or JPO. \n\n## Code Documentation\n\nCode documentation can be generated using [Doxygen](https://www.doxygen.org) by following the commands below:\n\n```bash\n$ sudo apt install doxygen\n$ cd <install root>/asn1_codec\n$ doxygen\n```\n\nThe documentation is in HTML and is written to the `<install root>/asn1_codec/docs/html` directory. Open `index.html` in a\nbrowser.  \n\n## Project Management\n\nThis project is managed using the Jira tool.\n\n- [Jira Project Portal](https://usdotjpoode.atlassian.net/secure/Dashboard.jsp)\n\n","url":"https://raw.githubusercontent.com/usdot-jpo-ode/asn1_codec/master/README.md"},"releases":[],"repository":"asn1_codec","repository_url":"https://github.com/usdot-jpo-ode/asn1_codec","stage_id":"23056647_99942371","stars":1,"suggest":[{"input":["asn1_codec"],"output":"asn1_codec# name"},{"input":["asn1_codec"],"output":"asn1_codec# name"},{"input":["Module","to","encode","and","decode","ASN1","streams","of","messages","using","Kafka","messaging","hub","for","communication","with","the","data","source","and","data","destination","in","a","pubsub","scheme"],"output":"asn1_codec# description"},{"input":["Shell","Batchfile","Python","C","C++","CMake","Dockerfile"],"output":"asn1_codec# languages"},{"input":["jmcarter9t","hmusavi","aferber","mvs5465","tonychen091"],"output":"asn1_codec# contributors"}],"updated_at":"2018-12-20T18:20:43Z","watchers":8}}
{"_index":"projects","_type":"project","_id":"23056647_95674278","_score":1,"_source":{"commits":43,"contributors":3,"contributors_list":[{"avatar_url":"https://avatars0.githubusercontent.com/u/12912578?v=4","profile_url":"https://github.com/mvs5465","user_type":"User","username":"mvs5465"},{"avatar_url":"https://avatars2.githubusercontent.com/u/10130982?v=4","profile_url":"https://github.com/hmusavi","user_type":"User","username":"hmusavi"},{"avatar_url":"https://avatars0.githubusercontent.com/u/9087336?v=4","profile_url":"https://github.com/0111sandesh","user_type":"User","username":"0111sandesh"}],"created_at":"2017-06-28T13:53:03Z","forks":{"forkedRepos":[{"id":"1860862_123641582","name":"jpo-security","org_name":"Edon07"},{"id":"13427284_116835861","name":"jpo-security","org_name":"wayties"}]},"full_name":"usdot-jpo-ode/jpo-security","language":"Java","languages":{"Batchfile":"41","CSS":"1854","Java":"207260","Shell":"28"},"organization":{"org_avatar_url":"https://avatars2.githubusercontent.com/u/23056647?v=4","org_type":"Organization","organization":"usdot-jpo-ode","organization_url":"https://github.com/usdot-jpo-ode"},"origin":"PUBLIC","project_description":"SUBMODULE: 1609.2 Java Security Library for handling security functions of the SCMS. Provided by Leidos for reuse in JPO projects.","project_name":"jpo-security","rank":107,"readMe":{"content":"#  US DOT 1609.2 Security Library\nThis module contains the Java API for handling security functions required by the US DOT Connected Vehicles (CV) Security Credentials Management System (SCMS). This code has been provided by Leidos for reuse in JPO projects. \n\nClick [here](Notes_on_1609.2_Security_Library_Update_6-2-17.pdf) to see the [Release Notes](Notes_on_1609.2_Security_Library_Update_6-2-17.pdf).","url":"https://raw.githubusercontent.com/usdot-jpo-ode/jpo-security/master/README.md"},"releases":[],"repository":"jpo-security","repository_url":"https://github.com/usdot-jpo-ode/jpo-security","stage_id":"23056647_95674278","stars":3,"suggest":[{"input":["jpo-security"],"output":"jpo-security# name"},{"input":["jpo-security"],"output":"jpo-security# name"},{"input":["SUBMODULE","16092","Java","Security","Library","for","handling","security","functions","of","the","SCMS","Provided","by","Leidos","for","reuse","in","JPO","projects"],"output":"jpo-security# description"},{"input":["Java","CSS","Batchfile","Shell"],"output":"jpo-security# languages"},{"input":["mvs5465","hmusavi","0111sandesh"],"output":"jpo-security# contributors"}],"updated_at":"2018-12-04T19:54:27Z","watchers":10}}
{"_index":"projects","_type":"project","_id":"23056647_77352013","_score":1,"_source":{"commits":119,"contributors":26,"contributors_list":[{"avatar_url":"https://avatars1.githubusercontent.com/u/4301434?v=4","profile_url":"https://github.com/wurstmeister","user_type":"User","username":"wurstmeister"},{"avatar_url":"https://avatars1.githubusercontent.com/u/575245?v=4","profile_url":"https://github.com/varju","user_type":"User","username":"varju"},{"avatar_url":"https://avatars0.githubusercontent.com/u/37565?v=4","profile_url":"https://github.com/eliasdorneles","user_type":"User","username":"eliasdorneles"},{"avatar_url":"https://avatars1.githubusercontent.com/u/4031715?v=4","profile_url":"https://github.com/prabhuinbarajan","user_type":"User","username":"prabhuinbarajan"},{"avatar_url":"https://avatars2.githubusercontent.com/u/3822?v=4","profile_url":"https://github.com/filiptepper","user_type":"User","username":"filiptepper"},{"avatar_url":"https://avatars0.githubusercontent.com/u/89186?v=4","profile_url":"https://github.com/bobrik","user_type":"User","username":"bobrik"},{"avatar_url":"https://avatars1.githubusercontent.com/u/106903?v=4","profile_url":"https://github.com/mdlavin","user_type":"User","username":"mdlavin"},{"avatar_url":"https://avatars2.githubusercontent.com/u/746474?v=4","profile_url":"https://github.com/kunickiaj","user_type":"User","username":"kunickiaj"},{"avatar_url":"https://avatars0.githubusercontent.com/u/1446134?v=4","profile_url":"https://github.com/andreimc","user_type":"User","username":"andreimc"},{"avatar_url":"https://avatars0.githubusercontent.com/u/397795?v=4","profile_url":"https://github.com/CRogers","user_type":"User","username":"CRogers"},{"avatar_url":"https://avatars1.githubusercontent.com/u/932644?v=4","profile_url":"https://github.com/chris-zen","user_type":"User","username":"chris-zen"},{"avatar_url":"https://avatars3.githubusercontent.com/u/123595?v=4","profile_url":"https://github.com/dblandin","user_type":"User","username":"dblandin"},{"avatar_url":"https://avatars3.githubusercontent.com/u/326795?v=4","profile_url":"https://github.com/grove","user_type":"User","username":"grove"},{"avatar_url":"https://avatars2.githubusercontent.com/u/1629231?v=4","profile_url":"https://github.com/jdavisonc","user_type":"User","username":"jdavisonc"},{"avatar_url":"https://avatars3.githubusercontent.com/u/5273295?v=4","profile_url":"https://github.com/jzakrzeski","user_type":"User","username":"jzakrzeski"},{"avatar_url":"https://avatars3.githubusercontent.com/u/32574529?v=4","profile_url":"https://github.com/repl-mathieu-fenniak","user_type":"User","username":"repl-mathieu-fenniak"},{"avatar_url":"https://avatars2.githubusercontent.com/u/48936?v=4","profile_url":"https://github.com/thedrow","user_type":"User","username":"thedrow"},{"avatar_url":"https://avatars3.githubusercontent.com/u/400411?v=4","profile_url":"https://github.com/ethx","user_type":"User","username":"ethx"},{"avatar_url":"https://avatars2.githubusercontent.com/u/611122?v=4","profile_url":"https://github.com/tdhopper","user_type":"User","username":"tdhopper"},{"avatar_url":"https://avatars3.githubusercontent.com/u/145250?v=4","profile_url":"https://github.com/zcox","user_type":"User","username":"zcox"},{"avatar_url":"https://avatars1.githubusercontent.com/u/6107881?v=4","profile_url":"https://github.com/d33d33","user_type":"User","username":"d33d33"},{"avatar_url":"https://avatars2.githubusercontent.com/u/10130982?v=4","profile_url":"https://github.com/hmusavi","user_type":"User","username":"hmusavi"},{"avatar_url":"https://avatars3.githubusercontent.com/u/50812?v=4","profile_url":"https://github.com/pbrisbin","user_type":"User","username":"pbrisbin"},{"avatar_url":"https://avatars2.githubusercontent.com/u/9962080?v=4","profile_url":"https://github.com/pazams","user_type":"User","username":"pazams"},{"avatar_url":"https://avatars1.githubusercontent.com/u/1625151?v=4","profile_url":"https://github.com/sscaling","user_type":"User","username":"sscaling"},{"avatar_url":"https://avatars1.githubusercontent.com/u/11371498?v=4","profile_url":"https://github.com/Writtic","user_type":"User","username":"Writtic"}],"created_at":"2016-12-26T02:17:30Z","forks":{"forkedRepos":[{"id":"1860862_123641657","name":"kafka-docker","org_name":"Edon07"}]},"full_name":"usdot-jpo-ode/kafka-docker","language":"Shell","languages":{"Shell":"3541"},"organization":{"org_avatar_url":"https://avatars2.githubusercontent.com/u/23056647?v=4","org_type":"Organization","organization":"usdot-jpo-ode","organization_url":"https://github.com/usdot-jpo-ode"},"origin":"PUBLIC","project_description":"Dockerfile for Apache Kafka","project_name":"kafka-docker","rank":260,"readMe":{"content":"[![Docker Pulls](https://img.shields.io/docker/pulls/wurstmeister/kafka.svg)](https://hub.docker.com/r/wurstmeister/kafka/)\n[![Docker Stars](https://img.shields.io/docker/stars/wurstmeister/kafka.svg)](https://hub.docker.com/r/wurstmeister/kafka/)\n[![](https://badge.imagelayers.io/wurstmeister/kafka:latest.svg)](https://imagelayers.io/?images=wurstmeister/kafka:latest)\n\nkafka-docker\n============\n\nDockerfile for [Apache Kafka](http://kafka.apache.org/)\n\nThe image is available directly from [Docker Hub](https://hub.docker.com/r/wurstmeister/kafka/)\n\n##Pre-Requisites\n\n- install docker-compose [https://docs.docker.com/compose/install/](https://docs.docker.com/compose/install/)\n- modify the ```KAFKA_ADVERTISED_HOST_NAME``` in ```docker-compose.yml``` to match your docker host IP (Note: Do not use localhost or 127.0.0.1 as the host ip if you want to run multiple brokers.)\n- if you want to customise any Kafka parameters, simply add them as environment variables in ```docker-compose.yml```, e.g. in order to increase the ```message.max.bytes``` parameter set the environment to ```KAFKA_MESSAGE_MAX_BYTES: 2000000```. To turn off automatic topic creation set ```KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'```\n\n##Usage\n\nStart a cluster:\n\n- ```docker-compose up -d ```\n\nAdd more brokers:\n\n- ```docker-compose scale kafka=3```\n\nDestroy a cluster:\n\n- ```docker-compose stop```\n\n##Note\n\nThe default ```docker-compose.yml``` should be seen as a starting point. By default each broker will get a new port number and broker id on restart. Depending on your use case this might not be desirable. If you need to use specific ports and broker ids, modify the docker-compose configuration accordingly, e.g. [docker-compose-single-broker.yml](https://github.com/wurstmeister/kafka-docker/blob/master/docker-compose-single-broker.yml):\n\n- ```docker-compose -f docker-compose-single-broker.yml up```\n\n##Broker IDs\n\nIf you don't specify a broker id in your docker-compose file, it will automatically be generated (see [https://issues.apache.org/jira/browse/KAFKA-1070](https://issues.apache.org/jira/browse/KAFKA-1070). This allows scaling up and down. In this case it is recommended to use the ```--no-recreate``` option of docker-compose to ensure that containers are not re-created and thus keep their names and ids.\n\n\n##Automatically create topics\n\nIf you want to have kafka-docker automatically create topics in Kafka during\ncreation, a ```KAFKA_CREATE_TOPICS``` environment variable can be\nadded in ```docker-compose.yml```.\n\nHere is an example snippet from ```docker-compose.yml```:\n\n        environment:\n          KAFKA_CREATE_TOPICS: \"Topic1:1:3,Topic2:1:1:compact\"\n\n```Topic 1``` will have 1 partition and 3 replicas, ```Topic 2``` will have 1 partition, 1 replica and a `cleanup.policy` set to `compact`.\n\n##Advertised hostname\n\nYou can configure the advertised hostname in different ways\n\n1. explicitly, using ```KAFKA_ADVERTISED_HOST_NAME```\n2. via a command, using ```HOSTNAME_COMMAND```, e.g. ```HOSTNAME_COMMAND: \"route -n | awk '/UG[ \\t]/{print $$2}'\"```\n\nWhen using commands, make sure you review the \"Variable Substitution\" section in [https://docs.docker.com/compose/compose-file/](https://docs.docker.com/compose/compose-file/)\n\nIf ```KAFKA_ADVERTISED_HOST_NAME``` is specified, it takes presendence over ```HOSTNAME_COMMAND```\n\nFor AWS deployment, you can use the Metadata service to get the container host's IP:\n```\nHOSTNAME_COMMAND=wget -t3 -T2 -qO-  http://169.254.169.254/latest/meta-data/local-ipv4\n```\nReference: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html\n\n## JMX\n\nFor monitoring purposes you may wish to configure JMX. Additional to the standard JMX parameters, problems could arise from the underlying RMI protocol used to connect\n\n* java.rmi.server.hostname - interface to bind listening port\n* com.sun.management.jmxremote.rmi.port - The port to service RMI requests\n\nFor example, to connect to a kafka running locally (assumes exposing port 1099)\n\n      KAFKA_JMX_OPTS: \"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=127.0.0.1 -Dcom.sun.management.jmxremote.rmi.port=1099\"\n      JMX_PORT: 1099\n\nJconsole can now connect at ```jconsole 192.168.99.100:1099```\n\n##Tutorial\n\n[http://wurstmeister.github.io/kafka-docker/](http://wurstmeister.github.io/kafka-docker/)\n","url":"https://raw.githubusercontent.com/usdot-jpo-ode/kafka-docker/master/README.md"},"releases":[],"repository":"kafka-docker","repository_url":"https://github.com/usdot-jpo-ode/kafka-docker","stage_id":"23056647_77352013","stars":1,"suggest":[{"input":["kafka-docker"],"output":"kafka-docker# name"},{"input":["kafka-docker"],"output":"kafka-docker# name"},{"input":["Dockerfile","for","Apache","Kafka"],"output":"kafka-docker# description"},{"input":["Shell"],"output":"kafka-docker# languages"},{"input":["wurstmeister","varju","eliasdorneles","prabhuinbarajan","filiptepper","bobrik","mdlavin","kunickiaj","andreimc","CRogers","chris-zen","dblandin","grove","jdavisonc","jzakrzeski","repl-mathieu-fenniak","thedrow","ethx","tdhopper","zcox","d33d33","hmusavi","pbrisbin","pazams","sscaling","Writtic"],"output":"kafka-docker# contributors"}],"updated_at":"2017-04-08T04:10:14Z","watchers":2}}
{"_index":"projects","_type":"project","_id":"33698304_117743882","_score":1,"_source":{"commits":3,"contributors":1,"contributors_list":[{"avatar_url":"https://avatars1.githubusercontent.com/u/33724014?v=4","profile_url":"https://github.com/andrewm-aero","user_type":"User","username":"andrewm-aero"}],"created_at":"2018-01-16T21:23:52Z","forks":{"forkedRepos":[]},"full_name":"usdot-jpo-sdcsdw/credentials-db","language":"Shell","languages":{"Shell":"889"},"organization":{"org_avatar_url":"https://avatars1.githubusercontent.com/u/33698304?v=4","org_type":"Organization","organization":"usdot-jpo-sdcsdw","organization_url":"https://github.com/usdot-jpo-sdcsdw"},"origin":"PUBLIC","project_description":"Docker image for storing SSO credentials","project_name":"credentials-db","rank":11,"readMe":{"content":"# Connected Vehicles Credentials Database\n\nThe Connected Vehicles Credentials Database project contains a docker image which \nruns a MySQL database, but also will create the necessary tables for storing\nthe credentials used by the Single Sign-On server.\n\n<a name=\"toc\"/>\n\n## Table of Contents\n\n[I. Release Notes](#release-notes)\n\n[II. Documentation](#documentation)\n\n[III. Getting Started](#getting-started)\n\n[IV. Running the Application](#running)\n\n---\n\n<a name=\"release-notes\" id=\"release-notes\"/>\n\n## [I. Release Notes](ReleaseNotes.md)\n\n<a name=\"documentation\"/>\n\n## II. Documentation\n\nThis repository produces a Docker image which when run, will create the necessary tables an expose a MySQL database.\n\nFor information on configuring the MySQL database itself, see the [documentation for the underlying image](https://hub.docker.com/_/mysql/)\n\nThe database contains two tables:\n\n* Users table: Contains a Username and Password-MD5 column\n* Admins table: Contains a Username column. Users in the Users table which appear in this table are considered administrators.\n\n<a name=\"getting-started\"/>\n\n## III. Getting Started\n\nThe following instructions describe the procedure to fetch, build, and run the application\n\n### Prerequisites\n* Git: https://git-scm.com/\n* Docker: https://docs.docker.com/engine/installation/\n\n---\n### Obtain the Source Code\n\n#### Step 1 - Clone public repository\n\nClone the source code from the GitHub repository using Git command:\n\n```bash\ngit clone TBD\n```\n\n<a name=\"running\"/>\n\n## IV. Running the application\n\n---\n### Build and Deploy the Application\n\n**Step 1**: Build Docker Image\n\n```bash\ndocker build -t dotcv/credentials-db .\n```\n\n**Step 2**: Run Docker Container\n\nIn addition to the variables expected by the underlying MySQL image, the following\nvariables are available to configure the image:\n\n* USERS_TABLE_NAME - Name of table which stores credentials. Defaults to \"users\".\n* ADMINS_TABLE_NAME - Name of table which stores which users are administrators. Defaults to \"admins\".\n* USERNAME_COLUMN_NAME - Name of column (for both tables) which stores user names. Defaults to \"username\".\n* USERNAME_COLUMN_TYPE - SQL type of user names. Defaults to \"char(128)\".\n* PASSWORD_COLUMN_NAME - Name of column which stores password MD5's. Defaults to \"password\".\n* PASSWORD_COLUMN_TYPE - SQL type of passwords. Defaults to \"char(64)\".\n\n```bash\ndocker run \\\n    -e MYSQL_DATABASE=... \\\n    -e MYSQL_ROOT_PASSWORD=... \\\n    -e USERS_TABLE_NAME=... \\\n    -e ADMINS_TABLE_NAME=... \\\n    -e USERNAME_COLUMN_NAME=... \\\n    -e USERNAME_COLUMN_TYPE=... \\\n    -e PASSWORD_COLUMN_NAME=... \\\n    -e PASSWORD_COLUMN_TYPE=... \\\n    dotcv/credentials-db\n```\n\n\n</a>","url":"https://raw.githubusercontent.com/usdot-jpo-sdcsdw/credentials-db/master/README.md"},"releases":[],"repository":"credentials-db","repository_url":"https://github.com/usdot-jpo-sdcsdw/credentials-db","stage_id":"33698304_117743882","stars":1,"suggest":[{"input":["credentials-db"],"output":"credentials-db# name"},{"input":["credentials-db"],"output":"credentials-db# name"},{"input":["Docker","image","for","storing","SSO","credentials"],"output":"credentials-db# description"},{"input":["Shell"],"output":"credentials-db# languages"},{"input":["andrewm-aero"],"output":"credentials-db# contributors"}],"updated_at":"2018-01-16T21:25:21Z","watchers":0}}
{"_index":"projects","_type":"project","_id":"33698304_117575789","_score":1,"_source":{"commits":18,"contributors":1,"contributors_list":[{"avatar_url":"https://avatars1.githubusercontent.com/u/33724014?v=4","profile_url":"https://github.com/andrewm-aero","user_type":"User","username":"andrewm-aero"}],"created_at":"2018-01-15T17:37:04Z","forks":{"forkedRepos":[]},"full_name":"usdot-jpo-sdcsdw/fedgov-cv-message-validator-webapp","language":"Java","languages":{"CSS":"3293","HTML":"14331","Java":"36901","JavaScript":"4078","Shell":"804"},"organization":{"org_avatar_url":"https://avatars1.githubusercontent.com/u/33698304?v=4","org_type":"Organization","organization":"usdot-jpo-sdcsdw","organization_url":"https://github.com/usdot-jpo-sdcsdw"},"origin":"PUBLIC","project_description":"Webapp for validating SEMI extension ASN.1 messages","project_name":"fedgov-cv-message-validator-webapp","rank":30,"readMe":{"content":"# Connected Vehicles Message Validator Project\n\nThe fedgov-cv-message-validator-webapp project is a webapp to validate ASN.1 messages by checking for accuracy against the\nspecifications and standards prior to depositing into a warehouse.\n\n![Diagram](doc/images/fedgov-cv-message-validator-webapp-screenshot.png)\n\n<a name=\"toc\"/>\n\n## Table of Contents\n\n[I. Release Notes](#release-notes)\n\n[II. Documentation](#documentation)\n\n[III. Getting Started](#getting-started)\n\n[IV. Running the Application (Standalone)](#running-standalone)\n\n[V. Running the Application (Docker)](#running-docker)\n\n---\n\n<a name=\"release-notes\" id=\"release-notes\"/>\n\n## [I. Release Notes](ReleaseNotes.md)\n\n<a name=\"documentation\"/>\n\n## II. Documentation\n\nThis repository produces a WAR file containing a Jersey Servlet, so it can be deployed on any web-server that supports it (e.g. Tomcat, Jetty).\n\nThe application can also be deployed using a docker container. This container will run the application under a Jetty server, and can be configured to use SSL certificates.\n\n<a name=\"getting-started\"/>\n\n## III. Getting Started\n\nThe following instructions describe the proceedure to fetch, build, and run the application\n\n### Prerequisites\n* JDK 1.8: http://www.oracle.com/technetwork/pt/java/javase/downloads/jdk8-downloads-2133151.html\n* Maven: https://maven.apache.org/install.html\n* Git: https://git-scm.com/\n* Docker: https://docs.docker.com/engine/installation/\n* PerXerCodec: https://github.com/usdot-jpo-sdcsdw/per-xer-codec.git\n\n---\n### Obtain the Source Code\n\n#### Step 1 - Clone public repository\n\nClone the source code from the GitHub repository using Git command:\n\n```bash\ngit clone TBD\n```\n\n<a name=\"running\"/>\n\n## IV. Running the application (Standalone)\n\n---\n### Build and Deploy the Application\n\n**Step 1**: Build the WAR file\n\n```bash\nmvn package\n```\n\n**Step 2**: Deploy the WAR file\n\n```bash\n# Consult your webserver's documentation for instructions on deploying war files \ncp target/validator.war ... \n```\n\n<a name=\"running-docker\"/>\n\n## V. Running the Application (Docker)\n\n---\n### Build and Deploy the Application\n\n**Step 1**: Build the WAR file\n\n```bash\nmvn package\n```\n\n**Step 2**: Build the Docker image, providing the path to the native library for the PER-XER codec\n\n```bash\ndocker build -t dotcv/message-validator-webapp --build-arg CODEC_SO_PATH=... .\n```\n\nThis path depends on which OS you are building on. If you are building on a Linux system, codec is located at target/libper-xer-codec.so after running the maven build.\nIf you are building on OSX, you will need to provide the path to the Linux SO you built manually, according to the instructions provided by that project.\n\n**Step 3**: Run the Docker image in a Container, mounting the SSL certificate keystore directory, and specifying the following:\n* Keystore filename\n* Keystore password\n* HTTP Port\n* HTTPS Port\n\n\n```bash\ndocker run -p HTTP_PORT:8080 \\\n           -p HTTPS:_PORT:8443 \\\n           -e JETTY_KEYSTORE_PASSWORD=... \\\n           -v KEYSTORE_DIRECTORY:/usr/local/jetty/etc/keystore_mount \\\n           -e JETTY_KEYSTORE_RELATIVE_PATH=... \\\n            dotcv/message-validator-webapp\n```\n\n</a>","url":"https://raw.githubusercontent.com/usdot-jpo-sdcsdw/fedgov-cv-message-validator-webapp/master/README.md"},"releases":[],"repository":"fedgov-cv-message-validator-webapp","repository_url":"https://github.com/usdot-jpo-sdcsdw/fedgov-cv-message-validator-webapp","stage_id":"33698304_117575789","stars":1,"suggest":[{"input":["fedgov-cv-message-validator-webapp"],"output":"fedgov-cv-message-validator-webapp# name"},{"input":["fedgov-cv-message-validator-webapp"],"output":"fedgov-cv-message-validator-webapp# name"},{"input":["Webapp","for","validating","SEMI","extension","ASN1","messages"],"output":"fedgov-cv-message-validator-webapp# description"},{"input":["JavaScript","Shell","CSS","HTML","Java"],"output":"fedgov-cv-message-validator-webapp# languages"},{"input":["andrewm-aero"],"output":"fedgov-cv-message-validator-webapp# contributors"}],"updated_at":"2018-11-30T00:34:01Z","watchers":1}}
{"_index":"projects","_type":"project","_id":"33698304_117864725","_score":1,"_source":{"commits":14,"contributors":1,"contributors_list":[{"avatar_url":"https://avatars1.githubusercontent.com/u/33724014?v=4","profile_url":"https://github.com/andrewm-aero","user_type":"User","username":"andrewm-aero"}],"created_at":"2018-01-17T16:50:08Z","forks":{"forkedRepos":[]},"full_name":"usdot-jpo-sdcsdw/jpo-sdcsdw","language":"Shell","languages":{"Shell":"1118"},"organization":{"org_avatar_url":"https://avatars1.githubusercontent.com/u/33698304?v=4","org_type":"Organization","organization":"usdot-jpo-sdcsdw","organization_url":"https://github.com/usdot-jpo-sdcsdw"},"origin":"PUBLIC","project_description":"Main project","project_name":"jpo-sdcsdw","rank":22,"readMe":{"content":"# jpo-sdcsdw\nUS Department of Transportation Joint Program office (JPO) Situational Data Clearinghouse/Situational Data Warehouse (SDC/SDW)\n\nIn the context of ITS, the Situation Data Warehouse is a software system that\nallows users to deposit situational data for use with connected vehicles (CV) and\nlater query for that data.\n\n![Diagram](doc/images/mvp-architecture-diagram.png)\n\n<a name=\"toc\"/>\n\n## Table of Contents\n\n[I. Release Notes](#release-notes)\n\n[II. Documentation](#documentation)\n\n[III. Collaboration Tools](#collaboration-tools)\n\n[IV. Getting Started](#getting-started)\n\n---\n\n<a name=\"release-notes\"/>\n\n## [I. Release Notes](ReleaseNotes.md)\n\n<a name=\"documentation\"/>\n\n## II. Documentation\n\n<a name=\"collaboration-tools\"/>\n\n## III. Collaboration Tools\n\n### Source Repositories - GitHub\n\n* Main repository on GitHub (public)\n    * https://github.com/usdot-jpo-sdcsdw/jpo-sdcsdw\n    * git@github.com:usdot-jpo-sdcsdw/jpo-sdcsdw.git\n\n### Agile Project Managment - Jira\n\nhttps://usdotjposdcsdw.atlassian.net/secure/RapidBoard.jspa?rapidView=1&projectKey=SDCSDW\n\n### Wiki - Confluence\n\nhttps://usdotjposdcsdw.atlassian.net/wiki/spaces/SDCSDW/overview\n\n### Continuous Integration and Delivery\n\nTODO\n\n### Static Code Analysis\n\nTODO\n\n<a name=\"getting-started\"/>\n\n## IV. Getting Started\n\nThe following instructions describe the procedure to fetch, build, and run the application.\n\n### Prerequisites\n\n* JDK 1.8: http://www.oracle.com/technetwork/pt/java/javase/downloads/jdk8-downloads-2133151.html\n* Maven: https://maven.apache.org/install.html\n* Git: https://git-scm.com/\n* Docker: https://www.docker.com/get-docker\n* Make: https://www.gnu.org/software/make/\n\n---\n\n### Obtain the Source Code\n\n|Name|Description|\n|----|-----------|\n|[common-models](https://github.com/usdot-jpo-sdcsdw/common-models)|Models containing traveler information|\n|[per-xer-codec](https://github.com/usdot-jpo-sdcsdw/per-xer-codec)|JNI Wrapper around asn1c-generated code|\n|[udp-interface](https://github.com/usdot-jpo-sdcsdw/udp-interface)|Interface for querying traveler information over UDP using the SEMI extensions protocol|\n|[fedgov-cv-webapp-websocket](https://github.com/usdot-jpo-sdcsdw/fedgov-cv-webapp-websocket)|Interface for depositing and querying for traveler information over Websockets|\n|[fedgov-cv-whtools-webapp](https://github.com/usdot-jpo-sdcsdw/fedgov-cv-whtools-webapp)|Web GUI front-end for the websockets interface|\n|[fedgov-cv-sso-webapp](https://github.com/usdot-jpo-sdcsdw/fedgov-cv-sso-webapp)|Central Authentication Server for securing the Web GUI and Websockets back-end|\n|[credentials-db](https://github.com/usdot-jpo-sdcsdw/credentials-db)|Docker image for storing user credentials|\n|[fedgov-cv-message-validator-webapp](https://github.com/usdot-jpo-sdcsdw/fedgov-cv-message-validator-webapp)|Web GUI for validating messages to be deposited|\n|[tim-db](https://github.com/usdot-jpo-sdcsdw/tim-db)|Docker image for storing traveler information|\n\n\n#### Step 1 - Clone public repository\n\nClone the source code from the GitHub Repository using Git Command:\n\n```bash\ngit clone --recurse-submodules https://github.com/usdot-jpo-sdcsdw/jpo-sdcsdw\n```\n\nNote: Make sure you specify the --recurse-submodules option on the clone command line. This option will cause the cloning of all dependent submodules:\n\n### Build the Application\n\n#### Build Process\n\nThe SDC/SDW uses Maven to manage builds\n\n**Step 1**: Generate ASN.1 Code\n\nFor more information on this process, please see the documenation for the [PER XER Codec](per-xer-codec/README.md)\n\n```bash\ncd per-xer-codex/asn1-codegen\nmake directories\ncp ... per-xer-codec/asn1-codegen/src/asn1/\nmake all install\ncd ../..\n```\n\n**Step 2**: Build the maven artifacts\n\n```bash\nmvn install\n```\n\n**Step 3**: Configure docker images\n\nEdit [build-docker-images.env](build-docker-images.env) to set the image names and versions appropriately.\n\n**Step 4**: Build docker images\n\n```bash\n./build-docker-imges.sh\n```\n\nSee the README's for each sub-project for information on configuring specific images.\n\n#### Building docker images on non-linux systems\n\nBuilding docker images on non-linux systems is not currently automated to the same\ndegree, due to the need to build a linux shared object file from the PER XER Codec.\nIf you are building and deploying natively, these steps are uncessary, but if you\nintend to build docker images on anything other than linux, you will need to perform\nthese additional steps.\n\n**Step 1**: Manually build PerXerCodec\n\nAfter completing step 1 from the main build sequence, use make to build the shared object\ninside of a docker container.\n\n```bash\ncd per-xer-codec/native\nmake all install\ncd ../..\n```\n\n**Step 2**: Manually copy shared object\n\nAfter completing step 2 from the main build sequence, copy the shared object file to the necessary directories\n\n```bash\ncp per-xer-codec/java/target/libper-xer-codec.so fedgov-cv-message-validator-webapp/target/\ncp per-xer-codec/java/target/libper-xer-codec.so fedgov-cv-whtools-webapp/target/\n```\n\nFrom here, you can continue with step 3 of the main build sequence.\n\n### Deploy the Application\n\nFrom here, please follow the [deployment guide](https://usdotjposdcsdw.atlassian.net/wiki/spaces/SDCSDW/pages/34340865/AWS+Bootstrap+Deployment) on the wiki\n","url":"https://raw.githubusercontent.com/usdot-jpo-sdcsdw/jpo-sdcsdw/master/README.md"},"releases":[],"repository":"jpo-sdcsdw","repository_url":"https://github.com/usdot-jpo-sdcsdw/jpo-sdcsdw","stage_id":"33698304_117864725","stars":1,"suggest":[{"input":["jpo-sdcsdw"],"output":"jpo-sdcsdw# name"},{"input":["jpo-sdcsdw"],"output":"jpo-sdcsdw# name"},{"input":["Main","project"],"output":"jpo-sdcsdw# description"},{"input":["Shell"],"output":"jpo-sdcsdw# languages"},{"input":["andrewm-aero"],"output":"jpo-sdcsdw# contributors"}],"updated_at":"2018-01-22T19:42:21Z","watchers":0}}
{"_index":"projects","_type":"project","_id":"33698304_117569063","_score":1,"_source":{"commits":74,"contributors":1,"contributors_list":[{"avatar_url":"https://avatars1.githubusercontent.com/u/33724014?v=4","profile_url":"https://github.com/andrewm-aero","user_type":"User","username":"andrewm-aero"}],"created_at":"2018-01-15T16:30:37Z","forks":{"forkedRepos":[{"id":"23056647_119436284","name":"per-xer-codec","org_name":"usdot-jpo-ode"}]},"full_name":"usdot-jpo-sdcsdw/per-xer-codec","language":"Java","languages":{"C":"12886","Java":"103687","Makefile":"4978","Shell":"620"},"organization":{"org_avatar_url":"https://avatars1.githubusercontent.com/u/33698304?v=4","org_type":"Organization","organization":"usdot-jpo-sdcsdw","organization_url":"https://github.com/usdot-jpo-sdcsdw"},"origin":"PUBLIC","project_description":"JNI Wrapper for SEMI ASN.1 J2735 Extensions","project_name":"per-xer-codec","rank":93,"readMe":{"content":"\n# ASN.1 J275 + SEMI Extensions PER-XER Codec\n\nThe PER-XER Codec is an open source library which wraps the C code generated by asn1c in a JNI wrapper.\n\n<a name=\"toc\"/>\n\n## Table of Contents\n\n[I. Release Notes](#release-notes)\n\n[II. Documentation](#documentation)\n\n[III. Getting Started](#getting-started)\n\n[IV. Code Generation](#code-generation)\n\n[V. Building the Library (Combined)](#building)\n\n[VI. Building the Library (Standalone Native)](#building-native)\n\n[VII. Usage](#usage)\n\n---\n\n<a name=\"release-notes\" id=\"release-notes\"/>\n\n## [I. Release Notes](ReleaseNotes.md)\n\n<a name=\"documentation\"/>\n\n## II. Documentation\n\nThis project produces two artifacts: A native-code Shared Object library, and a JAR wrapper around that library.\n\n<a name=\"getting-started\"/>\n\n## III. Getting Started\n\nThe following instructions describe the proceedure to fetch, build, and run the application\n\n### Prerequisites\n* git\n* make\n* java 1.8\n* maven\n* asn1c (See below)\n\n---\n### Obtain the Source Code\n\n#### Step 1 - Clone public repository\n\nClone the source code from the GitHub repository using Git command:\n\n```bash\ngit clone --recurse-submodules https://github.com/usdot-jpo-sdcsdw/per-xer-codec.git\n```\n\n<a name=\"code-generation\"/>\n\n## IV. Code Generation\n\n**Step 1**:  Create the various empty directories that git won't track\n\nNavigate to the codegen directory and run the appropriate make rule\n\n```bash\ncd asn1-codegen\nmake directories\n```\n\n**Step 2**: Add the necessary ASN 1 files\n\n```bash\ncp ... src/asn1/ \n```\n\n**Step 3**: Generate the ASN.1 C code\n\nStill in the codegen directory, run the full make, and then install\n\n```bash\nmake\nmake install\n```\n\nThis will:\n* Build the asn1c compiler\n* Install it into asn1-codegen/install/asn1c\n* Generate the c code using the ASN.1 files provided\n* Move them to the native/src/main/c/asn1 directory \n\n\n<a name=\"building\"/>\n\n## V. Building the Library (Combined)\n\nThis step is for building the library as a complete maven project, which will yield both the JNI Jar, and a native shared object library for it to use.\n\n**Step 1**: Follow the instructions in [IV. Code Generation](#code-generation)\n\n**Step 2**: Build using maven \n\nFrom the root directory, and run the maven install goal\n\n```bash\nmvn install\n```\n\nThe maven configuration is set up to detect what OS you are building on, and will install the appropriate native library. Cross compiling is not currently supported. OSX and Linux are currently the only supported OS's.\n\n<a name=\"building-native\"/>\n\n## VI. Building the Library (Standalone Native)\n\nThis step is for building the native shared library by itself, separate from the JNI wrapper\n\n**Step 1**: Follow the instructions in [IV. Code Generation](#code-generation)\n\n**Step 2**: Build using make\n\nFrom the native directory, run make\n\n```bash\ncd native\nmake\n```\n\nThis makefile supports building the Linux Shared object on OSX, using a docker container, and will detect this automatically\n\nThis will:\n    Produce a docker image capable of building the Linux shared object library\n    Produce a container of that image, and build the shared object in it\n    Copy the built shared object out of the container\n    Delete the container\n\n<a name=\"usage\"/>\n\n## VII. Usage\n\n### Intended Usage\n\nThe primary way to use this module is to include the PerXerCodec class, and use its perToXer and xerToPer methods.\nThese methods require 3 things: A PerData format, a XerData Format, and an Asn1Type.\nAsn1Types can be accessed as static members of PerXerCodec (e.g. ServiceRequestType).\nThe PerData can be any object that implements the PerData interface, Likewise for XerData.\nIf the type of data is unknown, the guessPerToXer and guessXerToPer take multiple possible data types and will try each.\n\n### Maven Dependency\n\nMaven projects which wish to use the codec must declare two dependencies, the java wrapper (per-xer-codec-java), and the appropriate shared object (per-xer-codec-native-osx|linux).\n\nIf you wish to run unit tests in your project which depend on the codec, you will need to ensure the native shared object exists at test run time, and specify the java.library.path to your unit test runner.\n\n### Deploying\n\n**Step 1**: Place the jar file generated by the build process on the classpath\n\n**Step 2**: Place the generated shared object file on the java native library path\n    This is set using the java.library.path java system property, i.e. using the ```-Djava.library.path=...``` command line argument. \n    If not provided, this property usually defaults to the system library path.\n    \n</a>\n","url":"https://raw.githubusercontent.com/usdot-jpo-sdcsdw/per-xer-codec/master/README.md"},"releases":[],"repository":"per-xer-codec","repository_url":"https://github.com/usdot-jpo-sdcsdw/per-xer-codec","stage_id":"33698304_117569063","stars":2,"suggest":[{"input":["per-xer-codec"],"output":"per-xer-codec# name"},{"input":["per-xer-codec"],"output":"per-xer-codec# name"},{"input":["JNI","Wrapper","for","SEMI","ASN1","J2735","Extensions"],"output":"per-xer-codec# description"},{"input":["Java","C","Makefile","Shell"],"output":"per-xer-codec# languages"},{"input":["andrewm-aero"],"output":"per-xer-codec# contributors"}],"updated_at":"2018-11-30T00:34:51Z","watchers":2}}
{"_index":"projects","_type":"project","_id":"23056647_99358953","_score":1,"_source":{"commits":2018,"contributors":16,"contributors_list":[{"avatar_url":"https://avatars2.githubusercontent.com/u/145781?v=4","profile_url":"https://github.com/vlm","user_type":"User","username":"vlm"},{"avatar_url":"https://avatars0.githubusercontent.com/u/6227912?v=4","profile_url":"https://github.com/brchiu","user_type":"User","username":"brchiu"},{"avatar_url":"https://avatars3.githubusercontent.com/u/1236714?v=4","profile_url":"https://github.com/velichkov","user_type":"User","username":"velichkov"},{"avatar_url":"https://avatars2.githubusercontent.com/u/156891?v=4","profile_url":"https://github.com/wiml","user_type":"User","username":"wiml"},{"avatar_url":"https://avatars2.githubusercontent.com/u/8332609?v=4","profile_url":"https://github.com/simo5","user_type":"User","username":"simo5"},{"avatar_url":"https://avatars1.githubusercontent.com/u/2324954?v=4","profile_url":"https://github.com/johvik","user_type":"User","username":"johvik"},{"avatar_url":"https://avatars0.githubusercontent.com/u/254519?v=4","profile_url":"https://github.com/akire","user_type":"User","username":"akire"},{"avatar_url":"https://avatars3.githubusercontent.com/u/18166483?v=4","profile_url":"https://github.com/DanyaFilatov","user_type":"User","username":"DanyaFilatov"},{"avatar_url":"https://avatars1.githubusercontent.com/u/660477?v=4","profile_url":"https://github.com/elfring","user_type":"User","username":"elfring"},{"avatar_url":"https://avatars0.githubusercontent.com/u/1487399?v=4","profile_url":"https://github.com/sleevi","user_type":"User","username":"sleevi"},{"avatar_url":"https://avatars1.githubusercontent.com/u/481351?v=4","profile_url":"https://github.com/theirix","user_type":"User","username":"theirix"},{"avatar_url":"https://avatars1.githubusercontent.com/u/167235?v=4","profile_url":"https://github.com/daa","user_type":"User","username":"daa"},{"avatar_url":"https://avatars2.githubusercontent.com/u/1749243?v=4","profile_url":"https://github.com/basinilya","user_type":"User","username":"basinilya"},{"avatar_url":"https://avatars3.githubusercontent.com/u/1163818?v=4","profile_url":"https://github.com/jariq","user_type":"User","username":"jariq"},{"avatar_url":"https://avatars0.githubusercontent.com/u/444024?v=4","profile_url":"https://github.com/sancane","user_type":"User","username":"sancane"},{"avatar_url":"https://avatars3.githubusercontent.com/u/15227658?v=4","profile_url":"https://github.com/mattipee","user_type":"User","username":"mattipee"}],"created_at":"2017-08-04T16:00:17Z","forks":{"forkedRepos":[{"id":"1860862_123641629","name":"asn1c","org_name":"Edon07"}]},"full_name":"usdot-jpo-ode/asn1c","language":"C","languages":{"C":"1787505","C++":"5778","Lex":"14013","M4":"22203","Makefile":"11982","Objective-C":"5234","Perl":"631","Perl 6":"1073","Shell":"23656","Yacc":"54176"},"organization":{"org_avatar_url":"https://avatars2.githubusercontent.com/u/23056647?v=4","org_type":"Organization","organization":"usdot-jpo-ode","organization_url":"https://github.com/usdot-jpo-ode"},"origin":"PUBLIC","project_description":"The ASN.1 Compiler","project_name":"asn1c","rank":2125,"readMe":{"content":"# About\n\nASN.1 to C compiler takes the ASN.1 module files (example) and generates\nthe C++ compatible C source code. That code can be used to serialize\nthe native C structures into compact and unambiguous BER/XER/PER-based\ndata files, and deserialize the files back.\n\nVarious ASN.1 based formats are widely used in the industry,\nsuch as to encode the X.509 certificates employed in the HTTPS handshake,\nto exchange control data between mobile phones and cellular networks,\nto car-to-car communication in intelligent transportation networks.\n\nThe ASN.1 standard is large and complex and no open source compiler supports\nit in its entirety. The asn1c is arguably the most evolved open source\nASN.1 compiler.\n\n# Build and Install\n\nIf you haven't installed the asn1c yet, read the [INSTALL.md](INSTALL.md) file\nfor a short installation guide.\n\n[![Build Status](https://travis-ci.org/vlm/asn1c.svg?branch=master)](https://travis-ci.org/vlm/asn1c)\n\n# Documentation\n\nFor the list of asn1c command line options, see `asn1c -h` or `man asn1c`.\n\nThe comprehensive documentation on this compiler is in [doc/asn1c-usage.pdf](doc/asn1c-usage.pdf).\n\nPlease also read the [FAQ](FAQ) file.\n\nAn excellent book on ASN.1 is written by Olivier Dubuisson:\n\"ASN.1 Communication between heterogeneous systems\", ISBN:0-12-6333361-0.\n\n# Quick start\n\n(also check out [doc/asn1c-quick.pdf](doc/asn1c-quick.pdf))\n\nAfter installing the compiler (see [INSTALL.md](INSTALL.md)), you may use\nthe asn1c command to compile the ASN.1 specification:\n\n    asn1c <module.asn1>                         # Compile module\n\nIf several specifications contain interdependencies, all of them must be\nspecified at the same time:\n\n    asn1c <module1.asn1> <module2.asn1> ...     # Compile interdependent modules\n\nThe asn1c source tarball contains the [examples/](examples/) directory\nwith several ASN.1 modules and a [script](examples/crfc2asn1.pl)\nto extract the ASN.1 modules from RFC documents.\nRefer to the [examples/README](examples/README) file in that directory.\n\nTo compile the X.509 PKI module:\n\n    ./asn1c/asn1c -P ./examples/rfc3280-*.asn1  # Compile-n-print\n\nIn this example, the **-P** option is to print the compiled text on the\nstandard output. The default behavior is that asn1c compiler creates\nmultiple .c and .h files for every ASN.1 type found inside the specified\nASN.1 modules.\n\nThe compiler's **-E** and **-EF** options are used for testing the parser and\nthe semantic fixer, respectively. These options will instruct the compiler\nto dump out the parsed (and fixed) ASN.1 specification as it was\n\"understood\" by the compiler. It might be useful for checking\nwhether a particular syntactic construction is properly supported\nby the compiler.\n\n    asn1c -EF <module-to-test.asn1>             # Check semantic validity\n\n# Model of operation\n\nThe asn1c compiler works by processing the ASN.1 module specifications\nin several stages:\n\n1. During the first stage, the ASN.1 file is parsed.\n   (Parsing produces an ASN.1 syntax tree for the subsequent levels)\n2. During the second stage, the syntax tree is \"fixed\".\n   (Fixing is a process of checking the tree for semantic errors,\n   accompanied by the tree transformation into the canonical form)\n3. During the third stage, the syntax tree is compiled into the target language.\n\nThere are several command-line options reserved for printing the results\nafter each stage of operation:\n\n    <parser> => print                                       (-E)\n    <parser> => <fixer> => print                            (-E -F)\n    <parser> => <fixer> => <compiler> => print              (-P)\n    <parser> => <fixer> => <compiler> => save-compiled      [default]\n\n\n-- \nLev Walkin\nvlm@lionet.info\n","url":"https://raw.githubusercontent.com/usdot-jpo-ode/asn1c/master/README.md"},"releases":[],"repository":"asn1c","repository_url":"https://github.com/usdot-jpo-ode/asn1c","stage_id":"23056647_99358953","stars":1,"suggest":[{"input":["asn1c"],"output":"asn1c# name"},{"input":["asn1c"],"output":"asn1c# name"},{"input":["The","ASN1","Compiler"],"output":"asn1c# description"},{"input":["Shell","Perl","Lex","Objective-C","Makefile","Yacc","M4","C","C++","Perl 6"],"output":"asn1c# languages"},{"input":["vlm","brchiu","velichkov","wiml","simo5","johvik","akire","DanyaFilatov","elfring","sleevi","theirix","daa","basinilya","jariq","sancane","mattipee"],"output":"asn1c# contributors"}],"updated_at":"2017-09-05T16:38:58Z","watchers":6}}
{"_index":"projects","_type":"project","_id":"23056647_129775801","_score":1,"_source":{"commits":195,"contributors":7,"contributors_list":[{"avatar_url":"https://avatars1.githubusercontent.com/u/7940050?v=4","profile_url":"https://github.com/conz27","user_type":"User","username":"conz27"},{"avatar_url":"https://avatars2.githubusercontent.com/u/6707566?v=4","profile_url":"https://github.com/vkumar-si","user_type":"User","username":"vkumar-si"},{"avatar_url":"https://avatars1.githubusercontent.com/u/4628742?v=4","profile_url":"https://github.com/wwhyte-si","user_type":"User","username":"wwhyte-si"},{"avatar_url":"https://avatars0.githubusercontent.com/u/5224646?v=4","profile_url":"https://github.com/davormrkoci","user_type":"User","username":"davormrkoci"},{"avatar_url":"https://avatars3.githubusercontent.com/u/17088771?v=4","profile_url":"https://github.com/aweimerskirch","user_type":"User","username":"aweimerskirch"},{"avatar_url":"https://avatars2.githubusercontent.com/u/10130982?v=4","profile_url":"https://github.com/hmusavi","user_type":"User","username":"hmusavi"},{"avatar_url":"https://avatars0.githubusercontent.com/u/16580364?v=4","profile_url":"https://github.com/fvillamor","user_type":"User","username":"fvillamor"}],"created_at":"2018-04-16T16:45:34Z","forks":{"forkedRepos":[]},"full_name":"usdot-jpo-ode/jpo-ode-scms-asn","languages":{},"organization":{"org_avatar_url":"https://avatars2.githubusercontent.com/u/23056647?v=4","org_type":"Organization","organization":"usdot-jpo-ode","organization_url":"https://github.com/usdot-jpo-ode"},"origin":"PUBLIC","project_name":"jpo-ode-scms-asn","rank":254,"readMe":{"content":"# scms-asn\n\nThis repository contains the ASN.1 definitions for data containers and protocols\nused in SCMS.\n\n## Initializing the Git Repository\n\nIt's important to note that this repository contains a Git submodule (i.e. a\ndependency on the 1609.2 public repository). Therefore, doing a simple Git\nclone is not enough to pull down the entire repo.\n\nInstructions to clone the SCMS-ASN repository are as follows:\n\n1. ```git clone http://<username>@stash.campllc.org/scm/scms/scms-asn.git```, where \n'<username>' is your CAMP login.\n\n2. Go into the scms-asn folder and run ```git submodule update --init --recursive``` \nto checkout the 1609.2 repository.\n\n","url":"https://raw.githubusercontent.com/usdot-jpo-ode/jpo-ode-scms-asn/scms-asn-v1.2.1-release/README.md"},"releases":[],"repository":"jpo-ode-scms-asn","repository_url":"https://github.com/usdot-jpo-ode/jpo-ode-scms-asn","stage_id":"23056647_129775801","stars":0,"suggest":[{"input":["jpo-ode-scms-asn"],"output":"jpo-ode-scms-asn# name"},{"input":["jpo-ode-scms-asn"],"output":"jpo-ode-scms-asn# name"},{"input":[""],"output":"jpo-ode-scms-asn# description"},{"input":[],"output":"jpo-ode-scms-asn# languages"},{"input":["conz27","vkumar-si","wwhyte-si","davormrkoci","aweimerskirch","hmusavi","fvillamor"],"output":"jpo-ode-scms-asn# contributors"}],"updated_at":"2018-04-16T16:47:11Z","watchers":6}}
{"_index":"projects","_type":"project","_id":"23056647_127138533","_score":1,"_source":{"commits":31,"contributors":2,"contributors_list":[{"avatar_url":"https://avatars2.githubusercontent.com/u/10130982?v=4","profile_url":"https://github.com/hmusavi","user_type":"User","username":"hmusavi"},{"avatar_url":"https://avatars0.githubusercontent.com/u/12912578?v=4","profile_url":"https://github.com/mvs5465","user_type":"User","username":"mvs5465"}],"created_at":"2018-03-28T12:42:41Z","forks":{"forkedRepos":[]},"full_name":"usdot-jpo-ode/jpo-security-svcs","language":"HTML","languages":{"Dockerfile":"397","HTML":"12428","Java":"9021"},"organization":{"org_avatar_url":"https://avatars2.githubusercontent.com/u/23056647?v=4","org_type":"Organization","organization":"usdot-jpo-ode","organization_url":"https://github.com/usdot-jpo-ode"},"origin":"PUBLIC","project_description":"This module exposes a RESTful API for performing cryptographic functions.","project_name":"jpo-security-svcs","rank":72,"readMe":{"content":"\n# jpo-security-svcs\nThis module expopsed a RESTful API for performing cryptographic functions. The following paths identify the functions:\n|Verb|path|Content Type|Functionality|Request Body Format|Response Body Format|\n|--|--|--|--|--|--|\n|POST|/sign|application/json|signs data provided in the body of the request|{\"message\":\"Base64 encoded unsigned data\"}|{\"result\": \"Base64 Encoded Signed Data\"}\n\n## Install\n\n`mvn clean install`\n\n## Run\n\n### Java JAR:\n\n`java -jar target/jpo-security-svcs-0.0.1-SNAPSHOT.jar`\n\n### Docker:\n\n`docker build .`\n\n(Take note of image reported by docker build)\n\n`docker run -p 8090:8090 <image>`\n\n## Test\n\nSend a POST request to `localhost:8090/sign` with a body of the form:\n\n```\n{\n\t\"message\": \"<hex encoded data>\"\n}\n```\n\nExpected output:\n\n```\n{\n\t\"result\": \"<hex encoded data + signature>\"\n}\n```\n\n## Configuration\n\nIn `./src/main/resources/application.properties` you'll find the following properties which can be defined wither on the command line or by environment variable. To define the property on the command line, insert `--` to the front of the Property name, for example, `--server.port=8091`:\n\n| Property | Meaning | Default Value | Environment Variable Substitute |\n| -----------|------------|-----------------|-----------|\n| server.port | The port number to which this service will be listening.| 8090 |SERVER_PORT|\n| sec.cryptoServiceBaseUri | Cryptographic service endpoint URI excluding path. For example, `http://<ip>:<port>` OR `http://server.dns.name` including the port number, if any. | - |SEC_CRYPTO_SERVICE_BASE_URI|\n| sec.cryptoServiceEndpointSignPath | The REST endpoint path of the external service. | /tmc/signtim |SEC_CRYPTO_SERVICE_ENDPOINT_SIGN_PATH|\n","url":"https://raw.githubusercontent.com/usdot-jpo-ode/jpo-security-svcs/master/README.md"},"releases":[],"repository":"jpo-security-svcs","repository_url":"https://github.com/usdot-jpo-ode/jpo-security-svcs","stage_id":"23056647_127138533","stars":1,"suggest":[{"input":["jpo-security-svcs"],"output":"jpo-security-svcs# name"},{"input":["jpo-security-svcs"],"output":"jpo-security-svcs# name"},{"input":["This","module","exposes","a","RESTful","API","for","performing","cryptographic","functions"],"output":"jpo-security-svcs# description"},{"input":["HTML","Java","Dockerfile"],"output":"jpo-security-svcs# languages"},{"input":["hmusavi","mvs5465"],"output":"jpo-security-svcs# contributors"}],"updated_at":"2018-12-21T15:58:06Z","watchers":7}}
{"_index":"projects","_type":"project","_id":"23056647_119436284","_score":1,"_source":{"commits":67,"contributors":1,"contributors_list":[{"avatar_url":"https://avatars1.githubusercontent.com/u/33724014?v=4","profile_url":"https://github.com/andrewm-aero","user_type":"User","username":"andrewm-aero"}],"created_at":"2018-01-29T20:19:43Z","forks":{"forkedRepos":[{"id":"1860862_123641682","name":"per-xer-codec","org_name":"Edon07"}]},"full_name":"usdot-jpo-ode/per-xer-codec","language":"Java","languages":{"C":"12886","Java":"115210","Makefile":"4978","Shell":"620"},"organization":{"org_avatar_url":"https://avatars2.githubusercontent.com/u/23056647?v=4","org_type":"Organization","organization":"usdot-jpo-ode","organization_url":"https://github.com/usdot-jpo-ode"},"origin":"PUBLIC","project_description":"JNI Wrapper for SEMI ASN.1 J2735 Extensions","project_name":"per-xer-codec","rank":107,"readMe":{"content":"# ASN.1 J275 + SEMI Extensions PER-XER Codec\n\nThe PER-XER Codec is an open source library which wraps the C code generated by asn1c in a JNI wrapper.\n\n<a name=\"toc\"/>\n\n## Table of Contents\n\n[I. Release Notes](#release-notes)\n\n[II. Documentation](#documentation)\n\n[III. Getting Started](#getting-started)\n\n[IV. Code Generation](#code-generation)\n\n[V. Building the Library (Combined)](#building)\n\n[VI. Building the Library (Standalone Native)](#building-native)\n\n[VII. Usage](#usage)\n\n---\n\n<a name=\"release-notes\" id=\"release-notes\"/>\n\n## [I. Release Notes](ReleaseNotes.md)\n\n<a name=\"documentation\"/>\n\n## II. Documentation\n\nThis project produces two artifacts: A native-code Shared Object library, and a JAR wrapper around that library.\n\n<a name=\"getting-started\"/>\n\n## III. Getting Started\n\nThe following instructions describe the proceedure to fetch, build, and run the application\n\n### Prerequisites\n* git\n* make\n* java 1.8\n* maven\n* asn1c (See below)\n\n---\n### Obtain the Source Code\n\n#### Step 1 - Clone public repository\n\nClone the source code from the GitHub repository using Git command:\n\n```bash\ngit clone --recurse-submodules https://github.com/usdot-jpo-sdcsdw/per-xer-codec.git\n```\n\n<a name=\"code-generation\"/>\n\n## IV. Code Generation\n\n**Step 1**:  Create the various empty directories that git won't track\n\nNavigate to the codegen directory and run the appropriate make rule\n\n```bash\ncd asn1-codegen\nmake directories\n```\n\n**Step 2**: Add the necessary ASN 1 files\n\n```bash\ncp ... src/asn1/ \n```\n\n**Step 3**: Generate the ASN.1 C code\n\nStill in the codegen directory, run the full make, and then install\n\n```bash\nmake\nmake install\n```\n\nThis will:\n* Build the asn1c compiler\n* Install it into asn1-codegen/install/asn1c\n* Generate the c code using the ASN.1 files provided\n* Move them to the native/src/main/c/asn1 directory \n\n\n<a name=\"building\"/>\n\n## V. Building the Library (Combined)\n\nThis step is for building the library as a complete maven project, which will yield both the JNI Jar, and a native shared object library for it to use.\n\n**Step 1**: Follow the instructions in [IV. Code Generation](#code-generation)\n\n**Step 2**: Build using maven \n\nFrom the root directory, and run the maven install goal\n\n```bash\nmvn install\n```\n\nThe maven configuration is set up to detect what OS you are building on, and will install the appropriate native library. Cross compiling is not currently supported. OSX and Linux are currently the only supported OS's.\n\n<a name=\"building-native\"/>\n\n## VI. Building the Library (Standalone Native)\n\nThis step is for building the native shared library by itself, separate from the JNI wrapper\n\n**Step 1**: Follow the instructions in [IV. Code Generation](#code-generation)\n\n**Step 2**: Build using make\n\nFrom the native directory, run make\n\n```bash\ncd native\nmake\n```\n\nThis makefile supports building the Linux Shared object on OSX, using a docker container, and will detect this automatically\n\nThis will:\n    Produce a docker image capable of building the Linux shared object library\n    Produce a container of that image, and build the shared object in it\n    Copy the built shared object out of the container\n    Delete the container\n\n<a name=\"usage\"/>\n\n## VII. Usage\n\n### Intended Usage\n\nThe primary way to use this module is to include the PerXerCodec class, and use its perToXer and xerToPer methods.\nThese methods require 3 things: A PerData format, a XerData Format, and an Asn1Type.\nAsn1Types can be accessed as static members of PerXerCodec (e.g. ServiceRequestType).\nThe PerData can be any object that implements the PerData interface, Likewise for XerData.\nIf the type of data is unknown, the guessPerToXer and guessXerToPer take multiple possible data types and will try each.\n\n### Maven Dependency\n\nMaven projects which wish to use the codec must declare two dependencies, the java wrapper (per-xer-codec-java), and the appropriate shared object (per-xer-codec-native-osx|linux).\n\nIf you wish to run unit tests in your project which depend on the codec, you will need to ensure the native shared object exists at test run time, and specify the java.library.path to your unit test runner.\n\n### Deploying\n\n**Step 1**: Place the jar file generated by the build process on the classpath\n\n**Step 2**: Place the generated shared object file on the java native library path\n    This is set using the java.library.path java system property, i.e. using the ```-Djava.library.path=...``` command line argument. \n    If not provided, this property usually defaults to the system library path.\n    \n</a>","url":"https://raw.githubusercontent.com/usdot-jpo-ode/per-xer-codec/master/README.md"},"releases":[],"repository":"per-xer-codec","repository_url":"https://github.com/usdot-jpo-ode/per-xer-codec","stage_id":"23056647_119436284","stars":1,"suggest":[{"input":["per-xer-codec"],"output":"per-xer-codec# name"},{"input":["per-xer-codec"],"output":"per-xer-codec# name"},{"input":["JNI","Wrapper","for","SEMI","ASN1","J2735","Extensions"],"output":"per-xer-codec# description"},{"input":["Java","C","Makefile","Shell"],"output":"per-xer-codec# languages"},{"input":["andrewm-aero"],"output":"per-xer-codec# contributors"}],"updated_at":"2018-01-24T14:38:06Z","watchers":8}}
{"_index":"projects","_type":"project","_id":"23056647_131851429","_score":1,"_source":{"commits":4,"contributors":1,"contributors_list":[{"avatar_url":"https://avatars2.githubusercontent.com/u/10130982?v=4","profile_url":"https://github.com/hmusavi","user_type":"User","username":"hmusavi"}],"created_at":"2018-05-02T13:03:53Z","forks":{"forkedRepos":[]},"full_name":"usdot-jpo-ode/scms-asn1","languages":{},"organization":{"org_avatar_url":"https://avatars2.githubusercontent.com/u/23056647?v=4","org_type":"Organization","organization":"usdot-jpo-ode","organization_url":"https://github.com/usdot-jpo-ode"},"origin":"PUBLIC","project_description":"This repository contains the ASN.1 definitions for data containers and protocols used in SCMS.","project_name":"scms-asn1","rank":33,"readMe":{"content":"# scms-asn1\n\nThis repository contains the ASN.1 definitions for data containers and protocols\nused in SCMS.\n\n## Initializing the Git Repository\n\nIt's important to note that this repository contains a Git submodule (i.e. a\ndependency on the 1609.2 public repository). Therefore, doing a simple Git\nclone is not enough to pull down the entire repo.\n\nInstructions to clone the SCMS-ASN1 repository are as follows:\n\n```git clone https://github.com/usdot-jpo-ode/scms-asn1.git```, where \n'<username>' is your github username.\n\n\n","url":"https://raw.githubusercontent.com/usdot-jpo-ode/scms-asn1/master/README.md"},"releases":[],"repository":"scms-asn1","repository_url":"https://github.com/usdot-jpo-ode/scms-asn1","stage_id":"23056647_131851429","stars":0,"suggest":[{"input":["scms-asn1"],"output":"scms-asn1# name"},{"input":["scms-asn1"],"output":"scms-asn1# name"},{"input":["This","repository","contains","the","ASN1","definitions","for","data","containers","and","protocols","used","in","SCMS"],"output":"scms-asn1# description"},{"input":[],"output":"scms-asn1# languages"},{"input":["hmusavi"],"output":"scms-asn1# contributors"}],"updated_at":"2018-05-02T20:44:36Z","watchers":6}}
{"_index":"projects","_type":"project","_id":"23056647_95809284","_score":1,"_source":{"commits":15,"contributors":3,"contributors_list":[{"avatar_url":"https://avatars0.githubusercontent.com/u/12912578?v=4","profile_url":"https://github.com/mvs5465","user_type":"User","username":"mvs5465"},{"avatar_url":"https://avatars2.githubusercontent.com/u/10130982?v=4","profile_url":"https://github.com/hmusavi","user_type":"User","username":"hmusavi"},{"avatar_url":"https://avatars0.githubusercontent.com/u/549261?v=4","profile_url":"https://github.com/tonychen091","user_type":"User","username":"tonychen091"}],"created_at":"2017-06-29T18:49:41Z","forks":{"forkedRepos":[]},"full_name":"usdot-jpo-ode/usdot-jpo-ode.github.io","language":"HTML","languages":{"HTML":"732340"},"organization":{"org_avatar_url":"https://avatars2.githubusercontent.com/u/23056647?v=4","org_type":"Organization","organization":"usdot-jpo-ode","organization_url":"https://github.com/usdot-jpo-ode"},"origin":"PUBLIC","project_description":"DOCUMENTATION: Static Website that highlights the Rest API for the ODE","project_name":"usdot-jpo-ode.github.io","rank":60,"readMe":{"content":"# usdot-jpo-ode.github.io\nDOCUMENTATION: Static Website that highlights the Rest API for the ODE\n\nLink: [usdot-jpo-ode.github.io](https://usdot-jpo-ode.github.io/)\n","url":"https://raw.githubusercontent.com/usdot-jpo-ode/usdot-jpo-ode.github.io/master/README.md"},"releases":[],"repository":"usdot-jpo-ode.github.io","repository_url":"https://github.com/usdot-jpo-ode/usdot-jpo-ode.github.io","stage_id":"23056647_95809284","stars":2,"suggest":[{"input":["usdot-jpo-odegithubio"],"output":"usdot-jpo-odegithubio# name"},{"input":["usdot-jpo-ode.github.io"],"output":"usdot-jpo-odegithubio# name"},{"input":["DOCUMENTATION","Static","Website","that","highlights","the","Rest","API","for","the","ODE"],"output":"usdot-jpo-odegithubio# description"},{"input":["HTML"],"output":"usdot-jpo-odegithubio# languages"},{"input":["mvs5465","hmusavi","tonychen091"],"output":"usdot-jpo-odegithubio# contributors"}],"updated_at":"2018-12-18T20:26:23Z","watchers":6}}
{"_index":"projects","_type":"project","_id":"29404495_94217500","_score":1,"_source":{"commits":42,"contributors":3,"contributors_list":[{"avatar_url":"https://avatars1.githubusercontent.com/u/15731135?v=4","profile_url":"https://github.com/James-OHara","user_type":"User","username":"James-OHara"},{"avatar_url":"https://avatars1.githubusercontent.com/u/29869812?v=4","profile_url":"https://github.com/tsaitimothy","user_type":"User","username":"tsaitimothy"},{"avatar_url":"https://avatars2.githubusercontent.com/u/44652226?v=4","profile_url":"https://github.com/mary-vandyke","user_type":"User","username":"mary-vandyke"}],"created_at":"2017-06-13T13:44:25Z","forks":{"forkedRepos":[{"id":"40578364_138880441","name":"microsite","org_name":"AlexaChristianBAH"},{"id":"1730320_102726861","name":"microsite","org_name":"iriberri"}]},"full_name":"usdot-its-jpo-data-portal/microsite","language":"CSS","languages":{"CSS":"319520","HTML":"44942","JavaScript":"37320","Shell":"72"},"organization":{"org_avatar_url":"https://avatars1.githubusercontent.com/u/29404495?v=4","org_type":"Organization","organization":"usdot-its-jpo-data-portal","organization_url":"https://github.com/usdot-its-jpo-data-portal"},"origin":"PUBLIC","project_description":"Public repository tracking progress in development of new USDOT Intelligent Transportation Systems Joint Program Office (ITS JPO) data sharing portal.","project_name":"microsite","rank":94,"readMe":{"content":"﻿# USDOT JPO Microsite Template\n\nThe microsite is a standalone website that has been templated so that small changes can be made in a couple json files to create the desired website look and functionality.  \n\nThe microsite template is designed to provide users of USDOT data a new standardized entry point for them to explore USDOT DOT data and for various groups within the USDOT to create custom data home pages with the template to promote their data.  The template is not intended to be a replacement for data.transportation.gov (DTG) or the National Transportation Library (NTL) sites but instead enhance those sites by providing additional entry points to those two systems in support of the ITS JPO No Wrong Door Policy. \n\nThe website capabilities include querying desired data from a given domain, creating custom categories/buttons that search through the given domain, displaying featured datasets for users to see on page load, displaying system use metrics, detailing data access and retention policy and hosting sample visualizations of data.\n\nThe microsite is developed completely using JavaScript, CSS, and HTML and requires no installation of software to run. External JavaScript and CSS libraries are:\n\n* [JavaScript jQuery](https://jquery.com/) - Used for AJAX calls\n* [JavaScript Vue](https://vuejs.org/) - Framework used for HTML to Javascript data transfer\n* [CSS font-awesome v4.7.0](http://fontawesome.io/) - Used to style the microsite\n* [CSS bulma v0.5.0](https://bulma.io/) - Used to handle grid layouts\n\nJavascript libraries are referenced using CDNs. All CSS files, include custom CSS are under the css folder. [Babel](https://babeljs.io/) is used to convert the usdot_data_microsite_template.js ES2015 JavaScript to be backward compatible and minified. \n\nThe core search functionality of the microsite is written with the Vue Javascript Framework consisting of seven Vue components. \n\n- **DOT header** - This component contains the HTML template for the USDOT standard header at the top of the page.\n- **Navigation top** - This component contains the HTML template for the data microsite's navigation bar.\n- **Microsite footer** - This components contains the HTML template for the data microsite's footer. The email address in the footer is loaded from template_categories.json and can be modified in that file.\n- **Search main** – This component sets the look for the search bar, including the placeholder text advertising the number of available datasets to search. The user enters their search query here and that data is then picked up by the search results component. \n- **Search results** - This components executes the code to pull down the latest NTL datasets into a searchable JSON, then searches DTG and NTL for the query the user entered either under the search bar or by clicking a category. The component also has the template for the search results page, which displays results for all domains in a common format including the dataset title, description and tags, as they are listed on the external domain. The user interface allows for the search results to be ordered by Relevance, Date or Alphabetically. For example, if a user enters “weather” into the search bar the site will return all dataset results from DTG and NTL that include the term “weather” somewhere in the dataset's tags, title or description and display them to the user in a common format. \n- **Category search** – This component executes the code that provides the user with a set of typical search term categories that the user can select from to help them discover data. The category information is set in template_categories.json. \n- **Featured data** – This component executes the code the creats the featured dataset boxes that are links to datasets or visualizations that the site manager wants to highlight. The dataset information is pulled from template_datasets.json and is currently limited to three featured datasets. \n\nThese Vue compontents are then included into HTML pages index.html and search.html that the user access through a standard branching URL. The metrics, about and public access pages are static HTML, but include the header and footer components to make it so that header and footer updates propogate out to the entire site with a single change. \n\n## Getting Started\n\nThe microsite is designed as a template with minimal Javascript updates necessary to be used for new data. A developer interested in search functionality for data.transportation.gov and the National Transportation Library will only need to update template_categories.json and template_datasets.json to set the values for the category search and featured datasets respectively. If a developer is interested in using this code to create a site that searches other domains, they will need to modify some methods under the search-main and search-results Vue components to work with the search API for their chosen domains. Details for the three files are below. \n\n### usdot_data_microsite_template.js\nThis file contains the seven Vue components that make up the microsite's javascript code as detailed above. Vue components are entirely self contained with the HTML used to display the element and the Javascript methods used to build it in a single component. Changes can be made to a component and will automatically be propogated out into each page of the site. The HTML codebase references only the components it needs to reduce load times. Some changes a developer might need to make are:\n\n1. Modifying links provided in the navigation bar. This can be done by changing the HTML in the template value of the nagivation-top component. \n```\nVue.component('navigation-top', {\n    template:`<div class=\"navigation-bar navBarLinks\">\n                 <a class=\"headHovers navBarLinks\" href=\"/\">ITS JPO SITE</a> <div style=\"font-size: 15px; padding:3px 7px 3px 7px; display: inline;\">|</div>\n                 <a class=\"headHovers navBarLinks\" href=\"/data\">HOME</a> <div style=\"font-size: 15px; padding:3px 5px 7px 7px; display: inline;\">|</div>\n                 <a class=\"headHovers navBarLinks\" href=\"/data/about/\">ABOUT</a><div style=\"font-size: 15px; padding:3px 7px 3px 7px; display: inline;\">|</div>\n                 <a class=\"headHovers navBarLinks\" href=\"/data/public-access/\">PUBLIC ACCESS</a> <div style=\"font-size: 15px; padding:3px 7px 3px 7px; display: inline;\">|</div>\n                 <a class=\"headHovers navBarLinks\" href=\"/data/metrics/\">METRICS</a><div style=\"font-size: 15px; padding:3px 7px 3px 7px; display: inline;\">|</div>\n                 <a class=\"headHovers navBarLinks\" href=\"/data/visualizations/\">VISUALIZATIONS</a>\n                 <a class=\"headHovers navBarLinks\" href=\"https://github.com/usdot-its-jpo-data-portal/microsite\" style=\"float:right; text-align:right;padding:3px 7px 3px 7px;\">VIEW THIS PROJECT ON GITHUB</a>\n             </div>`\n} )\n```\n2. Changing where a query is searched. This can be done by making a couple changes to the search-results component. First the developer needs to create their own version of the addNTLtoSearchResult and addSocratatoSearchResult that uses the searching API of their chosen domain. Then the search function needs to be modified to remove unnecessary addtoSearchResult functions and include the new one.\n ```\n addSocratatoSearchResult: function (search_query) {\n    var itemCount;\n    var self = this;\n    $.get(self.socrata_url + search_query + '&search_context=' + self.socrata_domain, function (items) {\n        for (itemCount = 0; itemCount < items.results.length; itemCount++) {\n            var tempJson = {};\n            tempJson[\"name\"] = items.results[itemCount].resource.name;\n            tempJson[\"description\"] = items.results[itemCount].resource.description;\n            // if string only has year then only print year, otherwise parse into formatting\n            tempJson[\"date\"] = (items.results[itemCount].resource.updatedAt.substring(0,10) < 7) ? items.results[itemCount].resource.updatedAt.substring(0,10) : self.formatDate(items.results[itemCount].resource.updatedAt.substring(0,10));\n            var tagCount;\n            var allTags = [];\n            for (tagCount = 0; tagCount < items.results[itemCount].classification.domain_tags.length; tagCount++) {\n                allTags[tagCount] = items.results[itemCount].classification.domain_tags[tagCount];\n            }\n            tempJson[\"tags\"] = allTags;\n            tempJson[\"tags\"].sort();\n            tempJson[\"link\"] = items.results[itemCount].link;\n            self.searchResults.push(tempJson);\n        }\n    });\n},\n ```\n3. Changing the placeholder text for the search box. The placeholder text is set in the search-main component method dataset count. The developer can change this so that the self.search_placeholder variable is set to a new value. \n```\n        datasetCount: function () {\n            var self = this;\n            $.get(self.socrata_url + '&search_context=' + self.socrata_domain, function (data) {\n                self.totalDataCount = data.results.length;\n                self.search_placeholder = self.totalDataCount.toString() + \" data sets and counting!\";\n            });\n```\n### template_categories.json:\nThis file contains all the category data and extra data (the background image on the main page and the contact email in the footer). Below is a cut out of what the category search looks like on the microsite and the JSON associated with the screenshot. \n![alt text](images/screenshots/ButtonScreenshot.png \"Category Layout Screenshot\")\n\n\"background_image\" is a chosen background image placed in \"/images\" will appear behind the search bar.  \n\"contact_email\": is the email that will be displayed in the footer of the page.\n\n```\n    \"background_image\": \"./images/Transportation Background.png\",\n    \"contact_email\": \"data.itsjpo@dot.gov\",\n```\n\nThe category search buttons are a list of JSON objects under the key \"buttons\". The buttons have a category name, image, rollover image and altText to be 508 compliant. Place all the icon image files for the categories in the \"/images/icons\" folder and reference them in the \"imgIcons\" and \"rolloverImages\" JSON field. The \"rolloverImages\" should be the same image exaggerated in some manner to make it clear that the user is clicking on that category.\n\n```\n \"buttons\": [\n        {\n            \"CategoryName\": \"Connected Vehicle Message\",\n            \"imgIcons\": \"/data/images/icons/CVPCars.png\",\n            \"rolloverImages\": \"/data/images/icons/CVPCars_Rollover.png\",\n            \"altText\": \"search for connected vehicle message\"\n        }, {\n            \"CategoryName\": \"Application Message\",\n            \"imgIcons\": \"/data/images/icons/AppMessage.png\",\n            \"rolloverImages\": \"/data/images/icons/AppMessage_Rollover.png\",\n            \"altText\": \"search for application message\"\n        }, {\n        ...\n```\n\n### template_datasets.json:\nThis file contains information for the featured datasets.  Below is a cut out of what the featured datasets look like on the microsite and the JSON associated with the screenshot.\n![alt text](images/screenshots/DatasetsScreenshot.PNG \"Datasets Layout Screenshot\")\nDataset Layout in JSON Format:\nThere are five fields for each dataset:\n1. 'name' This is the name that will be displayed.  If no name is provided the title of the page that is pulled from the URL will be used.\n2. 'description' This is the description of the dataset that will be displayed.  If no description is provided the description of the page that is pulled from the URL will be used.\n3. 'url' The URL is where the data will come from.\n4. 'image' The image is the image that will be displayed.\n5. 'altText' This is the alternate image text for accessibility.\n\n```\n    \"datasets\": [\n        {\n            \"name\": \"Wyoming CV Pilot Basic Safety Messages Sample\",\n            \"description\": \"Contains a sample of sanitized Basic Safety Messages collected from the Wyoming Connected Vehicle pilot.\",\n            \"url\": \"https://data.transportation.gov/Automobiles/Wyoming-CV-Pilot-Basic-Safety-Message-One-Day-Samp/9k4m-a3jc\",\n            \"image\": \"./images/Wyoming DOT Connected Vehicle Pilot.png\",\n            \"altText\": \"Link to sample Wyoming CV Pilot Basic Safety Messages on data.transportation.gov\"\n        },\n        {\n            \"name\": \"Wyoming Connected Vehicle Pilot Basic Safety Message Visualization\",\n            \"description\": \"A visualization portal for the Wyoming CV Pilot Basic Safety Messages.\",\n            \"url\": \"https://data.transportation.gov/Automobiles/Wyoming-Connected-Vehicle-Pilot-Basic-Safety-Messa/hchs-a7s6\",\n            \"image\": \"./images/Wyoming Connected Vehicle Pilot Basic Safety Message Visualization.png\",\n            \"altText\": \"Map and histograms of Wyoming CV Pilot Basic Safety Messages on data.transportation.gov\"\n        },\n        {\n            \"name\": \"Proof-of-Concept Vehicle Platooning Based on CACC\",\n            \"description\": \"Performance of a proof-of-concept vehicle platooning based on Cooperative Adaptive Cruise Control (CACC).\",\n            \"url\": \"https://data.transportation.gov/Automobiles/Test-Data-of-Proof-of-Concept-Vehicle-Platooning-B/wpek-zziu\",\n            \"image\": \"./images/video.png\",\n            \"altText\": \"Link to Test Data of Proof-of-Concept Vehicle Platooning Based on Cooperative Adaptive Cruise Control (CACC) on data.transportation.gov\"\n        }\n    ]\n```\n\n## Testing\n\n1. In order to run tests node package manager is installed (npm) must be installed.\n\n2. 'npm install' must then be run in the '/microsite' directory.  This command installs all dependencies specified in the package.json\n\n2. Open up a terminal at '/microsite' and run the command 'http-server'.  This will create an instance of the website to run locally.\n\n3. Navigate into the '/microsite/test' directory and from another terminal run the shell script using ./testRunner.js in the terminal. This script will run the browser tests.\n\nThe test cases use a variety of different softwares to run these tests\n* [Selenium Webdriver](http://www.seleniumhq.org/projects/webdriver/) - Software used to run the tests in the three web browsers IE, Firefox, and Chrome.\n* [Mocha JS](https://mochajs.org/) - Framework used to handle the test cases.\n* [Chai JS](http://chaijs.com/) - Assertion library used in conjunction with MochaJS.\n* [http-server](https://www.npmjs.com/package/http-server) - HTTP server created to run the website locally\n\n\n## Versioning\n\n* **Version 1.0** -  Initial version of the template \n\n\n## License\n\nThis project is licensed under the Apache License - see the [LICENSE.txt](LICENSE.txt) file for details","url":"https://raw.githubusercontent.com/usdot-its-jpo-data-portal/microsite/master/README.md"},"releases":[],"repository":"microsite","repository_url":"https://github.com/usdot-its-jpo-data-portal/microsite","stage_id":"29404495_94217500","stars":7,"suggest":[{"input":["microsite"],"output":"microsite# name"},{"input":["microsite"],"output":"microsite# name"},{"input":["Public","repository","tracking","progress","in","development","of","new","USDOT","Intelligent","Transportation","Systems","Joint","Program","Office","ITS","JPO","data","sharing","portal"],"output":"microsite# description"},{"input":["CSS","HTML","JavaScript","Shell"],"output":"microsite# languages"},{"input":["James-OHara","tsaitimothy","mary-vandyke"],"output":"microsite# contributors"}],"updated_at":"2018-11-16T16:22:21Z","watchers":4}}
{"_index":"projects","_type":"project","_id":"29404495_94217326","_score":1,"_source":{"commits":9,"contributors":1,"contributors_list":[{"avatar_url":"https://avatars1.githubusercontent.com/u/15731135?v=4","profile_url":"https://github.com/James-OHara","user_type":"User","username":"James-OHara"}],"created_at":"2017-06-13T13:42:48Z","forks":{"forkedRepos":[]},"full_name":"usdot-its-jpo-data-portal/performance-dashboard","language":"Python","languages":{"Python":"18805","Shell":"252"},"organization":{"org_avatar_url":"https://avatars1.githubusercontent.com/u/29404495?v=4","org_type":"Organization","organization":"usdot-its-jpo-data-portal","organization_url":"https://github.com/usdot-its-jpo-data-portal"},"origin":"PUBLIC","project_description":"Public repository tracking progress in development of new USDOT ITS JPO data sharing portal metrics dashboard.","project_name":"performance-dashboard","rank":27,"readMe":{"content":"# Performance Dashboard\nThis repository includes the three scripts used by the USDOT team to access performance metrics for data.transportation.gov, Amazon Web Services s3 and GitHub and upload them to two separate locations. The first is an internal Postgres database that will keep an all time historical record. The second is a Google Sheets object which keeps a running monthly total that are accessed by Google Data Studio to create the performance dashboard on the ITS Public Data Hub. This process is designed to be run daily. These scripts have been stripped of personal information and therefore will not run as-is and require information as noted in the files themselves. All code is written for Python 3.6. \n\nFor more information on the Google Drive Web API: https://developers.google.com/drive/v3/web/about-sdk\n\nFor more information on psycopg2: http://initd.org/psycopg/docs/\n\nFor more information on GitHub Traffic API: https://developer.github.com/v3/repos/traffic/\n","url":"https://raw.githubusercontent.com/usdot-its-jpo-data-portal/performance-dashboard/master/README.md"},"releases":[],"repository":"performance-dashboard","repository_url":"https://github.com/usdot-its-jpo-data-portal/performance-dashboard","stage_id":"29404495_94217326","stars":3,"suggest":[{"input":["performance-dashboard"],"output":"performance-dashboard# name"},{"input":["performance-dashboard"],"output":"performance-dashboard# name"},{"input":["Public","repository","tracking","progress","in","development","of","new","USDOT","ITS","JPO","data","sharing","portal","metrics","dashboard"],"output":"performance-dashboard# description"},{"input":["Python","Shell"],"output":"performance-dashboard# languages"},{"input":["James-OHara"],"output":"performance-dashboard# contributors"}],"updated_at":"2018-11-07T19:24:53Z","watchers":1}}
{"_index":"projects","_type":"project","_id":"29404495_109170470","_score":1,"_source":{"commits":97,"contributors":5,"contributors_list":[{"avatar_url":"https://avatars3.githubusercontent.com/u/8136928?v=4","profile_url":"https://github.com/jasonnance","user_type":"User","username":"jasonnance"},{"avatar_url":"https://avatars2.githubusercontent.com/u/205364?v=4","profile_url":"https://github.com/clayheaton","user_type":"User","username":"clayheaton"},{"avatar_url":"https://avatars1.githubusercontent.com/u/15731135?v=4","profile_url":"https://github.com/James-OHara","user_type":"User","username":"James-OHara"},{"avatar_url":"https://avatars1.githubusercontent.com/u/10566989?v=4","profile_url":"https://github.com/wengerm50","user_type":"User","username":"wengerm50"},{"avatar_url":"https://avatars1.githubusercontent.com/u/29869812?v=4","profile_url":"https://github.com/tsaitimothy","user_type":"User","username":"tsaitimothy"}],"created_at":"2017-11-01T18:50:43Z","forks":{"forkedRepos":[]},"full_name":"usdot-its-jpo-data-portal/RDE-Visualization-Website","language":"JavaScript","languages":{"CSS":"141813","HTML":"115609","JavaScript":"3726493"},"organization":{"org_avatar_url":"https://avatars1.githubusercontent.com/u/29404495?v=4","org_type":"Organization","organization":"usdot-its-jpo-data-portal","organization_url":"https://github.com/usdot-its-jpo-data-portal"},"origin":"PUBLIC","project_description":"Prototype visualizations of selected data originally developed for FHWA's Research Data Environment and updated for ITS JPO's ITS Public Data Hub.","project_name":"RDE-Visualization-Website","rank":136,"readMe":{"content":"# RDE Visualization Website\nThis repository contains code and other assets to render a website displaying prototype visualizations of selected data sets in the [Federal Highway Administration](https://www.fhwa.dot.gov/) (FHWA)'s [Research Data Environment](https://www.its-rde.net/) (RDE).  These visualizations are designed to give researchers and data aficionados ideas for projects using the data on the RDE; they also strive to provide concrete examples of how large, complex data sets can be distilled into useful products.\n\n## Running the Site\n\nClone this repository and enter the repository root; for example:\n\n```\ngit clone https://github.com/FHWA/RDE-Visualization-Website.git\ncd RDE-Visualization-Website\n```\n\nYou will need to run an HTTP server that supports Content-Range headers (i.e., not Python's `SimpleHTTPServer`/`http.server`) in the repository root.  For, example, using node.js' [http-server](https://github.com/indexzero/http-server):\n\n```\nhttp-server -p 8080\n```\n\nNow, you should be able to navigate [here](http://localhost:8080) to view the site.\n\n## Visualization Elements\n\nThe site is divided into 6 visualization elements, each designed to emphasize a particular aspect of the data environments on the RDE.  Each visualization element page includes the visualization, a short description of the methodology and technologies used, and download instructions for the code and scripts used to create the visualization.\n\n### Element 1: Characteristics of Transportation Systems\n![Element 1](img/el1example.jpg)\nThe Pasadena data environment in the RDE contains road-link level simulated traffic data for a period of several months in 2011. The visualization shows simulated traffic for the 24-hour period of October 1st, 2011, around a busy part of town.  The visualization uses [BabylonJS](http://www.babylonjs.com/) for 3D graphics.\n\n### Element 2: Connected Vehicle Communication\n![Element 2](img/el2example.jpg)\nThe Federal Highway Administration's Connected Vehicle Safety Pilot Program studies the utility and effectiveness of communicating safety data between vehicles and roadside equipment (RSE). The visualization focuses on a single RSE and plots the quality and quantity of data received by the equipment from vehicles with on-board devices capable of broadcasting safety data.  Utilizes [Leaflet](http://leafletjs.com/) for mapping and [d3](https://d3js.org/) for displaying data and making charts.\n\n### Element 3: Impact of Weather\n![Element 3](img/el3example.jpg)\nThe Federal Highway Administration also maintains the [Weather Data Environment](https://wxde.fhwa.dot.gov/), known as the WxDE, which is a repository of traffic-related weather data that can be used in research. Visualization element 3 combines data from the WxDE with data from the Minnesota DOT Mobile Observation data environment, on the RDE, to show weather, movement of snowplows, and road conditions during a blizzard in February of 2014.  It uses [WebGL](https://en.wikipedia.org/wiki/WebGL) to visualize data over a [Leaflet](http://leafletjs.com/) map.\n\n### Element 4: Effect of Interventions\n![Element 4](img/el4example.png)\nMulti-Modal Intelligent Traffic Signal Systems (MMITSS) is a technology that allows traffic signals to communicate with vehicles and possibly behave differently for different types of vehicles. For example, freight trucks may trigger traffic lights to turn green in an effort to reduce congestion. Element 4 provides an interactive interface for looking at the difference in traffic during a MMITSS field test in Anthem, Arizona.  Uses [d3](https://d3js.org/) to draw charts.\n\n### Element 5: Exploring the RDE\n![Element 5](img/el5example.jpg)\nThe RDE contains a variety of data environments totaling over 300 Gigabytes of compressed data. Element 5 is an interactive dashboard that allows users to explore the datasets available in the RDE. By filtering on attributes of the dataset, it is easy to narrow the list of 161 down to those matching your interests.  This is a [dc.js](https://dc-js.github.io/dc.js/) dashboard that includes a [Leaflet](http://leafletjs.com/) mapping component.\n\n### Element 6: Breadth and Depth of Connected Vehicle Data\n![Element 6](img/el6example.png)\nWhile visualization element 5 presents the depth of data available in the RDE, Element 6 presents a view into the breadth of data that is available in one of the many RDE data environments. Element 6 is an interactive visual overview of the Basic Safety Message data set in the Safety Pilot Model Deployment data enviroment that logs vehicles' attempts to communicate with roadside equipment, as also shown in Element 2.  Utilizes [d3](https://d3js.org/) for charting.\n\n### Data Preparation\nSee the [RDE-Visualization-Support](https://github.com/FHWA/RDE-Visualization-Support) repository for the scripts used to create the data files used for these visualizations.\n","url":"https://raw.githubusercontent.com/usdot-its-jpo-data-portal/RDE-Visualization-Website/master/README.md"},"releases":[],"repository":"RDE-Visualization-Website","repository_url":"https://github.com/usdot-its-jpo-data-portal/RDE-Visualization-Website","stage_id":"29404495_109170470","stars":2,"suggest":[{"input":["RDE-Visualization-Website"],"output":"RDE-Visualization-Website# name"},{"input":["RDE-Visualization-Website"],"output":"RDE-Visualization-Website# name"},{"input":["Prototype","visualizations","of","selected","data","originally","developed","for","FHWAs","Research","Data","Environment","and","updated","for","ITS","JPOs","ITS","Public","Data","Hub"],"output":"RDE-Visualization-Website# description"},{"input":["JavaScript","CSS","HTML"],"output":"RDE-Visualization-Website# languages"},{"input":["jasonnance","clayheaton","James-OHara","wengerm50","tsaitimothy"],"output":"RDE-Visualization-Website# contributors"}],"updated_at":"2018-04-06T15:15:21Z","watchers":2}}
{"_index":"projects","_type":"project","_id":"29147435_67631906","_score":1,"_source":{"commits":89,"contributors":2,"contributors_list":[{"avatar_url":"https://avatars0.githubusercontent.com/u/29106613?v=4","profile_url":"https://github.com/mtgoodwin","user_type":"User","username":"mtgoodwin"},{"avatar_url":"https://avatars2.githubusercontent.com/u/8005360?v=4","profile_url":"https://github.com/ldnash","user_type":"User","username":"ldnash"}],"created_at":"2016-09-07T18:12:46Z","forks":{"forkedRepos":[]},"full_name":"VolpeUSDOT/PLT-Web-Map","language":"CSS","languages":{"CSS":"22934","HTML":"16576","JavaScript":"12019"},"organization":{"org_avatar_url":"https://avatars0.githubusercontent.com/u/29147435?v=4","org_type":"Organization","organization":"VolpeUSDOT","organization_url":"https://github.com/VolpeUSDOT"},"origin":"PUBLIC","project_description":"Interactive map showing regional boundaries of Public Lands Team partner agencies. For Volpe PLT webpage.","project_name":"PLT-Web-Map","rank":103,"readMe":{"content":"","url":""},"releases":[],"repository":"PLT-Web-Map","repository_url":"https://github.com/VolpeUSDOT/PLT-Web-Map","stage_id":"29147435_67631906","stars":0,"suggest":[{"input":["PLT-Web-Map"],"output":"PLT-Web-Map# name"},{"input":["PLT-Web-Map"],"output":"PLT-Web-Map# name"},{"input":["Interactive","map","showing","regional","boundaries","of","Public","Lands","Team","partner","agencies","For","Volpe","PLT","webpage"],"output":"PLT-Web-Map# description"},{"input":["CSS","HTML","JavaScript"],"output":"PLT-Web-Map# languages"},{"input":["mtgoodwin","ldnash"],"output":"PLT-Web-Map# contributors"}],"updated_at":"2017-11-06T16:30:22Z","watchers":1}}
{"_index":"projects","_type":"project","_id":"29147435_88868655","_score":1,"_source":{"commits":75,"contributors":2,"contributors_list":[{"avatar_url":"https://avatars0.githubusercontent.com/u/29106613?v=4","profile_url":"https://github.com/mtgoodwin","user_type":"User","username":"mtgoodwin"},{"avatar_url":"https://avatars2.githubusercontent.com/u/8005360?v=4","profile_url":"https://github.com/ldnash","user_type":"User","username":"ldnash"}],"created_at":"2017-04-20T13:28:34Z","forks":{"forkedRepos":[]},"full_name":"VolpeUSDOT/rec-gov-analysis","language":"Python","languages":{"Python":"65923"},"organization":{"org_avatar_url":"https://avatars0.githubusercontent.com/u/29147435?v=4","org_type":"Organization","organization":"VolpeUSDOT","organization_url":"https://github.com/VolpeUSDOT"},"origin":"PUBLIC","project_description":"Tools for analyzing recreation.gov historical site reservations for transportation planning","project_name":"rec-gov-analysis","rank":89,"readMe":{"content":"# Recreation.gov Historical Reservations Analysis tools\n\nThis repository contains tools to help analyze and use open recreation.gov reservation data for transportation and recreation planning. The Volpe Center developed these tools to help develop a summary for the U.S. Forest Service of how this reservation data could be used in transportation planning within national forests.\n\n## Known Issues\nThis code was developed for the specific purpose of assisting with an internal analysis and is provided as-is in the hope that others working with this data will find the tools useful and be able to build from them for their own needs. These reservations are a rich data source that could benefit from additional analysis beyond the scope of these exploratory tools, especially geospatial analysis.\n\nWe welcome contributions back to this code base to improve these tools for other users. All code contributions are subjects to the public domain license and conditions described in [CONTRIBUTING](CONTRIBUTING.md).\n\n## Data\nThe historical reservation data is available from Recreation.gov [here](https://ridb.recreation.gov/?action=datadownload).\n\n## Installation and Configuration\nBasic information about setting up and using these tools is available in the [instructions](Instructions.md).\n\n## Dependencies\n * [python v3](https://www.python.org/)\n * [Pandas](https://pandas.pydata.org/)\n * [Sqlite3](https://sqlite.org/)\n","url":"https://raw.githubusercontent.com/VolpeUSDOT/rec-gov-analysis/master/README.md"},"releases":[],"repository":"rec-gov-analysis","repository_url":"https://github.com/VolpeUSDOT/rec-gov-analysis","stage_id":"29147435_88868655","stars":0,"suggest":[{"input":["rec-gov-analysis"],"output":"rec-gov-analysis# name"},{"input":["rec-gov-analysis"],"output":"rec-gov-analysis# name"},{"input":["Tools","for","analyzing","recreationgov","historical","site","reservations","for","transportation","planning"],"output":"rec-gov-analysis# description"},{"input":["Python"],"output":"rec-gov-analysis# languages"},{"input":["mtgoodwin","ldnash"],"output":"rec-gov-analysis# contributors"}],"updated_at":"2018-01-17T15:10:01Z","watchers":1}}
{"_index":"projects","_type":"project","_id":"23056647_164494056","_score":1,"_source":{"commits":3,"contributors":1,"contributors_list":[{"avatar_url":"https://avatars0.githubusercontent.com/u/12912578?v=4","profile_url":"https://github.com/mvs5465","user_type":"User","username":"mvs5465"}],"created_at":"2019-01-07T20:54:03Z","forks":{"forkedRepos":[]},"full_name":"usdot-jpo-ode/jpo-record-parser","language":"Java","languages":{"Dockerfile":"288","Java":"73591"},"organization":{"org_avatar_url":"https://avatars2.githubusercontent.com/u/23056647?v=4","org_type":"Organization","organization":"usdot-jpo-ode","organization_url":"https://github.com/usdot-jpo-ode"},"origin":"PUBLIC","project_name":"jpo-record-parser","rank":28,"readMe":{"content":"# jpo-record-parser\n","url":"https://raw.githubusercontent.com/usdot-jpo-ode/jpo-record-parser/master/README.md"},"releases":[],"repository":"jpo-record-parser","repository_url":"https://github.com/usdot-jpo-ode/jpo-record-parser","stage_id":"23056647_164494056","stars":0,"suggest":[{"input":["jpo-record-parser"],"output":"jpo-record-parser# name"},{"input":["jpo-record-parser"],"output":"jpo-record-parser# name"},{"input":[""],"output":"jpo-record-parser# description"},{"input":["Java","Dockerfile"],"output":"jpo-record-parser# languages"},{"input":["mvs5465"],"output":"jpo-record-parser# contributors"}],"updated_at":"2019-01-09T19:51:12Z","watchers":5}}
{"_index":"projects","_type":"project","_id":"23056647_160194771","_score":1,"_source":{"commits":171,"contributors":4,"contributors_list":[{"user_type":"User","avatar_url":"https://avatars1.githubusercontent.com/u/45394909?v=4","profile_url":"https://github.com/acosta-dani-bah","username":"acosta-dani-bah"},{"user_type":"User","avatar_url":"https://avatars0.githubusercontent.com/u/45435037?v=4","profile_url":"https://github.com/natedeshmukhtowery","username":"natedeshmukhtowery"},{"user_type":"User","avatar_url":"https://avatars2.githubusercontent.com/u/2624577?v=4","profile_url":"https://github.com/arielsgold","username":"arielsgold"},{"user_type":"User","avatar_url":"https://avatars0.githubusercontent.com/u/45495077?v=4","profile_url":"https://github.com/Newland-Agbenowosi-BAH","username":"Newland-Agbenowosi-BAH"}],"created_at":"2018-12-03T13:25:35Z","forks":{"forkedRepos":[{"name":"jpo-wzdx","id":"8418873_161197979","org_name":"acost22d"}]},"full_name":"usdot-jpo-ode/jpo-wzdx","languages":{},"organization":{"org_avatar_url":"https://avatars2.githubusercontent.com/u/23056647?v=4","org_type":"Organization","organization":"usdot-jpo-ode","organization_url":"https://github.com/usdot-jpo-ode"},"origin":"PUBLIC","project_name":"jpo-wzdx","rank":195,"readMe":{"content":"### Work Zone Data Exchange (WZDx)\n\n# What is the WZDx Specification?\nThe Work Zone Data Exchange (WZDx) Specification enables infrastructure owners and operators (IOOs) to make harmonized work zone data available for third party use. The intent is to make travel on public roads safer and more efficient through ubiquitious access to data on work zone activity. Specifically, the project aims to get data on work zones into vehicles to help automated driving systems (ADS) and human drivers navigate more safely. \n\n# Why is WZDx being developed?\nImproving access to work zone data is one of the top needs identified through the US Department of Transportation (USDOT) [Data for Automated Vehicle Integration (DAVI)](https://www.transportation.gov/av/data) effort. \n\nUp-to-date information about dynamic conditions occurring on roads – such as construction events – can help ADS and humans navigate safely and efficiently. Many IOOs maintain data on work zone activity. However, a lack of common data standards and convening mechanisms makes it difficult and costly for third parties – including original equipment manufacturers (OEMs) and navigation applications – to access and use these data across various jurisdictions. \n\nThus, inspired by [GTFS](https://developers.google.com/transit/gtfs/reference/), USDOT launched WZDx to jumpstart the voluntary adoption of a basic work zone data specification through collaboration with data producers and data users. Longer term, the goal is to enable collaborative maintenance and expansion of the specification to meet the emerging needs of ADS.\n\n## Who is involved in developing WZDx?\nThe [Federal Highway Administration (FHWA)](https://www.fhwa.dot.gov/) and [Intelligent Transportation Systems Joint Program Office (ITS JPO)](https://www.its.dot.gov/) are co-leading the early stages of the WZDx project. \n\nSeveral **data producers** and **data users** (i.e., the WZDx Working Group) voluntarily developed v1.1 of the specification in collaboration with USDOT. Below are the members of the WZDx Working Group:\n\n| &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Data Producers &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Data Users &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n|     :------------:      |     :------------:      |\n|     Pennsylvania Turnpike Authority<br>(also representing the Smart Belt Coalition)     |     HERE      |\n|     Michigan Department of Transportation     |     Waze      |\n|     Iowa Department of Transportation     |     Panasonic     |\n|     Colorado Department of Transportation     |     Toyota      |\n|     Kentucky Department of Transportation     |     Uber      |\n|     iCone     |     Embark      |\n\n## How can I participate?\nUSDOT welcomes feedback and comments on the v1.1 specification. Comments can be made by making an Github Issue, while suggested changes can be made using a pull request. \n\n[v1.1](https://github.com/usdot-jpo-ode/jpo-wzdx/blob/master/full-spec/full-spec.md) is now available for IOOs to stand up data feeds. Once these data feeds are available, OEMs, navigation applications, and others can use the data. Below are steps for IOOs to get started. \n\n1. Continue reading about the [Purpose and Scope](#purpose-and-scope)\n2. Learn about using GitHub as a [tool for collaboration and support](https://github.com/usdot-jpo-ode/jpo-wzdx/blob/master/create-feed/README.md#collaborate-via-github).\n3. Use the [Data Tables](https://github.com/usdot-jpo-ode/jpo-wzdx/blob/master/data-tables/README.md) to understand the data components of the spec.\n4. [Create your own feed](https://github.com/usdot-jpo-ode/jpo-wzdx/blob/master/create-feed/README.md) using the example feeds and learn about the supported data files.\n5. Test your feed with validation tools. **(TBA)**\n6. Publish your feed. **(TBA)**\n\nThis project will be updated with resources to help with implementation; in the meantime, please make a GitHub issue if you need help implementing the specification.\n\n## Purpose and Scope\n\nThis specification was developed through collaboration with the WZDx Working Group to describe a set of “common core” data concepts, their meaning, and their enumeration (as applicable) in order to standardize a data feed specification to be used to publish work zone information.\n\nFor purposes of this effort, “common core” is defined as data elements needed for most (if not all) possible work zone data use cases. The data specification includes data elements that data producers (i.e., State transportation agencies and other IOOs) are already producing (“required”) as well as those that may not currently be produced (“optional”). This common core is designed to be extensible, meaning both required and optional data elements can be added to support specific use cases now and in the future.\n\nThe WZDx data specification will be incrementally enhanced to evolve into a data standard that supports advanced warnings to automated vehicles in and around work zones. The [full and current version, (WZDx v1.1)](https://github.com/usdot-jpo-ode/jpo-wzdx/blob/master/full-spec/full-spec.md) which is included in this repository, will serve as a first step in this effort. It highlights common core elements which serve as a foundation for required data. This version addresses work zone information currently supported by existing data feeds published by public and private sector organizations.\n\n","url":"https://raw.githubusercontent.com/usdot-jpo-ode/jpo-wzdx/master/README.md"},"releases":[],"repository":"jpo-wzdx","repository_url":"https://github.com/usdot-jpo-ode/jpo-wzdx","stage_id":"23056647_160194771","stars":0,"suggest":[{"output":"jpo-wzdx# name","input":["jpo-wzdx"]},{"output":"jpo-wzdx# name","input":["jpo-wzdx"]},{"output":"jpo-wzdx# description","input":[""]},{"output":"jpo-wzdx# languages","input":[]},{"output":"jpo-wzdx# contributors","input":["acosta-dani-bah","natedeshmukhtowery","arielsgold","Newland-Agbenowosi-BAH"]}],"updated_at":"2019-01-09T14:33:38Z","watchers":1}}
{"_index":"projects","_type":"project","_id":"29404495_99607823","_score":1,"_source":{"commits":8,"contributors":2,"contributors_list":[{"user_type":"User","avatar_url":"https://avatars1.githubusercontent.com/u/15731135?v=4","profile_url":"https://github.com/James-OHara","username":"James-OHara"},{"user_type":"User","avatar_url":"https://avatars2.githubusercontent.com/u/6864257?v=4","profile_url":"https://github.com/chuehlien","username":"chuehlien"}],"created_at":"2017-08-07T18:31:38Z","forks":{"forkedRepos":[{"name":"sandbox","id":"6864257_160430561","org_name":"chuehlien"},{"name":"sandbox","id":"30755361_120250741","org_name":"PrinkleSharma"},{"name":"sandbox","id":"7633685_120007209","org_name":"stanvir"},{"name":"sandbox","id":"15110729_118955436","org_name":"shuaidongzhao"},{"name":"sandbox","id":"14318360_118943959","org_name":"caocscar"},{"name":"sandbox","id":"13427284_109106862","org_name":"wayties"},{"name":"sandbox","id":"6037092_100713586","org_name":"errinjohnson"}]},"full_name":"usdot-its-jpo-data-portal/sandbox","language":"Jupyter Notebook","languages":{"Jupyter Notebook":"55548","Python":"27448"},"organization":{"org_avatar_url":"https://avatars1.githubusercontent.com/u/29404495?v=4","org_type":"Organization","organization":"usdot-its-jpo-data-portal","organization_url":"https://github.com/usdot-its-jpo-data-portal"},"origin":"PUBLIC","project_description":"Public repository tracking progress in development of new USDOT ITS JPO data sandbox.","project_name":"sandbox","rank":59,"readMe":{"content":"# Accessing CV Pilots Data From a Public Amazon S3 Bucket\n\n\n**Table of Contents**\n\n* [Background](#backgound)\n\t* [Related ITS JPO Projects](#related-its-jpo-projects)\n* [Getting Started](#getting-started)\n\t* [Prerequisites for using AWS CLI](#prerequisites-for-using-aws-cli)\n\t* [Accessing Files through AWS CLI](#accessing-files-through-aws-cli)\n\t* [Directory Structure](#directory-structure)\n\t* [Downloading from S3](#downloading-from-s3)\n * [Data Types](#data-types)\n \t* [Wyoming CV Data](#wyoming-cv-data)\n* [Get Involved](#get-involved)\n\n## Background\nThis repository contains information on accessing complete data sets from the United States Department of Transportation (USDOT) Joint Program Office (JPO) data program. It is meant to propose a data folder hierarchy to structure the processed data ingested from the Connected Vehicles (CV) Pilot programs and other streaming data sources. Currently this is a beta system using a folder hierarchy for processed Basic Safety Messages (BSM) and Traveler Information Messages (TIM) from the Wyoming CV Pilot site.\n\nUSDOT JPO is soliciting user feedback on the current folder hierarchy to determine what the best approach is and to help inform future directory hierarchies for other data types. To provide input on the hierarchy or the data please [Open an Issue](https://github.com/usdot-its-jpo-data-portal/sandbox/issues). \n\nThe AWS S3 bucket provides an alternative that is [similar to traversing a directory structure](http://usdot-its-cvpilot-public-data.s3.amazonaws.com/index.html). The intention of the hierarchy is to: \n\n- Provide a consistent structure within a pilot program\n- Be easily understood by a human traversing the directories\n- Be structured sufficiently so third parties can build software applications using the data\n- Be flexible enough to capture different data types. \n\nThe expectation is that different data types will lend themselves to different directory hierarchies. In addition, the pilot sites may have compelling reasons to organize the data in different hierarchies for the same data type. The below hierarchy is intended for processed BSMs from the Wyoming CV Pilot site. \n\nAdditional information about CV data is available at:\n\n- [ITS JPO Connected Vehicles (CV) Pilot Deployment Program](https://www.its.dot.gov/pilots/cv_pilot_plan.htm)-  The pilot deployments are expected to integrate connected vehicle research concepts into practical and effective elements, enhancing existing operational capabilities.\n- [J2735 Standard](http://standards.sae.org/j2735_201603/) -  Standard for CV data\n- [General CV information: Vehicle Based Data and Availability](https://www.its.dot.gov/itspac/october2012/PDF/data_availability.pdf) - General introduction slides on CV data\n- [Sample of the WYDOT BSM data](https://data.transportation.gov/Automobiles/Wyoming-CV-Pilot-Basic-Safety-Message-One-Day-Samp/9k4m-a3jc) - Sample of WYDOT BSM data\n\n### Related ITS JPO Projects\n\n- [ITS JPO Data Site ](https://www.its.dot.gov/data/) -  ITS JPO data site which allows users to search for various ITS data.\n- [Operational Data Environment (ODE)](https://github.com/usdot-jpo-ode/jpo-ode) - This ITS JPO Open Source tool is used to collect and process Connected Vehicle data in near real time, and route it to other data repositories, including the Amazon S3 bucket.  \n- [Privacy Module](https://github.com/usdot-jpo-ode/jpo-cvdp) - This  ITS JPO Open source module is used to sanitize the data to ensure no personal information is shared with the public.  \n- [Connected Vehicles Performance Evaluation Platform (CVPEP)](https://github.com/usdot-jpo-sdc) - Limited access Platform for storing raw CV data for evaluation.\n\n\n\n## Getting Started\n\nThere are two ways to access the full data sets on Amazon s3. The first way is through the [Web Interface](http://usdot-its-cvpilot-public-data.s3.amazonaws.com/index.html). This allows the user to browse through the folder structure and click and download individual BSMs. Alternatively, the data can be downloaded programmatically using the Amazon Command Line Interface (CLI) by following the directions below.\n\n### Prerequisites for using AWS CLI\n\n1) Have your own Free Amazon Web Services account.\n\n\t- Create one at http://aws.amazon.com\n \n2) Obtain Access Keys:\n \n\t- On your Amazon account, go to your profile (at the top right)\n\t \n\t- My Security Credentials > Access Keys > Create New Access Key\n\t \n\t- Record the Access Key ID and Secret Access Key ID (you will need them in step 4)\n \n3) Have Amazon Web Services Command Line Interface (AWS CLI) installed on your computer.\n\n\t- Installation options can be found at http://aws.amazon.com/cli\n\n\t- To run AWS CLI on Windows, navigate to C:\\Program Files\\Amazon\\ and run \"aws\n\t --version\" to confirm that the program is installed.  This should return the version number of aws that you are running.\n \n4) Run the following command through AWS CLI:\n\t```\n\taws configure\n\t```\n\tand enter the following:\n\t \n\t* Access Key (from step 2)\n\t* Secret Access Key (from step 2)\n\t* Default region name (us-east-1)\n\t* Default output format (ex: json)\n\n### Accessing files through AWS CLI\n\nNow go to your command window. The title of the s3 bucket is: \n\n *\tRDE (public access): usdot-its-cvpilot-public-data\n\nRun the following to check access:\n```\naws s3 ls s3://{bucket name}/ --recursive --human-readable --summarize --profile {profile_name}\n```\n\nFor Example:\n```\naws s3 ls s3://usdot-its-cvpilot-public-data/ --recursive --human-readable --summarize --profile default\n```\n\n### Directory Structure\n\nThe directory structure within the buckets will take the following form:\n\n\t{Source_Name}/{Data_Type}/{Year}/{Month}/{Day}/{Hour}\n\nSo for example, accessing Wyoming CV Pilots BSM data for a specific time will look like: \n\n\n\twydot/BSM/2017/08/15/23/wydot-filtered-bsm-1501782546127.json\n \nWhere in this example the actual BSM file is titled 'wydot-filtered-bsm-1501782546127.json'. Data prior to January 18, 2018 is one message per file, from that date onwards files will contain multiple messages. \n\n### Downloading from S3\n\nTo download all data from the S3 Bucket, enter the following command:\n\n```\naws s3 cp s3://{bucketname}/{local_directory} --recursive\n```\n\nFor example, to download all BSM data from 2017:\n```\naws s3 cp s3://usdot-its-cvpilot-public-data/wydot/BSM/2017/ --recursive\n```\n\nTo limit the data being dowloaded you can use AWS CLI's filtering which is detailed here: http://docs.aws.amazon.com/cli/latest/reference/s3/#use-of-exclude-and-include-filters.\n\n## Data Types\n\n\n\n### Wyoming CV Data\n\n- [Details on Wyoming CV DATA BSMs and TIMs messages and samples](https://github.com/usdot-jpo-ode/jpo-ode/blob/master/docs/ODE_Output_Schema_Reference.docx)\n- [Full data set in AWS](http://usdot-its-cvpilot-public-data.s3.amazonaws.com/index.html)\n\n#### WYDOT BSM\n\n- [Sample Data](https://data.transportation.gov/Automobiles/Wyoming-CV-Pilot-Basic-Safety-Message-One-Day-Samp/9k4m-a3jc)\n\n\n#### Doing simple data analysis on the Wyoming Connected Vehicles (CV) Data\t\t\n  \t\t  \n -A basic tutorial covering accessing the data in a Python Jupyter Notebook \n (note analysis of the data can be done by almost any programming langauge just Python was selected for this example):\n\n - [Introduction to WY CV data through ITS JPO Sandbox](example/accessing_wydot.ipynb)\n\n## Get Involved\n------------\n\nWe welcome your feedback and ideas. Here's how to reach us:\n\n- [Open an Issue](https://github.com/usdot-its-jpo-data-portal/sandbox/issues)\n\n\n\n\n","url":"https://raw.githubusercontent.com/usdot-its-jpo-data-portal/sandbox/master/README.md"},"releases":[],"repository":"sandbox","repository_url":"https://github.com/usdot-its-jpo-data-portal/sandbox","stage_id":"29404495_99607823","stars":7,"suggest":[{"output":"sandbox# name","input":["sandbox"]},{"output":"sandbox# name","input":["sandbox"]},{"output":"sandbox# description","input":["Public","repository","tracking","progress","in","development","of","new","USDOT","ITS","JPO","data","sandbox"]},{"output":"sandbox# languages","input":["Jupyter Notebook","Python"]},{"output":"sandbox# contributors","input":["James-OHara","chuehlien"]}],"updated_at":"2019-01-08T20:52:26Z","watchers":5}}
{"_index":"projects","_type":"project","_id":"23056647_88916902","_score":1,"_source":{"commits":122,"contributors":4,"contributors_list":[{"avatar_url":"https://avatars3.githubusercontent.com/u/1867490?v=4","profile_url":"https://github.com/aferber","user_type":"User","username":"aferber"},{"avatar_url":"https://avatars0.githubusercontent.com/u/1124162?v=4","profile_url":"https://github.com/jmcarter9t","user_type":"User","username":"jmcarter9t"},{"avatar_url":"https://avatars2.githubusercontent.com/u/10130982?v=4","profile_url":"https://github.com/hmusavi","user_type":"User","username":"hmusavi"},{"avatar_url":"https://avatars0.githubusercontent.com/u/549261?v=4","profile_url":"https://github.com/tonychen091","user_type":"User","username":"tonychen091"}],"created_at":"2017-04-20T22:46:26Z","forks":{"forkedRepos":[{"id":"18469242_138419780","name":"jpo-cvdp","org_name":"pir8aye"},{"id":"13427284_116835764","name":"jpo-cvdp","org_name":"wayties"},{"id":"5230957_102653689","name":"jpo-cvdp","org_name":"daheise"},{"id":"10130982_91606154","name":"jpo-cvdp","org_name":"hmusavi"}]},"full_name":"usdot-jpo-ode/jpo-cvdp","language":"C++","languages":{"Batchfile":"173","C":"30710","C++":"1619956","CMake":"6836","Dockerfile":"1245","Python":"40456","Shell":"9368"},"organization":{"org_avatar_url":"https://avatars2.githubusercontent.com/u/23056647?v=4","org_type":"Organization","organization":"usdot-jpo-ode","organization_url":"https://github.com/usdot-jpo-ode"},"origin":"PUBLIC","project_description":"SUBMODULE: Connected Vehicle Data Privacy module for the USDOT ITS JPO data programs. Filters geolocations using geofencing and certain data fields. Also redacts fields based on definable conditions.","project_name":"jpo-cvdp","rank":199,"readMe":{"content":"Master: [![Build Status](https://travis-ci.org/usdot-jpo-ode/jpo-cvdp.svg?branch=master)](https://travis-ci.org/usdot-jpo-ode/jpo-cvdp) [![Quality Gate](https://sonarqube.com/api/badges/gate?key=jpo-cvdp-key)](https://sonarqube.com/dashboard?id=jpo-cvdp-key)\n\n# jpo-cvdp\n\nThe United States Department of Transportation Joint Program Office (JPO)\nConnected Vehicle Data Privacy (CVDP) Project is developing a variety of methods\nto enhance the privacy of individuals who generated connected vehicle data.\n\nConnected vehicle technology uses in-vehicle wireless transceivers to broadcast\nand receive basic safety messages (BSMs) that include accurate spatiotemporal\ninformation to enhance transportation safety. Integrated Global Positioning\nSystem (GPS) measurements are included in BSMs.  Databases, some publicly\navailable, of BSM sequences, called trajectories, are being used to develop\nsafety and traffic management applications. **BSMs do not contain explicit\nidentifiers that link trajectories to individuals; however, the locations they\nexpose may be sensitive and associated with a very small subset of the\npopulation; protecting these locations from unwanted disclosure is extremely\nimportant.** Developing procedures that minimize the risk of associating\ntrajectories with individuals is the objective of this project.\n\n# The Operational Data Environment (ODE) Privacy Protection Module (PPM)\n\nThe PPM operates on streams of raw BSMs generated by the ODE. It determines\nwhether individual BSMs should be retained or suppressed (deleted) based on the\ninformation in that BSM and auxiliary map information used to define a geofence.\nBSM geoposition (latitude and longitude) and speed are used to determine the\ndisposition of each BSM processed. The PPM also redacts other BSM fields.\n\n## PPM Limitations\n\nProtecting against inference-based privacy attacks on spatiotemporal\ntrajectories (i.e., sequences of BSMs from a single vehicle) in **general** is\na challenging task. An example of an inference-based privacy attack is\nidentifying the driver that generated a sequence of BSMs using specific\nlocations they visit during their trip, or other features discernable from the\ninformation in the BSM sequence. **This PPM treats a specific use case: a\ngeofenced area where residences do not exist, e.g., a highway corridor, with\ncertain speed restrictions.** Do not assume this strategy will work in general.\nThere are alternative strategies that must be employed to handle cases where\nloitering locations can aid in learning the identity of the driver.\n\n## Table of Contents\n\n1. [Release Notes](#release-notes)\n2. [Documentation](#documentation)\n3. [Development and Collaboration Tools](#development-and-collaboration-tools)\n3. [Getting Started](#getting-started)\n4. [Installation](docs/installation.md)\n5. [Configuration and Operation](docs/configuration.md)\n6. [Testing](docs/testing.md)\n7. [Development](docs/coding-standards.md)\n\n## Release Notes\n\n### ODE Sprint 38\n\n- ODE-771: Fixed reported bug where the PPM exits when connections to Kafka brokers fail.\n\n### ODE Sprint 15\n\n- ODE-369/ORNL-15: Logging\n- Updated Identifier Redactor to include random assignment in lieu of fixed assignment.\n\n### ODE Sprint 14\n\n- ORNL-17: USDOT Playbook\n\n### ODE Sprint 13\n\n- ODE-290: Integration with the ODE.\n\n### ODE Sprint 12\n\n- ODE-77: Complete documentation\n\n### ODE Sprint 11\n\n- (Partial Complete) ODE-282 Implement a Module that Interfaces with the ODE.\n- (Partially Complete) ODE-77 Implement a PPM that uses a Geofence to Filter BSMs.\n\n# Documentation\n\nThe following document will help practitioners build, test, deploy, and understand the PPM's functions:\n\n- [Privacy Protection Module User Guide](docs/ppm_user_manual.docx)\n\nAll stakeholders are invited to provide input to these documents. Stakeholders should direct all input on this document\nto the JPO Product Owner at DOT, FHWA, or JPO. To provide feedback, we recommend that you create an \"issue\" in this\nrepository (https://github.com/usdot-jpo-ode/jpo-cvdp/issues). You will need a GitHub account to create an issue. If you\ndon’t have an account, a dialog will be presented to you to create one at no cost.\n\n## Code Documentation\n\nCode documentation can be generated using [Doxygen](https://www.doxygen.org) by following the commands below:\n\n```bash\n$ sudo apt install doxygen\n$ cd <install root>/jpo-cvdp\n$ doxygen\n```\n\nThe documentation is in HTML and is written to the `<install root>/jpo-cvdp/docs/html` directory. Open `index.html` in a\nbrowser.\n\n# Development and Collaboration Tools\n\n## Source Repositories - GitHub\n\n- https://github.com/usdot-jpo-ode/jpo-cvdp\n- `git@github.com:usdot-jpo-ode/jpo-cvdp.git`\n\n## Agile Project Management - Jira\nhttps://usdotjpoode.atlassian.net/secure/Dashboard.jspa\n\n## Continuous Integration and Delivery\n\nThe PPM is tested using [Travis Continuous Integration](https://travis-ci.org).\n\n# Getting Started\n\n## Prerequisites\n\nYou will need Git to obtain the code and documents in this repository.\nFurthermore, we recommend using Docker to build the necessary containers to\nbuild, test, and experiment with the PPM. The [Docker](#docker) instructions can be found in that section.\n\n- [Git](https://git-scm.com/)\n- [Docker](https://www.docker.com)\n\nYou can find more information in our [installation and setup](docs/installation.md) directions.\n\n## Getting the Source Code\n\nSee the installation and setup instructions unless you just want to examine the code.\n\n**Step 1.** Disable Git `core.autocrlf` (Only the First Time)\n\n   **NOTE**: If running on Windows, please make sure that your global git config is\n   set up to not convert End-of-Line characters during checkout. This is important\n   for building docker images correctly.\n\n```bash\ngit config --global core.autocrlf false\n```\n\n**Step 2.** Clone the source code from GitHub repositories using Git commands:\n\n```bash\ngit clone https://github.com/usdot-jpo-ode/jpo-cvdp.git\n```\n","url":"https://raw.githubusercontent.com/usdot-jpo-ode/jpo-cvdp/master/README.md"},"releases":[],"repository":"jpo-cvdp","repository_url":"https://github.com/usdot-jpo-ode/jpo-cvdp","stage_id":"23056647_88916902","stars":7,"suggest":[{"input":["jpo-cvdp"],"output":"jpo-cvdp# name"},{"input":["jpo-cvdp"],"output":"jpo-cvdp# name"},{"input":["SUBMODULE","Connected","Vehicle","Data","Privacy","module","for","the","USDOT","ITS","JPO","data","programs","Filters","geolocations","using","geofencing","and","certain","data","fields","Also","redacts","fields","based","on","definable","conditions"],"output":"jpo-cvdp# description"},{"input":["Shell","Batchfile","Python","C","C++","CMake","Dockerfile"],"output":"jpo-cvdp# languages"},{"input":["aferber","jmcarter9t","hmusavi","tonychen091"],"output":"jpo-cvdp# contributors"}],"updated_at":"2018-11-14T14:05:26Z","watchers":9}}
{"_index":"projects","_type":"project","_id":"23056647_95120750","_score":1,"_source":{"commits":55,"contributors":4,"contributors_list":[{"avatar_url":"https://avatars0.githubusercontent.com/u/549261?v=4","profile_url":"https://github.com/tonychen091","user_type":"User","username":"tonychen091"},{"avatar_url":"https://avatars0.githubusercontent.com/u/12912578?v=4","profile_url":"https://github.com/mvs5465","user_type":"User","username":"mvs5465"},{"avatar_url":"https://avatars2.githubusercontent.com/u/10130982?v=4","profile_url":"https://github.com/hmusavi","user_type":"User","username":"hmusavi"},{"avatar_url":"https://avatars1.githubusercontent.com/u/29437753?v=4","profile_url":"https://github.com/vmayorskiy-al","user_type":"User","username":"vmayorskiy-al"}],"created_at":"2017-06-22T13:52:15Z","forks":{"forkedRepos":[{"id":"13427284_116835838","name":"jpo-s3-deposit","org_name":"wayties"}]},"full_name":"usdot-jpo-ode/jpo-s3-deposit","language":"Java","languages":{"Dockerfile":"654","HTML":"12428","Java":"13836"},"organization":{"org_avatar_url":"https://avatars2.githubusercontent.com/u/23056647?v=4","org_type":"Organization","organization":"usdot-jpo-ode","organization_url":"https://github.com/usdot-jpo-ode"},"origin":"PUBLIC","project_description":"SUBMODULE: Generic Kafka stream to S3 depositing modules. Packages JSON kafka streams into files for depositing into research data environments.","project_name":"jpo-s3-deposit","rank":106,"readMe":{"content":"# AWS Deposit Service\n\nThis project is intended to serve as a  consumer application to subscribe to a Kafka topic of streaming JSON, package the results as a JSON file, and deposits the resulting file into a predetermined Firehose/Kinesis or S3 bucket. This runs alongside the ODE and when deployed using Docker Compose, runs in a Docker container.\n\n## Quick Run\nThe use of AWS credentials is being read from the machine's environmental variables. You may also set them in your bash profile. Note that when using Docker Compose from the main `jpo-ode` repository, these variables are set in the `.env` present in that repo.\n\n```\nexport AWS_ACCESS_KEY_ID=<AWS_ACCESS_KEY>\nexport AWS_SECRET_ACCESS_KEY=<AWS_SECRET_KEY>\n```\n\nThe project needs to be compiled with assembly to ensure that that resulting jar is runnable with the Kafka libraries. It will produce a jar under `target/` with a \"with-dependencies\" tag.\n\n```\nmvn clean compile assembly:single install\n```\n\nTo run the jar, be sure to include the topic at the end and group id at the end. If this is not a distributed system, the group can be any string.\n\n```\njava -jar target/jpo-aws-depositor-0.0.1-SNAPSHOT-jar-with-dependencies.jar   \n\nusage: Consumer Example\n -s,--bootstrap-server <arg>   Endpoint ('ip:port')\n -d,--destination <arg>        Destination (Optional, defaults to Kinesis/Firehose, put \"s3\" to override) \n -g,--group <arg>              Consumer Group\n -k,--key_name <arg>           Key Name\n -b,--bucket-name <arg>        Bucket Name\n -t,--topic <arg>              Topic Name\n -type,--type <arg>            string|byte message type\n```\nExample Usage As Of: 3/2/18\n\n``` \njava -jar target/jpo-aws-depositor-0.0.1-SNAPSHOT-jar-with-dependencies.jar --bootstrap-server 192.168.1.1:9092 -g group1 -t topic.OdeTimJson -b test-bucket-name -k \"bsm/ingest/bsm-\"\n```\n\nIt should return the following confirmation\n\n```\nDEBUG - Bucket name: test-usdot-its-cvpilot-wydot-bsm\nDEBUG - Key name: bsm/ingest/wydot-bsm-\nDEBUG - Kafka topic: topic.OdeBsmJson\nDEBUG - Type: string\nDEBUG - Destination: null\n\nSubscribed to topic OdeTimJson \n```\nTriggering an upload into the ODE, the output should be seen decoded into JSON in the console.\n\n![CLI-output](images/cli-output.png)\n\n## Additional Resources\n\nWith the Kafka installed locally on a machine, here are a few additional commands that may be helpful while debugging Kafka topics.\n\n[Kafka Install Instructions](https://www.cloudera.com/documentation/kafka/latest/topics/kafka_installing.html#concept_ngx_4l4_4r)\n\nThe IP used is the location of the Kafka endpoints.\n\n#### Create, alter, list, and describe topics.\n\n```\nkafka-topics --zookeeper 192.168.1.151:2181 --list\nsink1\nt1\nt2\n```\n\n#### Read data from a Kafka topic and write it to standard output. \n\n```\nkafka-console-consumer --zookeeper 192.168.1.151:2181 --topic topic.J2735Bsm\n```\n\n#### Push data from standard output and write it into a Kafka topic. \n\n```\nkafka-console-producer --broker-list 192.168.1.151:9092 --topic topic.J2735Bsm \n```\n","url":"https://raw.githubusercontent.com/usdot-jpo-ode/jpo-s3-deposit/master/README.md"},"releases":[],"repository":"jpo-s3-deposit","repository_url":"https://github.com/usdot-jpo-ode/jpo-s3-deposit","stage_id":"23056647_95120750","stars":1,"suggest":[{"input":["jpo-s3-deposit"],"output":"jpo-s3-deposit# name"},{"input":["jpo-s3-deposit"],"output":"jpo-s3-deposit# name"},{"input":["SUBMODULE","Generic","Kafka","stream","to","S3","depositing","modules","Packages","JSON","kafka","streams","into","files","for","depositing","into","research","data","environments"],"output":"jpo-s3-deposit# description"},{"input":["Java","HTML","Dockerfile"],"output":"jpo-s3-deposit# languages"},{"input":["tonychen091","mvs5465","hmusavi","vmayorskiy-al"],"output":"jpo-s3-deposit# contributors"}],"updated_at":"2018-12-21T15:57:19Z","watchers":7}}
{"_index":"projects","_type":"project","_id":"29147435_97514282","_score":1,"_source":{"commits":287,"contributors":4,"contributors_list":[{"avatar_url":"https://avatars1.githubusercontent.com/u/29666393?v=4","profile_url":"https://github.com/foabodo","user_type":"User","username":"foabodo"},{"avatar_url":"https://avatars2.githubusercontent.com/u/464800?v=4","profile_url":"https://github.com/bsumne01","user_type":"User","username":"bsumne01"},{"avatar_url":"https://avatars1.githubusercontent.com/u/24627287?v=4","profile_url":"https://github.com/robertrittmuller","user_type":"User","username":"robertrittmuller"},{"avatar_url":"https://avatars0.githubusercontent.com/u/26333958?v=4","profile_url":"https://github.com/rittmuller-volpe","user_type":"User","username":"rittmuller-volpe"}],"created_at":"2017-07-17T19:28:39Z","forks":{"forkedRepos":[]},"full_name":"VolpeUSDOT/SNVA","language":"Python","languages":{"Python":"104493"},"organization":{"org_avatar_url":"https://avatars0.githubusercontent.com/u/29147435?v=4","org_type":"Organization","organization":"VolpeUSDOT","organization_url":"https://github.com/VolpeUSDOT"},"origin":"PUBLIC","project_description":"SHRP2 NDS Video Analytics built on TensorFlow","project_name":"SNVA","rank":319,"readMe":{"content":"# SHRP2 NDS Video Analytics (SNVA) v0.1.2\n\nThis repository houses the SNVA application and additional code used to develop the computer vision models at the core of SNVA. Model development code is based on [TensorFlow-Slim](https://github.com/tensorflow/models/tree/master/research/slim).\n\nSNVA is intended to expand the Roadway Information Database (RID)’s ability to help transportation safety researchers develop and answer research questions. The RID is the primary source of data collected as part of the FHWA’s SHRP2 Naturalistic Driving Study, including vehicle telemetry, geolocation, and roadway characteristics data. Missing from the RID are the locations of work zones driven through by NDS volunteer drivers. The app’s first release will focus on enabling/enhancing research questions related to work zone safety by using machine learning-based computer vision techniques to exhaustively and automatically detect the presence of work zone features across the entire ~1 million-hour forward-facing video data set, and then conflating that information with the RID’s time-series records. Previously, researchers depended on sparse and low fidelity 511 data provided by states that hosted the routes driven by volunteers. A successful deployment of the SNVA app will make it possible to query the RID for the exact start/stop locations, lengths, and frequencies of work zones in trip videos; a long-standing, highly desired ability within the SHRP2 community.\n\n\n## Required Software Dependencies\n\nSNVA has been tested using the following software stack:\n\n- Ubuntu = 16.04\n- Python >= 3.5\n- TensorFlow = 1.8 (and its published dependencies)\n- FFmpeg >= 2.8\n\n\n## Optional Software Dependencies\n\nInference speed was observed to improve by ~10% by building TensorFlow from source and including:\n\n- TensorRT = 3.0.4\n\nInstallation of the Docker-containerized version of SNVA has been tested using:\n\n- NVIDIA-Docker = 2.0.3\n- Docker = 18.03.1-CE\n\n\n## System Requirements and Performance Expectations\n\nSNVA is intended to run on systems with NVIDIA GPUs, but can also run in a CPU-only mode. SNVA runs ~10x faster on a single NVIDIA GeForce GTX 1080 Ti together with a 3.00GHz 10-core Intel Core i7-6950X CPU than it does on the 10-core CPU alone. For a system with N GPUs and for --numprocessesperdevice = M, SNVA will process N * M videos concurrently, but is not (at this time) designed to distribute the processing of a single video across multiple GPUs. Inference speeds depend on the particular CNN architecture used to develop the model. When tested on two GPUs against 31,535,862 video frames spanning 1,344 videos, InceptionV3 inferred class labels at 826.24 fps on average over 10:40:04 hours, MobilenetV2 (with one video processor assigned to each GPU) averaged 1491.36 fps over 05:58:20 hours, and MobilenetV2 (with two video processors assigned to each GPU) averaged 1833.1 fps over 4:53:42 hours. RAM consumption on our development machine appeared to be safely bounded above by 3.75GB per active video processor.\n\n\n## To install on Ubuntu:\n\n```shell\nexport SNVA_HOME=/path/to/parent/folder/of/snva.py\nexport FFMPEG_HOME=/path/to/parent/folder/of/ffmpeg/binary\nexport FFPROBE_HOME=/path/to/parent/folder/of/ffprobe/binary\n\ncd /path/to/parent/folder/of/SNVA/repo\nmkdir SNVA\ngit clone https://github.com/VolpeUSDOT/SNVA.git SNVA\n```\n\n## To run on Ubuntu:\n\n```shell\npython3 snva.py\n  --inputpath /path/to/your/desired/video_file/source/directory/or/file \\\n  --modelname mobilenet_v2\n```\n\n```shell\npython snva.py\n  --modelname mobilenet_v2 \\\n  --inputpath /path/to/your/desired/video_file/source/directory/or/file \\\n  --outputpath /path/to/your/desired/report/directory \\\n  --logpath /path/to/your/desired/report/directory/log\n  --batchsize 128 --loglevel debug --smoothprobs --extracttimestamps --crop \\\n  --writeinferencereports True\n```\n\n## To run using NVIDIA-Docker on Ubuntu (for a text file listing absolute paths to videos):\n\n```shell\nsudo nvidia-docker run \\\n  --mount type=bind, \\\n    src=/path/to/your/desired/video/source/file/or/directory,dst=/media/input \\\n  --mount type=bind, \\\n    src=/path/to/a/common/root/video/directory/when/inputpath/is/a/text/file, \\\n    dst=/media/root \\\n  --mount type=bind, \\\n    src=/path/to/your/desired/csv_file/destination/directory,dst=/media/output \\\n  --mount type=bind, \\\n    src=/path/to/your/desired/log_file/destination/directory,dst=/media/logs \\\n  volpeusdot/snva \\\n  --inputlistrootdirpath /common/root/path/on/the/host \\\n  --inputpath /media/input --outputpath /media/output --logspath /media/logs \\\n  --modelname inception_v3 --batchsize 64 --smoothprobs --extracttimestamps \\\n  --crop --writeinferencereports True\n```\n## To run using NVIDIA-Docker on Ubuntu (for a directory of videos):\n\n```shell\nsudo nvidia-docker run \\\n  --mount type=bind, \\\n    src=/path/to/your/desired/video_file/source/directory,dst=/media/input \\\n  --mount type=bind, \\\n    src=/path/to/your/desired/csv_file/destination/directory,dst=/media/output \\\n  --mount type=bind, \\\n    src=/path/to/your/desired/log_file/destination/directory,dst=/media/logs \\\n  volpeusdot/snva \\\n  --inputpath /media/input --outputpath /media/output --logspath /media/logs \\\n  --modelname inception_v3 --batchsize 64 --smoothprobs --extracttimestamps \\\n  --crop --writeinferencereports True\n```\n\n## To run using NVIDIA-Docker on Ubuntu (for a single video):\n\n```shell\nsudo nvidia-docker run \\\n  --mount type=bind, \\\n    src=/path/to/your/desired/video_file/source/directory/video_file_name.ext, \\\n    dst=/media/input/video_file_name.ext \\\n  --mount type=bind, \\\n    src=/path/to/your/desired/csv_file/destination/directory,dst=/media/output \\\n  --mount type=bind, \\\n    src=/path/to/your/desired/log_file/destination/directory,dst=/media/logs \\\n  volpeusdot/snva \\\n  --inputpath /media/input/ --outputpath /media/output --logspath /media/logs \\\n  --modelname inception_v3 --batchsize 64 --smoothprobs --extracttimestamps \\\n  --crop --writeinferencereports True\n```\n\n\n## Usage\n\nFlag | Short Flag | Properties | Description\n:------:|:---------------:|:---------------------:|:-----------:\n--batchsize|-bs|type=int, default=32|Number of concurrent neural net inputs\n--binarizeprobs|-b|action=store_true|Round probs to zero or one. For distributions with two 0.5 values, both will be rounded up to 1.0\n--classnamesfilepath|-cnfp||Path to the class ids/names text file\n--cpuonly|-cpu|action=store_true|Useful for systems without an NVIDIA GPU\n--crop|-c|action=store_true|Crop video frames to [offsetheight, offsetwidth, targetheight, targetwidth]\n--cropheight|-ch|type=int, default=320|y-component of bottom-right corner of crop\n--cropwidth|-cw|type=int, default=474|x-component of bottom-right corner of crop\n--cropx|-cx|type=int, default=2|x-component of top-left corner of crop\n--cropy|-cy|type=int, default=0|y-component of top-left corner of crop\n--deinterlace|-d|action=store_true|Apply de-interlacing to video frames during extraction\n--excludepreviouslyprocessed|-epp|action=store_true|Skip processing of videos for which reports already exist in outputpath\n--extracttimestamps|-et|action=store_true|Crop timestamps out of video frames and map them to strings for inclusion in the output CSV\n--gpumemoryfraction|-gmf|type=float, default=0.9|% of GPU memory available to this process\n--inputpath|-ip|required=True|Path to a single video file, a folder containing video files, or a text file that lists absolute video file paths\n--inputlistrootdirpath|-ilrdp|Path to the common root directory shared by video file paths listed in the text file specified using --inputpath\n--ionodenamesfilepath|-ifp|Path to the io tensor names text file\n--loglevel|-ll|default=info|Defaults to 'info'. Pass 'debug' or 'error' for verbose or minimal logging, respectively\n--logmode|-lm|default=verbose|If verbose, log to file and console. If silent, log to file only\n--logpath|-l|default=logs|Path to the directory where log files are stored\n--logmaxbytes|-lmb|type=int|default=2**23|File size in bytes at which the log rolls over\n--modelsdirpath|-mdp|default=models/work_zone_scene_detection|Path to the parent directory of model directories\n--modelname|-mn|required=True|The square input dimensions of the neural net\n--numchannels|-nc|type=int, default=3|The fourth dimension of image batches\n--numprocessesperdevice|-nppd|type=int, default=1|The number of instances of inference to perform on each device\n--protobuffilename|-pbfn|default=model.pb|Name of the model protobuf file\n--outputpath|-op|default=reports|Path to the directory where reports are stored\n--smoothprobs|-sp|action=store_true|Apply class-wise smoothing across video frame class probability distributions\n--smoothingfactor|-sf|type=int, default=16|The class-wise probability smoothing factor\n--timestampheight|-th|type=int, default=16|The length of the y-dimension of the timestamp overlay\n--timestampmaxwidth|-tw|type=int, default=160|The length of the x-dimension of the timestamp overlay\n--timestampx|-tx|type=int, default=25|x-component of top-left corner of timestamp (before cropping)\n--timestampy|-ty|type=int, default=340|y-component of top-left corner of timestamp (before cropping)\n--writeeventreports|-wer|type=bool, default=True|Output a CVS file for each video containing one or more feature events\n--writeinferencereports|-wir|type=bool, default=False|For every video, output a CSV file containing a probability distribution over class labels, a timestamp, and a frame number for each frame\n\n\n## Troubleshooting and Additional Considerations\n\nIf a timestamp cannot be interpreted, a -1 will be written in its place in the output CSV.\n\nWhile inference speed has been observed to monotonically increase with batch size, it is important to not exceed the GPU's memory capacity. The SNVA app does not automatically determine the optimal batch size for maximum inference speed. It is best to discover the optimal batch size by testing the app on a small sample of videos (say ~15) starting at a relatively low batch size, then iteratively incrementing the batch size while monitoring GPU memory utilization (e.g. using the NVIDIA X Server Settings GUI app or nvidia-smi CLI app: nvidia-smi --query-compute-apps=process_name,pid,used_gpu_memory --format=csv) and also observing the cumulative analysis duration printed at the end of each run. GPU memory is set to be dynamically allocated, so one should monitor its usage over time to increase the chance of observing peak utilization.\n\nWhen terminating the app using ctrl-c, there may be a delay while the app terminates gracefully.\n\nWhen terminating the dockerized app, use ctrl-c to let the app terminate gracefully before invoking the nvidia-docker stop command (which actually shouldn't be needed).\n\nWindows is not officially supported but may be used with minor code tweaks.\n\nWhen using Docker, some extraneous C++ output is passed to the host machine's console that is not actually logged to file and is not intended to be seen. Consider this a bug and ignore it.\n## License\n\n[MIT](https://opensource.org/licenses/MIT)\n","url":"https://raw.githubusercontent.com/VolpeUSDOT/SNVA/master/README.md"},"releases":[],"repository":"SNVA","repository_url":"https://github.com/VolpeUSDOT/SNVA","stage_id":"29147435_97514282","stars":0,"suggest":[{"input":["SNVA"],"output":"SNVA# name"},{"input":["SNVA"],"output":"SNVA# name"},{"input":["SHRP2","NDS","Video","Analytics","built","on","TensorFlow"],"output":"SNVA# description"},{"input":["Python"],"output":"SNVA# languages"},{"input":["foabodo","bsumne01","robertrittmuller","rittmuller-volpe"],"output":"SNVA# contributors"}],"updated_at":"2018-09-15T15:17:14Z","watchers":3}}
{"_index":"projects","_type":"project","_id":"33698304_117572049","_score":1,"_source":{"commits":0,"contributors":0,"contributors_list":[],"created_at":"2018-01-15T16:58:58Z","forks":{"forkedRepos":[]},"full_name":"usdot-jpo-sdcsdw/common-models","language":"Java","languages":{"Java":"51831"},"organization":{"org_avatar_url":"https://avatars1.githubusercontent.com/u/33698304?v=4","org_type":"Organization","organization":"usdot-jpo-sdcsdw","organization_url":"https://github.com/usdot-jpo-sdcsdw"},"origin":"PUBLIC","project_name":"common-models","rank":7,"readMe":{"content":"# Common Models\n\nThis repository contains (1) The parser to convert from XML to POJO and vice versa, and (2) the necessary models.\n\n\n### Prerequisites\n* JDK 1.8: http://www.oracle.com/technetwork/pt/java/javase/downloads/jdk8-downloads-2133151.html\n* Maven: https://maven.apache.org/install.html\n* Git: https://git-scm.com/\n\n\n### Getting Started\n\n#### Step 1 - Clone this repository\n```\ngit clone https://github.com/usdot-jpo-sdcsdw/common-models.git\n```\n#### Step 2 - Build the application\n```\ncd common-models\nmvn clean install\n```\n\n\n## Built With\n\n* [Maven](https://maven.apache.org/) - Dependency Management\n\n\n\n## License\n\nThis project is licensed under the Apache License - see  [LICENSE](LICENSE) file for details\n\n\n","url":"https://raw.githubusercontent.com/usdot-jpo-sdcsdw/common-models/master/README.md"},"releases":[],"repository":"common-models","repository_url":"https://github.com/usdot-jpo-sdcsdw/common-models","stage_id":"33698304_117572049","stars":1,"suggest":[{"input":["common-models"],"output":"common-models# name"},{"input":["common-models"],"output":"common-models# name"},{"input":[""],"output":"common-models# description"},{"input":["Java"],"output":"common-models# languages"},{"input":[],"output":"common-models# contributors"}],"updated_at":"2018-01-15T21:16:43Z","watchers":1}}
{"_index":"projects","_type":"project","_id":"33698304_117576271","_score":1,"_source":{"commits":6,"contributors":1,"contributors_list":[{"avatar_url":"https://avatars1.githubusercontent.com/u/33724014?v=4","profile_url":"https://github.com/andrewm-aero","user_type":"User","username":"andrewm-aero"}],"created_at":"2018-01-15T17:42:23Z","forks":{"forkedRepos":[]},"full_name":"usdot-jpo-sdcsdw/fedgov-cv-sso-webapp","language":"CSS","languages":{"CSS":"110316","Java":"25341","Shell":"804"},"organization":{"org_avatar_url":"https://avatars1.githubusercontent.com/u/33698304?v=4","org_type":"Organization","organization":"usdot-jpo-sdcsdw","organization_url":"https://github.com/usdot-jpo-sdcsdw"},"origin":"PUBLIC","project_description":"Single sign-on authentication server for the SDW ","project_name":"fedgov-cv-sso-webapp","rank":14,"readMe":{"content":"# Connected Vehicles SSO Webapp Project\n\nThe fedgov-cv-sso-webapp project is a webapp which contains the configuration and code to run CAS for authentication to allow access\nto other webapps.\n\n![Diagram](doc/images/fedgov-cv-sso-webapp-diagram.png)\n\n![Diagram](doc/images/fedgov-cv-sso-webapp-screenshot.png)\n\n<a name=\"toc\"/>\n\n## Table of Contents\n\n[I. Release Notes](#release-notes)\n\n[II. Documentation](#documentation)\n\n[III. Getting Started](#getting-started)\n\n[IV. Configuration](#configuration)\n\n[V. Running the Application (Standalone)](#running-standalone)\n\n[VI. Running the Application (Docker)](#running-docker)\n\n---\n\n<a name=\"release-notes\" id=\"release-notes\"/>\n\n## [I. Release Notes](ReleaseNotes.md)\n\n<a name=\"documentation\"/>\n\n## II. Documentation\n\nThis repository produces a WAR file containing a Servlet, so it can be deployed on any web-server that supports it (e.g. Tomcat, Jetty).\n\nThe application can also be deployed using a docker container. This container will run the application under a Jetty server, and can be configured to use SSL certificates.\n\n<a name=\"getting-started\"/>\n\n## III. Getting Started\n\nThe following instructions describe the procedure to fetch, build, and run the application\n\n### Prerequisites\n* JDK 1.8: http://www.oracle.com/technetwork/pt/java/javase/downloads/jdk8-downloads-2133151.html\n* Maven: https://maven.apache.org/install.html\n* Git: https://git-scm.com/\n* Docker: https://docs.docker.com/engine/installation/\n\n---\n### Obtain the Source Code\n\n#### Step 1 - Clone public repository\n\nClone the source code from the GitHub repository using Git command:\n\n```bash\ngit clone TBD\n```\n\n<a name=\"configuration\"/>\n\n## IV. Configuration\n\nThe servlet expects the following java properties to be set:\n\n* sso.auth.mysql.url - Url to the MySQL credentials database\n* sso.auth.mysql.username - Username For the MySQL credentials database\n* sso.auth.mysql.password - Password For the MySQL credentials database\n\nThrough spring, these will automatically be set to the matching SNAKE_CASE environment variable, i.e. SSO_AUTH_MYSQL_URL \n\n<a name=\"running\"/>\n\n## V. Running the application (Standalone)\n\n---\n### Build and Deploy the Application\n\n**Step 1**: Build the WAR file\n\n```bash\nmvn package\n```\n\n**Step 2**: Deploy the WAR file\n\n```bash\n# Consult your webserver's documentation for instructions on deploying war files \ncp target/accounts.war ... \n```\n\n<a name=\"running-docker\"/>\n\n## VI. Running the Application (Docker)\n\n---\n### Build and Deploy the Application\n\n**Step 1**: Build the WAR file\n\n```bash\nmvn package\n```\n\n**Step 2**: Build the Docker image, providing the path to the native library for the PER-XER codec\n\n```bash\ndocker build -t dotcv/webapp-sso .\n```\n\n**Step 3**: Run the Docker image in a Container, mounting the SSL certificate keystore directory, and specifying the following:\n* Keystore filename\n* Keystore password\n* HTTP Port\n* HTTPS Port\n\n\n```bash\ndocker run -p HTTP_PORT:8080 \\\n           -p HTTPS_PORT:8443 \\\n           -e JETTY_KEYSTORE_PASSWORD=... \\\n           -v KEYSTORE_DIRECTORY:/usr/local/jetty/etc/keystore_mount \\\n           -e JETTY_KEYSTORE_RELATIVE_PATH=... \\\n           -e SSO_AUTH_MYSQL_URL=... \\\n           -e SSO_AUTH_MYSQL_USERNAME=... \\\n           -e SSO_AUTH_MYSQL_PASSWORD=... \\\n           dotcv/webapp-sso:latest\n```\n\n</a>","url":"https://raw.githubusercontent.com/usdot-jpo-sdcsdw/fedgov-cv-sso-webapp/master/README.md"},"releases":[],"repository":"fedgov-cv-sso-webapp","repository_url":"https://github.com/usdot-jpo-sdcsdw/fedgov-cv-sso-webapp","stage_id":"33698304_117576271","stars":1,"suggest":[{"input":["fedgov-cv-sso-webapp"],"output":"fedgov-cv-sso-webapp# name"},{"input":["fedgov-cv-sso-webapp"],"output":"fedgov-cv-sso-webapp# name"},{"input":["Single","sign-on","authentication","server","for","the","SDW"],"output":"fedgov-cv-sso-webapp# description"},{"input":["CSS","Java","Shell"],"output":"fedgov-cv-sso-webapp# languages"},{"input":["andrewm-aero"],"output":"fedgov-cv-sso-webapp# contributors"}],"updated_at":"2018-01-22T19:40:45Z","watchers":0}}
{"_index":"projects","_type":"project","_id":"33698304_117576093","_score":1,"_source":{"commits":31,"contributors":1,"contributors_list":[{"avatar_url":"https://avatars1.githubusercontent.com/u/33724014?v=4","profile_url":"https://github.com/andrewm-aero","user_type":"User","username":"andrewm-aero"}],"created_at":"2018-01-15T17:40:33Z","forks":{"forkedRepos":[]},"full_name":"usdot-jpo-sdcsdw/fedgov-cv-webapp-websocket","language":"Java","languages":{"HTML":"4438","Java":"133316","Shell":"1351"},"organization":{"org_avatar_url":"https://avatars1.githubusercontent.com/u/33698304?v=4","org_type":"Organization","organization":"usdot-jpo-sdcsdw","organization_url":"https://github.com/usdot-jpo-sdcsdw"},"origin":"PUBLIC","project_description":"Websockets interface to the SDW","project_name":"fedgov-cv-webapp-websocket","rank":39,"readMe":{"content":"# Connected Vehicles WebFragment WebSocket Project\n\nThe fedgov-cv-webfragment-websocket project is a library containing a simple WebSocket Server implemented as a WebFragment.\n\n![Diagram](doc/images/fedgov-cv-webapp-websocket-diagram.png)\n\n<a name=\"toc\"/>\n\n## Table of Contents\n\n[I. Release Notes](#release-notes)\n\n[II. Documentation](#documentation)\n\n[III. Getting Started](#getting-started)\n\n[IV. Building](#building)\n\n[V. Deploying](#deploying)\n\n---\n\n<a name=\"release-notes\" id=\"release-notes\"/>\n\n## [I. Release Notes](ReleaseNotes.md)\n\n<a name=\"documentation\"/>\n\n## II. Documentation\n\nThis repository produces a JAR file containing a Web Fragment, so it can be incorporated into a WAR file and deployed.\n\n<a name=\"getting-started\"/>\n\n## III. Getting Started\n\nThe following instructions describe the procedure to fetch, build, and run the application\n\n### Prerequisites\n* JDK 1.8: http://www.oracle.com/technetwork/pt/java/javase/downloads/jdk8-downloads-2133151.html\n* Maven: https://maven.apache.org/install.html\n* Git: https://git-scm.com/\n* Docker: https://docs.docker.com/engine/installation/\n* PER XER Codec: https://github.com/usdot-jpo-sdcsdw/per-xer-codec\n\n---\n### Obtain the Source Code\n\n#### Step 1 - Clone public repository\n\nClone the source code from the GitHub repository using Git command:\n\n```bash\ngit clone https://github.com/usdot-jpo-sdcsdw/fedgov-cv-webapp-websocket.git\n```\n\n<a name=\"building\"/>\n\n## IV. Building\n\n**Step 1**: Build the JAR file\n\n```bash\nmvn package\n```\n\n<a name=\"deploying\"/>\n\n## V. Deploying\n\nAs this project depends on the PER XER Codec, both the JAR file and the native shared object produced by that project need to be deployed according to its instructions. \n\n</a>","url":"https://raw.githubusercontent.com/usdot-jpo-sdcsdw/fedgov-cv-webapp-websocket/master/README.md"},"releases":[],"repository":"fedgov-cv-webapp-websocket","repository_url":"https://github.com/usdot-jpo-sdcsdw/fedgov-cv-webapp-websocket","stage_id":"33698304_117576093","stars":1,"suggest":[{"input":["fedgov-cv-webapp-websocket"],"output":"fedgov-cv-webapp-websocket# name"},{"input":["fedgov-cv-webapp-websocket"],"output":"fedgov-cv-webapp-websocket# name"},{"input":["Websockets","interface","to","the","SDW"],"output":"fedgov-cv-webapp-websocket# description"},{"input":["Java","HTML","Shell"],"output":"fedgov-cv-webapp-websocket# languages"},{"input":["andrewm-aero"],"output":"fedgov-cv-webapp-websocket# contributors"}],"updated_at":"2018-11-30T00:34:04Z","watchers":0}}
{"_index":"projects","_type":"project","_id":"33698304_117745474","_score":1,"_source":{"commits":3,"contributors":1,"contributors_list":[{"avatar_url":"https://avatars1.githubusercontent.com/u/33724014?v=4","profile_url":"https://github.com/andrewm-aero","user_type":"User","username":"andrewm-aero"}],"created_at":"2018-01-16T21:39:02Z","forks":{"forkedRepos":[]},"full_name":"usdot-jpo-sdcsdw/tim-db","language":"Shell","languages":{"Shell":"1601"},"organization":{"org_avatar_url":"https://avatars1.githubusercontent.com/u/33698304?v=4","org_type":"Organization","organization":"usdot-jpo-sdcsdw","organization_url":"https://github.com/usdot-jpo-sdcsdw"},"origin":"PUBLIC","project_description":"Docker image for storing traveler information","project_name":"tim-db","rank":11,"readMe":{"content":"# Connected Vehicles Traveler Information Database\n\nThe Connected Vehicles Traveler Information Database project contains a docker image which \nruns a Mongo database, but also will create the collections and indices for running the\nnecessary queries.\n\n<a name=\"toc\"/>\n\n## Table of Contents\n\n[I. Release Notes](#release-notes)\n\n[II. Documentation](#documentation)\n\n[III. Getting Started](#getting-started)\n\n[IV. Running the Application](#running)\n\n---\n\n<a name=\"release-notes\" id=\"release-notes\"/>\n\n## [I. Release Notes](ReleaseNotes.md)\n\n<a name=\"documentation\"/>\n\n## II. Documentation\n\nThis repository produces a Docker image which, when run, will create the necessary\ncollections and indices in, and then expose, a mongo database.\n\nFor information on configuring the Mongo database itself, see the [documentation for the underlying image](https://hub.docker.com/_/mongo/)\n\nThe database contains two collections:\n\n* Traveler Information: Contains deposited traveler information\n* Sessions: Used to maintain sessions statelessly\n\n<a name=\"getting-started\"/>\n\n## III. Getting Started\n\nThe following instructions describe the procedure to fetch, build, and run the application\n\n### Prerequisites\n* Git: https://git-scm.com/\n* Docker: https://docs.docker.com/engine/installation/\n\n---\n### Obtain the Source Code\n\n#### Step 1 - Clone public repository\n\nClone the source code from the GitHub repository using Git command:\n\n```bash\ngit clone TBD\n```\n\n<a name=\"running\"/>\n\n## IV. Running the application\n\n---\n### Build and Deploy the Application\n\n**Step 1**: Build Docker Image\n\n```bash\ndocker build -t dotcv/traveler-information-db .\n```\n\n**Step 2**: Run Docker Container\n\nThe following variables can be used to configure the database:\n\n* MONGO_DATABASE_NAME - Name of the database to store all relevant information in. Defaults to \"cvdb\".\n* TRAVELER_INFORMATION_COLLECTION_NAME - Name of the collection to store traveler information in. Defaults to \"travelerInformation\".\n* SESSION_COLLECTION_NAME - Name of the collection to store session information in. Defaults to \"session\".\n* EXPIRATION_INDEX_FIELD - Name of the JSON field which contains the expiration date of traveler information. Defaults to \"expireAt\".\n* CREATION_TIME_INDEX_FIELD - Name of the JSON field which contains the creation date of traveler information. Defaults to \"createdAt\".\n* SERVICE_REGION_INDEX_FIELD - Name of the JSON field which contains the service region of traveler information. Defaults to \"region\".\n* REQUEST_ID_INDEX_FIELD - Name of the JSON field which contains the request ID of traveler information. Defaults to \"requestId\".\n\n```bash\ndocker run \\\n    -e MONGO_DATABASE_NAME=... \\\n    -e TRAVELER_INFORMATION_COLLECTION_NAME=... \\\n    -e SESSION_COLLECTION_NAME=... \\\n    -e EXPIRATION_INDEX_FIELD=... \\\n    -e CREATION_TIME_INDEX_FIELD=... \\\n    -e SERVICE_REGION_INDEX_FIELD=... \\\n    -e REQUEST_ID_INDEX_FIELD=... \\\n    dotcv/traveler-information-db\n```\n\n</a>","url":"https://raw.githubusercontent.com/usdot-jpo-sdcsdw/tim-db/master/README.md"},"releases":[],"repository":"tim-db","repository_url":"https://github.com/usdot-jpo-sdcsdw/tim-db","stage_id":"33698304_117745474","stars":1,"suggest":[{"input":["tim-db"],"output":"tim-db# name"},{"input":["tim-db"],"output":"tim-db# name"},{"input":["Docker","image","for","storing","traveler","information"],"output":"tim-db# description"},{"input":["Shell"],"output":"tim-db# languages"},{"input":["andrewm-aero"],"output":"tim-db# contributors"}],"updated_at":"2018-01-16T21:39:40Z","watchers":0}}
